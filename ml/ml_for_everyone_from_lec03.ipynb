{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_for_everyone_from_lec03.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [    
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9b49rPihu4y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "628c1af3-54b2-4648-f651-85ff28ee23af"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwqXrGbRXV7F",
        "colab_type": "text"
      },
      "source": [
        "<h1> hypothesis and cost function </h1>\n",
        "- 1차 방정식\n",
        "$H(x) = Wx + b$\n",
        "- 임의로 표현한 1차방정식과 실제 값과의 차이를 구하는 식 (제곱 하면 음수도 양수 표현 가능)\n",
        "$(H(x) - y)^2$<br>\n",
        "- formal하게 바꾼 수식\n",
        "$ cost(W,b) = {1 \\over m}\\sum_{i=1}^m(H(x^{(i)}) - y^{(i)})^2 $\n",
        "    * m = 데이터 개수<br>\n",
        "- 결론 minimize cost(W,b) -> 학습을 통해 W, b의 최소화 값(가장 근접한 값) 구하는 것\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiHhLXjBLidM",
        "colab_type": "code",
        "outputId": "da8353d6-30b3-499c-94ce-412eba0510f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "node1 = tf.constant(3.0, tf.float32)\n",
        "node2 = tf.constant(4.0)\n",
        "node3 = tf.add(node1, node2)\n",
        "\n",
        "print('node1:', node1, \"node2:\", node2)\n",
        "print(\"node3:\", node3)\n",
        "\n",
        "print('sess.run(node1, node2): ', sess.run([node1, node2]))\n",
        "print('sess.run(node3): ', sess.run(node3))\n",
        "\n",
        "a = tf.placeholder(tf.float32)\n",
        "b = tf.placeholder(tf.float32)\n",
        "adder_node = a + b\n",
        "\n",
        "print(sess.run(adder_node, feed_dict={a:3, b:4.5}))\n",
        "print(sess.run(adder_node, feed_dict={a:[1,3], b:[2,4]}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "node1: Tensor(\"Const_13:0\", shape=(), dtype=float32) node2: Tensor(\"Const_14:0\", shape=(), dtype=float32)\n",
            "node3: Tensor(\"Add_15:0\", shape=(), dtype=float32)\n",
            "sess.run(node1, node2):  [3.0, 4.0]\n",
            "sess.run(node3):  7.0\n",
            "7.5\n",
            "[3. 7.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWbaHBishxi7",
        "colab_type": "code",
        "outputId": "38387534-addf-4f52-8979-4dba012a3522",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "x_train = [1, 2, 3]\n",
        "y_train = [1, 2, 3]\n",
        "\n",
        "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "# H(x) = Wx + b\n",
        "hypothesis = x_train * W + b\n",
        "\n",
        "# cost/loss function\n",
        "# t = [1., 2., 3., 4.]\n",
        "# tf.reduce_mean(t) ==> 2.5\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
        "\n",
        "# minimize\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "# initializes global variables in the graph.\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "'''\n",
        "adder_node = a + b\n",
        "print(sess.run(adder_node, feed_dict={a:3, b:4.5}))\n",
        "print(sess.run(adder_node, feed_dict={a:[1,3], b:[2,4]}))\n",
        "'''\n",
        "\n",
        "X = tf.placeholder(tf.float32)\n",
        "Y = tf.placeholder(tf.float32)\n",
        "\n",
        "for step in range(2001):\n",
        "  cost_val, W_val, b_val, _ = sess.run([cost, W, b, train], feed_dict={X:[1,2,3], Y:[1,2,3]})\n",
        "  if step % 20 == 0:\n",
        "    print(step, cost_val, W_val, b_val)\n",
        "\n",
        "'''\n",
        "# Fit the line\n",
        "for step in range(2001):\n",
        "  sess.run(train)\n",
        "  if step % 20 == 0:\n",
        "    print(step, sess.run(cost), sess.run(W), sess.run(b))\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 4.9856014 [0.01783126] [0.14645904]\n",
            "20 0.07565526 [0.7235749] [0.42972872]\n",
            "40 0.028343134 [0.7993711] [0.4371612]\n",
            "60 0.025375687 [0.81478024] [0.419247]\n",
            "80 0.023043295 [0.82405436] [0.39979443]\n",
            "100 0.020928249 [0.8323773] [0.38102943]\n",
            "120 0.01900737 [0.84026015] [0.36312476]\n",
            "140 0.017262803 [0.8477678] [0.3460594]\n",
            "160 0.01567834 [0.8549222] [0.3297959]\n",
            "180 0.014239346 [0.86174035] [0.3142967]\n",
            "200 0.01293238 [0.86823803] [0.29952592]\n",
            "220 0.011745408 [0.87443036] [0.2854493]\n",
            "240 0.010667349 [0.8803317] [0.2720342]\n",
            "260 0.009688272 [0.8859557] [0.2592496]\n",
            "280 0.00879903 [0.8913154] [0.24706578]\n",
            "300 0.007991419 [0.89642316] [0.23545457]\n",
            "320 0.007257938 [0.9012909] [0.22438909]\n",
            "340 0.0065917806 [0.9059298] [0.21384363]\n",
            "360 0.0059867557 [0.9103508] [0.20379378]\n",
            "380 0.0054372703 [0.91456395] [0.19421622]\n",
            "400 0.004938217 [0.9185793] [0.18508875]\n",
            "420 0.0044849645 [0.9224057] [0.17639019]\n",
            "440 0.004073314 [0.92605233] [0.1681005]\n",
            "460 0.0036994468 [0.92952764] [0.16020037]\n",
            "480 0.0033598973 [0.9328395] [0.15267152]\n",
            "500 0.003051512 [0.9359959] [0.14549652]\n",
            "520 0.0027714332 [0.9390038] [0.13865872]\n",
            "540 0.0025170615 [0.9418704] [0.13214228]\n",
            "560 0.002286037 [0.94460225] [0.1259321]\n",
            "580 0.0020762135 [0.9472057] [0.12001377]\n",
            "600 0.001885655 [0.949687] [0.11437355]\n",
            "620 0.001712574 [0.9520514] [0.10899838]\n",
            "640 0.0015553888 [0.95430475] [0.10387588]\n",
            "660 0.0014126339 [0.9564523] [0.09899414]\n",
            "680 0.0012829704 [0.9584989] [0.09434177]\n",
            "700 0.0011652192 [0.96044934] [0.08990802]\n",
            "720 0.0010582684 [0.9623081] [0.08568266]\n",
            "740 0.00096113543 [0.96407944] [0.08165589]\n",
            "760 0.0008729229 [0.96576756] [0.07781836]\n",
            "780 0.0007928 [0.96737635] [0.07416118]\n",
            "800 0.0007200328 [0.9689095] [0.0706759]\n",
            "820 0.000653943 [0.9703707] [0.0673544]\n",
            "840 0.0005939253 [0.9717632] [0.06418898]\n",
            "860 0.0005394094 [0.9730902] [0.06117233]\n",
            "880 0.0004899009 [0.9743548] [0.05829747]\n",
            "900 0.00044493438 [0.97556007] [0.05555772]\n",
            "920 0.0004040997 [0.97670865] [0.05294671]\n",
            "940 0.00036701027 [0.97780323] [0.05045844]\n",
            "960 0.0003333233 [0.9788463] [0.04808708]\n",
            "980 0.00030273176 [0.9798405] [0.04582722]\n",
            "1000 0.000274947 [0.980788] [0.04367353]\n",
            "1020 0.0002497088 [0.9816909] [0.041621]\n",
            "1040 0.00022678993 [0.98255134] [0.03966495]\n",
            "1060 0.00020597332 [0.9833714] [0.03780082]\n",
            "1080 0.00018706878 [0.98415285] [0.03602431]\n",
            "1100 0.00016989867 [0.9848976] [0.03433131]\n",
            "1120 0.00015430608 [0.9856074] [0.03271785]\n",
            "1140 0.00014014351 [0.98628384] [0.03118022]\n",
            "1160 0.00012727921 [0.98692834] [0.02971487]\n",
            "1180 0.00011559722 [0.9875427] [0.02831841]\n",
            "1200 0.00010498746 [0.9881281] [0.02698755]\n",
            "1220 9.5352334e-05 [0.988686] [0.02571925]\n",
            "1240 8.659961e-05 [0.9892177] [0.02451058]\n",
            "1260 7.865186e-05 [0.98972446] [0.02335867]\n",
            "1280 7.1431525e-05 [0.99020755] [0.02226087]\n",
            "1300 6.4875065e-05 [0.9906677] [0.02121462]\n",
            "1320 5.8920745e-05 [0.9911063] [0.02021759]\n",
            "1340 5.351237e-05 [0.9915242] [0.01926741]\n",
            "1360 4.8601098e-05 [0.99192256] [0.01836191]\n",
            "1380 4.4140474e-05 [0.9923021] [0.01749898]\n",
            "1400 4.0089308e-05 [0.9926639] [0.0166766]\n",
            "1420 3.6410183e-05 [0.9930087] [0.01589288]\n",
            "1440 3.3067565e-05 [0.9933373] [0.01514597]\n",
            "1460 3.003322e-05 [0.9936504] [0.01443417]\n",
            "1480 2.727586e-05 [0.9939488] [0.01375582]\n",
            "1500 2.477278e-05 [0.99423313] [0.01310935]\n",
            "1520 2.2498934e-05 [0.99450415] [0.01249328]\n",
            "1540 2.0434445e-05 [0.9947624] [0.01190616]\n",
            "1560 1.855863e-05 [0.9950086] [0.01134664]\n",
            "1580 1.685491e-05 [0.9952432] [0.01081339]\n",
            "1600 1.5307976e-05 [0.99546677] [0.01030519]\n",
            "1620 1.3903064e-05 [0.9956798] [0.00982088]\n",
            "1640 1.2627331e-05 [0.9958828] [0.00935933]\n",
            "1660 1.14681925e-05 [0.9960763] [0.0089195]\n",
            "1680 1.0415165e-05 [0.9962607] [0.00850031]\n",
            "1700 9.459502e-06 [0.9964364] [0.00810082]\n",
            "1720 8.5912425e-06 [0.9966039] [0.00772012]\n",
            "1740 7.802494e-06 [0.9967635] [0.0073573]\n",
            "1760 7.086795e-06 [0.99691564] [0.00701154]\n",
            "1780 6.4360993e-06 [0.99706054] [0.00668204]\n",
            "1800 5.8452474e-06 [0.9971987] [0.00636801]\n",
            "1820 5.309277e-06 [0.9973303] [0.00606875]\n",
            "1840 4.8218835e-06 [0.9974558] [0.00578357]\n",
            "1860 4.3793166e-06 [0.9975753] [0.00551178]\n",
            "1880 3.977208e-06 [0.9976893] [0.00525276]\n",
            "1900 3.61227e-06 [0.9977979] [0.00500588]\n",
            "1920 3.2804837e-06 [0.9979014] [0.00477062]\n",
            "1940 2.979565e-06 [0.99799997] [0.00454644]\n",
            "1960 2.706014e-06 [0.99809396] [0.00433279]\n",
            "1980 2.4577082e-06 [0.99818355] [0.00412919]\n",
            "2000 2.232258e-06 [0.9982689] [0.00393515]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Fit the line\\nfor step in range(2001):\\n  sess.run(train)\\n  if step % 20 == 0:\\n    print(step, sess.run(cost), sess.run(W), sess.run(b))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvhrq_wxKkVs",
        "colab_type": "code",
        "outputId": "dd2eb614-9542-4ddc-86cb-43d230d8e8a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "X = tf.placeholder(tf.float32, shape=[None])\n",
        "Y = tf.placeholder(tf.float32, shape=[None])\n",
        "\n",
        "hypothesis = X * W + b\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(2001):\n",
        "  cost_val, W_val, b_val, _ = sess.run([cost, W, b, train],\n",
        "    feed_dict={X:[1,2,3,4,5], Y:[2.1, 3.1, 4.1, 5.1, 6.1]})\n",
        "  if step % 20 == 0:\n",
        "    print(step, cost_val, W_val, b_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 83.96294 [-0.44505155] [-1.2734768]\n",
            "20 0.5391868 [1.4655435] [-0.61504465]\n",
            "40 0.46939096 [1.443257] [-0.5004533]\n",
            "60 0.4099225 [1.4142648] [-0.39562804]\n",
            "80 0.3579885 [1.3871344] [-0.2976782]\n",
            "100 0.31263405 [1.3617806] [-0.20614313]\n",
            "120 0.2730257 [1.3380873] [-0.12060278]\n",
            "140 0.2384353 [1.3159457] [-0.04066447]\n",
            "160 0.20822737 [1.2952542] [0.03403853]\n",
            "180 0.18184637 [1.2759178] [0.10384915]\n",
            "200 0.15880787 [1.2578477] [0.16908783]\n",
            "220 0.13868809 [1.2409611] [0.23005402]\n",
            "240 0.12111728 [1.2251803] [0.2870275]\n",
            "260 0.105772614 [1.210433] [0.34026977]\n",
            "280 0.092372075 [1.1966517] [0.39002514]\n",
            "300 0.08066925 [1.1837728] [0.43652192]\n",
            "320 0.07044903 [1.1717373] [0.4799736]\n",
            "340 0.061523624 [1.1604902] [0.5205797]\n",
            "360 0.05372907 [1.1499795] [0.55852634]\n",
            "380 0.04692197 [1.1401572] [0.59398794]\n",
            "400 0.04097733 [1.1309781] [0.6271271]\n",
            "420 0.035785787 [1.1224003] [0.65809596]\n",
            "440 0.03125199 [1.1143843] [0.68703663]\n",
            "460 0.02729262 [1.1068932] [0.7140819]\n",
            "480 0.023834813 [1.0998925] [0.7393559]\n",
            "500 0.020815104 [1.0933505] [0.7629748]\n",
            "520 0.018178016 [1.0872369] [0.78504694]\n",
            "540 0.01587495 [1.0815235] [0.80567366]\n",
            "560 0.013863725 [1.0761846] [0.8249494]\n",
            "580 0.012107299 [1.0711951] [0.8429626]\n",
            "600 0.010573385 [1.0665326] [0.8597962]\n",
            "620 0.009233853 [1.0621754] [0.8755272]\n",
            "640 0.008063979 [1.0581034] [0.89022803]\n",
            "660 0.0070423246 [1.0542983] [0.9039662]\n",
            "680 0.0061501144 [1.050742] [0.9168046]\n",
            "700 0.005370938 [1.0474191] [0.9288022]\n",
            "720 0.0046904944 [1.0443135] [0.94001406]\n",
            "740 0.004096249 [1.0414114] [0.95049155]\n",
            "760 0.0035772803 [1.0386994] [0.960283]\n",
            "780 0.0031240652 [1.0361649] [0.9694331]\n",
            "800 0.0027282715 [1.0337964] [0.9779841]\n",
            "820 0.0023826144 [1.0315831] [0.98597497]\n",
            "840 0.0020807565 [1.0295147] [0.9934426]\n",
            "860 0.0018171361 [1.0275817] [1.000421]\n",
            "880 0.0015869222 [1.0257754] [1.0069424]\n",
            "900 0.0013858742 [1.0240873] [1.013037]\n",
            "920 0.0012103016 [1.0225099] [1.018732]\n",
            "940 0.0010569601 [1.0210356] [1.0240544]\n",
            "960 0.0009230537 [1.0196581] [1.029028]\n",
            "980 0.00080610515 [1.0183705] [1.033676]\n",
            "1000 0.000703982 [1.0171676] [1.0380198]\n",
            "1020 0.00061479135 [1.0160432] [1.0420789]\n",
            "1040 0.00053690025 [1.0149925] [1.0458722]\n",
            "1060 0.00046888104 [1.0140105] [1.049417]\n",
            "1080 0.00040947134 [1.013093] [1.0527298]\n",
            "1100 0.00035759795 [1.0122355] [1.0558257]\n",
            "1120 0.00031228623 [1.0114342] [1.0587188]\n",
            "1140 0.00027272193 [1.0106854] [1.0614225]\n",
            "1160 0.00023817396 [1.0099857] [1.0639488]\n",
            "1180 0.00020799872 [1.0093316] [1.0663098]\n",
            "1200 0.00018164542 [1.0087204] [1.0685163]\n",
            "1220 0.00015863145 [1.0081493] [1.0705782]\n",
            "1240 0.00013853433 [1.0076157] [1.072505]\n",
            "1260 0.000120984114 [1.0071169] [1.0743057]\n",
            "1280 0.00010565562 [1.0066508] [1.0759884]\n",
            "1300 9.2269984e-05 [1.0062152] [1.077561]\n",
            "1320 8.0579244e-05 [1.0058082] [1.0790305]\n",
            "1340 7.0369635e-05 [1.0054277] [1.080404]\n",
            "1360 6.145424e-05 [1.0050724] [1.0816873]\n",
            "1380 5.3666958e-05 [1.0047401] [1.0828867]\n",
            "1400 4.6868143e-05 [1.0044297] [1.0840074]\n",
            "1420 4.0932464e-05 [1.0041397] [1.0850545]\n",
            "1440 3.5747034e-05 [1.0038686] [1.0860332]\n",
            "1460 3.1217114e-05 [1.0036151] [1.0869479]\n",
            "1480 2.7262713e-05 [1.0033784] [1.0878026]\n",
            "1500 2.3809163e-05 [1.0031571] [1.0886016]\n",
            "1520 2.0791851e-05 [1.0029504] [1.089348]\n",
            "1540 1.8159026e-05 [1.0027572] [1.0900453]\n",
            "1560 1.5858324e-05 [1.0025767] [1.0906974]\n",
            "1580 1.3849203e-05 [1.0024079] [1.0913066]\n",
            "1600 1.2094586e-05 [1.0022502] [1.0918759]\n",
            "1620 1.0562317e-05 [1.002103] [1.0924078]\n",
            "1640 9.224166e-06 [1.0019652] [1.092905]\n",
            "1660 8.055321e-06 [1.0018365] [1.0933697]\n",
            "1680 7.03536e-06 [1.0017163] [1.0938039]\n",
            "1700 6.144339e-06 [1.0016037] [1.0942097]\n",
            "1720 5.3656536e-06 [1.0014988] [1.0945888]\n",
            "1740 4.687031e-06 [1.0014007] [1.0949428]\n",
            "1760 4.0925993e-06 [1.001309] [1.095274]\n",
            "1780 3.574085e-06 [1.0012233] [1.0955834]\n",
            "1800 3.1219122e-06 [1.0011432] [1.0958725]\n",
            "1820 2.7262645e-06 [1.0010684] [1.0961428]\n",
            "1840 2.3810403e-06 [1.0009985] [1.0963955]\n",
            "1860 2.0793905e-06 [1.0009329] [1.0966314]\n",
            "1880 1.815945e-06 [1.000872] [1.0968521]\n",
            "1900 1.5861657e-06 [1.0008149] [1.097058]\n",
            "1920 1.3851843e-06 [1.0007615] [1.0972506]\n",
            "1940 1.2099865e-06 [1.0007117] [1.0974306]\n",
            "1960 1.0567439e-06 [1.0006652] [1.0975988]\n",
            "1980 9.2280334e-07 [1.0006216] [1.0977559]\n",
            "2000 8.0598363e-07 [1.0005809] [1.0979028]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK-Qp-FaOyw3",
        "colab_type": "text"
      },
      "source": [
        "- W = 1, cost(W) = ?\n",
        "$ {1 \\over 3}((1*1-1)^2+(1*2-2)^2+(1*3-3)^2) $\n",
        "- W = 0, cost(W) = 4.67\n",
        "$ {1 \\over 3}((0*1-1)^2+(0*2-2)^2+(0*3-3)^2) $\n",
        "- W = 2, cost(W) = 4.67\n",
        "$ {1 \\over 3}((2*1-1)^2+(2*2-2)^2+(2*3-3)^2) $\n",
        "\n",
        "- Gradient descent algorithm (경사하강법)\n",
        "  - cost(W, b) 를 최소화 할때 사용하는 알고리즘\n",
        "  - 그래프 상 아무 값에서 시작 후 경사도를 반복 계산 해서 최소점을 찾음\n",
        "\n",
        "- Formal definition\n",
        "$ cost(W) = {1 \\over 2m}\\sum_{i=1}^m(H(x^{(i)}) - y^{(i)})^2 $\n",
        "$ W := W - \\alpha{\\partial\\over\\partial W}cost(W) $\n",
        "- alpha = learning rate (상수)\n",
        "  - 그래프에서 최소화된 W를 찾기위한 미분 할 때 바깥으로 튀지 않고 안쪽으로 (-는 +쪽으로, +는 -쪽으로) 옮겨올 수 있도록 튜닝하는 값\n",
        "\n",
        "  ![대체 텍스트](https://t1.daumcdn.net/cfile/tistory/223B87385890215809)\n",
        "\n",
        "$ W := W - \\alpha{1 \\over m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})x^{(i)} $\n",
        "- 이건 다시 볼 필요 있음.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh14rwiXguXc",
        "colab_type": "code",
        "outputId": "bbb5de88-d6a5-4d41-852b-9426d5e8ed38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = [1, 2, 3]\n",
        "Y = [1, 2, 3]\n",
        "\n",
        "W = tf.placeholder(tf.float32)\n",
        "\n",
        "hypothesis = X * W\n",
        "\n",
        "# cost/loss function\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "\n",
        "W_val = []\n",
        "cost_val = []\n",
        "\n",
        "sess = tf.Session()\n",
        "# init global variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(-30, 50):\n",
        "  feed_W = i * 0.1\n",
        "  curr_cost, curr_W = sess.run([cost, W], feed_dict={W: feed_W})\n",
        "  W_val.append(curr_W)\n",
        "  cost_val.append(curr_cost)\n",
        "\n",
        "# show the cost function\n",
        "plt.plot(W_val, cost_val)\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC1wppdzU/wfMjZcCPi6P6AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKKSr7VUsd2S",
        "colab_type": "code",
        "outputId": "591ab08f-d557-4fb5-d079-d19871614430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        }
      },
      "source": [
        "X = [1, 2, 3]\n",
        "Y = [1, 2, 3]\n",
        "\n",
        "# set wrong modal weights\n",
        "#W = tf.Variable(5.0)\n",
        "W = tf.Variable(-5.0)\n",
        "\n",
        "# linear model\n",
        "hypothesis = X * W\n",
        "\n",
        "# cost/loss function\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "\n",
        "# minimize : Gradient Descent Magic\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "# launch the graph in a session\n",
        "sess = tf.Session()\n",
        "# init global variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(50):\n",
        "  print(step, sess.run(W))\n",
        "  sess.run(train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 -5.0\n",
            "1 0.6000001\n",
            "2 0.97333336\n",
            "3 0.99822223\n",
            "4 0.9998815\n",
            "5 0.9999921\n",
            "6 0.99999946\n",
            "7 1.0\n",
            "8 1.0\n",
            "9 1.0\n",
            "10 1.0\n",
            "11 1.0\n",
            "12 1.0\n",
            "13 1.0\n",
            "14 1.0\n",
            "15 1.0\n",
            "16 1.0\n",
            "17 1.0\n",
            "18 1.0\n",
            "19 1.0\n",
            "20 1.0\n",
            "21 1.0\n",
            "22 1.0\n",
            "23 1.0\n",
            "24 1.0\n",
            "25 1.0\n",
            "26 1.0\n",
            "27 1.0\n",
            "28 1.0\n",
            "29 1.0\n",
            "30 1.0\n",
            "31 1.0\n",
            "32 1.0\n",
            "33 1.0\n",
            "34 1.0\n",
            "35 1.0\n",
            "36 1.0\n",
            "37 1.0\n",
            "38 1.0\n",
            "39 1.0\n",
            "40 1.0\n",
            "41 1.0\n",
            "42 1.0\n",
            "43 1.0\n",
            "44 1.0\n",
            "45 1.0\n",
            "46 1.0\n",
            "47 1.0\n",
            "48 1.0\n",
            "49 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}