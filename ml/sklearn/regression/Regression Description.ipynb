{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression Description.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjvaskSOVP5i",
        "colab_type": "text"
      },
      "source": [
        "회귀\n",
        "- 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계학 기법\n",
        "- 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법\n",
        "- 주어진 피처와 결정 값 데이터 기반에서 학습을 통해 최적의 회귀 계수를 찾아내는 것\n",
        "  - Y는 종속변수, X는 독립 변수(feature), W는 이 독립변수의 값에 영향을 미치는 회귀 계수(Regression coefficients)\n",
        "$$ Y = W_1*X_1 + W_2*X_2+...+W_n*X_n $$\n",
        "\n",
        "회귀 유형\n",
        "- 회귀 계수의 선형/비선형 여부, 독립변수의 개수, 종속변수의 개수에 따라 나눌수 있음\n",
        "  - 회귀 계수가 '선형이냐 아니냐' 에 따라 선형회귀, 비선형회귀로 나뉨 \n",
        "  - 독립변수의 개수가 '한개인지 여러 개인지'에 따라 단일회귀, 다중회귀로 나뉨\n",
        "\n",
        "선형 회귀 종류\n",
        "- 일반 선형 회귀 : 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화할 수 있또록 회귀 계수를 최적화 하며, 규제(Regularization)를 적용하지 않은 모델\n",
        "- 릿지(Ridge) : 선형 회귀에 L2 규제를 추가한 회귀 모델\n",
        "- 라쏘(Lasso) : 선형 회귀에 L1 규제를 적용한 방식\n",
        "- 엘라스틱넷(ElasticNet) : L2, L1 규제를 함께 결합한 모델\n",
        "- 로지스틱 회귀(Logistic Regression) : 분류에 사용되는 선형 모델\n",
        "\n",
        "최적의 회귀 모델을 만든다는 것은 전체 데이터의 잔차(오류값) 합이 최소가 되는 모델을 만든다는 의미. 동시에 오류 값 합이 최소가 될 수 있는 최적의 회귀 계수를 찾는 의미(절편과 기울기를 찾는 의미)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1izWJzvbMCB",
        "colab_type": "text"
      },
      "source": [
        "RSS(Residual Sum of Squares) 기반의 회귀 오류 측정\n",
        "- RSS : 오류 값의 제곱을 구해서 더하는 방식. 미분 등의 계산을 편리하게 하기 위해서 RSS 방식으로 오류 합을 구함 \n",
        "$$ERROR^2 = RSS$$\n",
        "$$ RSS = (1번 주택가격 - (w0 + w1 * 1번 주택크기)^2 + (2번 주택가격 - (w0 + w1 * 2번 주택크기)^2 + ...(모든 학습 데이터에 대해 RSS 수행) $$\n",
        "\n",
        "RSS의 이해\n",
        "- RSS는 이제 변수가 $W_0, W_1$인 식으로 표현할 수 있으며, 이 RSS를 최소로 하는 $W_0, W_1$, 즉 회귀 계수를 학습을 통해서 찾는 것이 핵심.\n",
        "- 회귀식의 독립변수 X, 종속변수 Y가 중심 변수가 아니라 w 변수(회귀 계수)가 중심 변수임을 인지하는 것이 매우 중요(학습 데이터로 입력되는 독립 변수와 종속 변수는 RSS에서 모두 상수로 간주함)\n",
        "- 일반적으로 RSS는 학습 데이터의 건수로 나누어서 정규화된 식으로 표현됨\n",
        "$$ RSS(w_0, w_1) = \\frac{1}{N}\\sum_{i=1}^N(y_i-(w_0+w1*x_i))^2  $$\n",
        "$$ (i는 1부터 학습 데이터의 총 건수 N까지) $$\n",
        "\n",
        "RSS : 회귀의 비용 함수(Cost function)\n",
        "- 회귀에서 이 RSS는 비용(Cost)이며, w 변수(회귀 계수)로 구성되는 RSS를 비용함수라고 함. 데이터를 계속 학습하면서 이 비용 함수가 반환하는 값(즉, 오류값)을 지속해서 감소시키고 최종적으로 더 이상 감소하지 않는 최소의 오류 값을 구하는 것. 비용 함수를 손실함수(loss function)라고도 함.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdeet-ZBjOkq",
        "colab_type": "text"
      },
      "source": [
        "비용 최소화 하기 - 경사 하강법(Gradient Descent)\n",
        "- W 파라미터의 개수가 적다면 고차원 방정식으로 비용 함수가 최소가 되는 W변수값을 도출 할 수 있겠지만, W 파라미터가 많으면 고차원 방정식을 동원하더라도 해결하기 어려움. 경사 하강법은 이런 고차원 방정식에 대한 문제를 해결해 주면서 비용 함수 RSS를 최소화 하는 방법을 직관적으로 제공\n",
        "  - '점진적으로' 반복적인 계산을 통해 W 파라미터 값을 업데이트하면서 오류 값이 최소가 되는 W 파라미터를 구하는 방식\n",
        "- 반복적으로 비용 함수의 반환 값, 즉 예측값과 실제 값의 차이가 작아지는 방향성을 가지고 W파라미터를 지속해서 보정해 나감\n",
        "- 최초 오류 값이 100이었다면 두 번째 오류 값은 100보다 작은 90, 세 번째는 80과 같은 방식으로 지속해서 오류를 감소시키는 방향으로 W 값을 계속 업데이트\n",
        "- 오류 값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그때의 W 값을 최적 파라미터로 반환\n",
        "- '어떻게 하면 오류가 작아지는 방향으로 W 값을 보정할 수 있을까?'\n",
        "\n",
        "미분을 통해 비용 함수의 최소값 찾기\n",
        "- 비용함수가 포물선 형태의 2차 함수라면 경사 하강법은 최초 w에서부터 미분을 적용한 뒤 이 미분 값을 계속 감소하는 방향으로 순차적으로 w를 업데이트함\n",
        "- 더 이상 미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용 함수가 최소인 지점으로 간주하고 그때의 w를 반환"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY8ve3O3mHd8",
        "colab_type": "text"
      },
      "source": [
        "RSS의 편미분\n",
        "- R(w)를 미분해 미분 함수의 최소값을 구해야 하는데, R(W)는 두 개의 w 파라미터인 w0와 w1을 각각 가지고 있기 때문에 일반적인 미분을 적용할 수가 없고, w0, w1 각 변수에 편미분을 적용해야 함. R(w)를 최소화 하는 w0와 w1의 값은 각각 r(w)를 w0, w1으로 순차적으로 편미분을 수행해 얻을 수 있음\n",
        "\n",
        "$$ \\frac{\\partial R(w)}{\\partial w_1} = \\frac{2}{N}\\sum_{i=1}^N-x_t*(y_i-(w_0+w_1x_i)) = -\\frac{2}{N}\\sum_{i=1}^Nx_i*(실제값_i-예측값_i) $$\n",
        "$$ \\frac{\\partial R(w)}{\\partial w_0} = \\frac{2}{N}\\sum_{i=1}^N-(y_i-(w_0+w_1x_i)) = -\\frac{2}{N}\\sum_{i=1}^N(실제값_i-예측값_i) $$\n",
        "\n",
        "- 편미분 : 다변수 함수에 대하여 그 중 하나의 변수에 주목하고 나머지 변수의 값을 고정시켜 놓고 그 변수로 미분하는 일\n",
        "  - $x^2$ 미분 -> $2x$\n",
        "  - $2x$ 미분 -> 2\n",
        "  - 3 미분 -> 0\n",
        "  - $ (A + B)^2 = A^2 + 2AB + B^2 $\n",
        "\n",
        "$$ R(w_0, w_1) = \\frac{1}{N}\\sum_{i=1}^N(y_i - (w_0+w_1x_i))^2  $$\n",
        "$$ = \\frac{1}{N}\\sum_{i=1}^N{y_i}^2 - 2y_i(w_0+w_1x_i) + (w_0+w_1x_i)^2  $$\n",
        "$$ = \\frac{1}{N}\\sum_{i=1}^N {y_i}^2 - 2y_i(w_0+w_1x_i) + {w_0}^2 + 2w_0w_1x_i + {w_1}^2{x_i}^2 $$\n",
        "\n",
        "(1) $w_1$ 으로 편미분\n",
        "$$ \\frac{\\partial R(w_0,w_1)}{\\partial w_1} = \\frac{1}{N}\\sum_{i=1}^N-2y_ix_i + 2w_0x_i + 2w_1{x_i}^2 $$\n",
        "$$ = \\frac{2}{N}\\sum_{i=1}^N-x_i*(y_i-w_0-w_1x_i) $$\n",
        "$$ = \\frac{2}{N}\\sum_{i=1}^N-x_i*(y_i-(w_0+w_1x_i)) $$\n",
        "$$ = \\frac{-2}{N}\\sum_{i=1}^Nx_i*(y_i-(w_0+w_1x_i)) $$\n",
        "$$ = \\frac{2}{N}\\sum_{i=1}^Nx_i*(실제값_i-예측값_i) $$\n",
        "\n",
        "(2) $w_0$ 으로 편미분\n",
        "$$ \\frac{\\partial R(w_0,w_1)}{\\partial w_0} = \\frac{1}{N}\\sum_{i=1}^N-2y_i+2w_0+2w_1x_i $$\n",
        "$$ = \\frac{2}{N}\\sum_{i=1}^N-(y-(w_0+w_1x_i)) $$\n",
        "$$ = \\frac{-2}{N}\\sum_{i=1}^N(실제값_i-예측값_i) $$\n",
        "\n",
        "- w1, w0의 편미분 결과값을 반복적으로 보정하면서 w1, w0 값을 업데이트하면 비용함수 R(W)가 최소가 되는 w1, w0값을 구할 수 있음. 실제로는 위 편미분 값이 너무 클 수 있기 때문에 보정계수 n을 곱하는데, 이를 '학습률'이라고 함\n",
        "  - 새로운 $w_1$ = 이전 $w_1 - \\eta\\frac{2}{N}\\sum_{i=1}^Nx_i*(실제값_i-예측값_i)$\n",
        "  - 새로운 $w_0$ = 이전 $w_0 - \\eta\\frac{2}{N}\\sum_{i=1}^N(실제값_i-예측값_i)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq6hUC9IqmVN",
        "colab_type": "text"
      },
      "source": [
        "경사 하강법 수행 프로세스\n",
        "\n",
        "- Step 1 : $w_1, w_0$를 임의의 값으로 설정하고 첫 비용 함수의 값을 계산함\n",
        "- Step 2 : $w_1$을 $w_1 - \\eta\\frac{2}{N}\\sum_{i=1}^Nx_i*(실제값_i-예측값_i)$, $w_0$을 $w_0 - \\eta\\frac{2}{N}\\sum_{i=1}^N(실제값_i-예측값_i)$으로 업데이트 한 후 다시 비용 함수의 값을 계산함\n",
        "- Step 3 : 비용 함수의 값이 감소했으면 다시 Step 2를 반복함. 더 이상 비용 함수의 값이 감소하지 않으면 그때의 $w_1,w_0$를 구하고 반복을 중지함"
      ]
    }
  ]
}