{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Initialization\n",
    "\n",
    "nn xavier 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f61940b5df0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/', train=True, \n",
    "                          transform=transforms.ToTensor(), download=False)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/', train=False, \n",
    "                        transform=transforms.ToTensor(), download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size,\n",
    "                                         shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn layer\n",
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0215, -0.0894,  0.0598,  ...,  0.0200,  0.0203,  0.1212],\n",
       "        [ 0.0078,  0.1378,  0.0920,  ...,  0.0975,  0.1458, -0.0302],\n",
       "        [ 0.1270, -0.1296,  0.1049,  ...,  0.0124,  0.1173, -0.0901],\n",
       "        ...,\n",
       "        [ 0.0661, -0.1025,  0.1437,  ...,  0.0784,  0.0977, -0.0396],\n",
       "        [ 0.0430, -0.1274, -0.0134,  ..., -0.0582,  0.1201,  0.1479],\n",
       "        [-0.1433,  0.0200, -0.0568,  ...,  0.0787,  0.0428, -0.0036]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xavier initialization\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=  0.249939442\n",
      "Epoch: 0002 cost=  0.094024383\n",
      "Epoch: 0003 cost=  0.060639817\n",
      "Epoch: 0004 cost=  0.043911427\n",
      "Epoch: 0005 cost=  0.032522038\n",
      "Epoch: 0006 cost=  0.026136832\n",
      "Epoch: 0007 cost=  0.020799598\n",
      "Epoch: 0008 cost=  0.017474931\n",
      "Epoch: 0009 cost=  0.017735897\n",
      "Epoch: 0010 cost=  0.015532637\n",
      "Epoch: 0011 cost=  0.011290390\n",
      "Epoch: 0012 cost=  0.013556553\n",
      "Epoch: 0013 cost=  0.009562553\n",
      "Epoch: 0014 cost=  0.009786872\n",
      "Epoch: 0015 cost=  0.007886958\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # label is not one-hot encoded\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / total_batch\n",
    "    \n",
    "    print('Epoch:', '%04d'%(epoch+1), 'cost= ', '{:.9f}'.format(avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9821000099182129\n",
      "Label:  8\n",
      "Prediction:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "# Test the model using test sets\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28*28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy: ', accuracy.item())\n",
    "    \n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r+1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r+1].to(device)\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f61940b5df0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/', train=True, \n",
    "                         transform=transforms.ToTensor(), download=False)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/', train=False,\n",
    "                        transform=transforms.ToTensor(), download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset laoder\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size,\n",
    "                                         shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
    "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear5 = torch.nn.Linear(512, 0, bias=True)\n",
    "relu = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([], size=(0, 512), requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xavier initialization\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "torch.nn.init.xavier_uniform_(linear4.weight)\n",
    "torch.nn.init.xavier_uniform_(linear5.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3, relu, linear4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost = 0.335444301\n",
      "Epoch:  0002 cost = 0.017243179\n",
      "Epoch:  0003 cost = 0.007788241\n",
      "Epoch:  0004 cost = 0.008319030\n",
      "Epoch:  0005 cost = 0.008062833\n",
      "Epoch:  0006 cost = 0.008930523\n",
      "Epoch:  0007 cost = 0.004767351\n",
      "Epoch:  0008 cost = 0.004186654\n",
      "Epoch:  0009 cost = 0.010489789\n",
      "Epoch:  0010 cost = 0.003139920\n",
      "Epoch:  0011 cost = 0.008709079\n",
      "Epoch:  0012 cost = 0.003905431\n",
      "Epoch:  0013 cost = 0.006266262\n",
      "Epoch:  0014 cost = 0.005823899\n",
      "Epoch:  0015 cost = 0.003512780\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # label is not one-hot encoded\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / total_batch\n",
    "    \n",
    "    print('Epoch: ', '%04d'%(epoch+1), 'cost =', '{:.9f}'.format(avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9839000105857849\n",
      "Label:  5\n",
      "Prediction:  5\n"
     ]
    }
   ],
   "source": [
    "# Test the model using test sets\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "    \n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r+1].view(-1, 28*28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r+1].to(device)\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop-out\n",
    "\n",
    "nn dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "keep_prob = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size,\n",
    "                                         shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
    "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=1 - keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0791, -0.0094,  0.0358,  ...,  0.0774,  0.0999,  0.0604],\n",
       "        [-0.0215, -0.0271,  0.1056,  ..., -0.0275, -0.0523, -0.0696],\n",
       "        [-0.0010, -0.0021,  0.0241,  ...,  0.0242, -0.0450,  0.0677],\n",
       "        ...,\n",
       "        [ 0.0492,  0.0851,  0.0222,  ...,  0.0762,  0.0951,  0.0509],\n",
       "        [-0.0665,  0.1007, -0.0244,  ...,  0.1043, -0.0374,  0.0935],\n",
       "        [-0.0456, -0.0044,  0.0308,  ...,  0.1072, -0.0781,  0.0756]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xavier initialization\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "torch.nn.init.xavier_uniform_(linear4.weight)\n",
    "torch.nn.init.xavier_uniform_(linear5.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = torch.nn.Sequential(linear1, relu, dropout,\n",
    "                           linear2, relu, dropout,\n",
    "                           linear3, relu, dropout,\n",
    "                           linear4, relu, dropout,\n",
    "                           linear5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device) # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.311255127\n",
      "Epoch: 0002 cost = 0.143018186\n",
      "Epoch: 0003 cost = 0.112317204\n",
      "Epoch: 0004 cost = 0.094562627\n",
      "Epoch: 0005 cost = 0.082408361\n",
      "Epoch: 0006 cost = 0.075918607\n",
      "Epoch: 0007 cost = 0.069509819\n",
      "Epoch: 0008 cost = 0.063819580\n",
      "Epoch: 0009 cost = 0.059796285\n",
      "Epoch: 0010 cost = 0.054471701\n",
      "Epoch: 0011 cost = 0.050672572\n",
      "Epoch: 0012 cost = 0.052874759\n",
      "Epoch: 0013 cost = 0.047587086\n",
      "Epoch: 0014 cost = 0.043440599\n",
      "Epoch: 0015 cost = 0.041974988\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "model.train() # set the model to train mode (dropout=True)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # label is not one-hot encoded\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' %(epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.978600025177002\n",
      "Label:  9\n",
      "Prediction:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "with torch.no_grad():\n",
    "    model.eval() # set the model to evaluation mode (dropout=False)\n",
    "    \n",
    "    # Test the model using test sets.\n",
    "    X_test = mnist_test.test_data.view(-1, 28*28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "    \n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r+1].view(-1, 28*28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r+1].to(device)\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f61940b5df0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/', train=True, \n",
    "                         transform=transforms.ToTensor(), download=False)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/', train=False,\n",
    "                        transform=transforms.ToTensor(), download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size,\n",
    "                                          shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test, batch_size=batch_size,\n",
    "                                         shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 32, bias=True)\n",
    "linear2 = torch.nn.Linear(32, 32, bias=True)\n",
    "linear3 = torch.nn.Linear(32, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "bn1 = torch.nn.BatchNorm1d(32)\n",
    "bn2 = torch.nn.BatchNorm1d(32)\n",
    "\n",
    "nn_linear1 = torch.nn.Linear(784, 32, bias=True)\n",
    "nn_linear2 = torch.nn.Linear(32, 32, bias=True)\n",
    "nn_linear3 = torch.nn.Linear(32, 10, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "bn_model = torch.nn.Sequential(linear1, relu, bn1, linear2, relu, bn2, \n",
    "                               linear3).to(device)\n",
    "nn_model = torch.nn.Sequential(nn_linear1, relu, nn_linear2, relu, nn_linear3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device) # Softmax is internally computed.\n",
    "bn_optimizer = torch.optim.Adam(bn_model.parameters(), lr=learning_rate)\n",
    "nn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1-TRAIN] Batchnorm Loss(Acc): bn_loss:0.11799(bn_acc:0.96)             vs No Batchnorm Loss(Acc): nn_loss:0.15207(nn_acc:0.96)\n",
      "[Epoch 1-VALID] Batchnorm Loss(Acc): bn_loss:0.13781(bn_acc:0.96)         vs No Batchnorm Loss(Acc): nn_loss:0.17707(nn_acc:0.95)\n",
      "\n",
      "[Epoch 2-TRAIN] Batchnorm Loss(Acc): bn_loss:0.10077(bn_acc:0.97)             vs No Batchnorm Loss(Acc): nn_loss:0.17726(nn_acc:0.95)\n",
      "[Epoch 2-VALID] Batchnorm Loss(Acc): bn_loss:0.11951(bn_acc:0.96)         vs No Batchnorm Loss(Acc): nn_loss:0.20659(nn_acc:0.94)\n",
      "\n",
      "[Epoch 3-TRAIN] Batchnorm Loss(Acc): bn_loss:0.10121(bn_acc:0.97)             vs No Batchnorm Loss(Acc): nn_loss:0.13749(nn_acc:0.96)\n",
      "[Epoch 3-VALID] Batchnorm Loss(Acc): bn_loss:0.12749(bn_acc:0.96)         vs No Batchnorm Loss(Acc): nn_loss:0.16713(nn_acc:0.95)\n",
      "\n",
      "[Epoch 4-TRAIN] Batchnorm Loss(Acc): bn_loss:0.08971(bn_acc:0.97)             vs No Batchnorm Loss(Acc): nn_loss:0.14323(nn_acc:0.96)\n",
      "[Epoch 4-VALID] Batchnorm Loss(Acc): bn_loss:0.11640(bn_acc:0.96)         vs No Batchnorm Loss(Acc): nn_loss:0.19432(nn_acc:0.95)\n",
      "\n",
      "[Epoch 5-TRAIN] Batchnorm Loss(Acc): bn_loss:0.07653(bn_acc:0.98)             vs No Batchnorm Loss(Acc): nn_loss:0.13808(nn_acc:0.96)\n",
      "[Epoch 5-VALID] Batchnorm Loss(Acc): bn_loss:0.10885(bn_acc:0.97)         vs No Batchnorm Loss(Acc): nn_loss:0.19832(nn_acc:0.95)\n",
      "\n",
      "[Epoch 6-TRAIN] Batchnorm Loss(Acc): bn_loss:0.07517(bn_acc:0.98)             vs No Batchnorm Loss(Acc): nn_loss:0.11981(nn_acc:0.97)\n",
      "[Epoch 6-VALID] Batchnorm Loss(Acc): bn_loss:0.10827(bn_acc:0.97)         vs No Batchnorm Loss(Acc): nn_loss:0.17893(nn_acc:0.96)\n",
      "\n",
      "[Epoch 7-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06998(bn_acc:0.98)             vs No Batchnorm Loss(Acc): nn_loss:0.11741(nn_acc:0.97)\n",
      "[Epoch 7-VALID] Batchnorm Loss(Acc): bn_loss:0.10808(bn_acc:0.97)         vs No Batchnorm Loss(Acc): nn_loss:0.17986(nn_acc:0.95)\n",
      "\n",
      "[Epoch 8-TRAIN] Batchnorm Loss(Acc): bn_loss:0.07532(bn_acc:0.98)             vs No Batchnorm Loss(Acc): nn_loss:0.11900(nn_acc:0.97)\n",
      "[Epoch 8-VALID] Batchnorm Loss(Acc): bn_loss:0.11596(bn_acc:0.96)         vs No Batchnorm Loss(Acc): nn_loss:0.17909(nn_acc:0.96)\n",
      "\n",
      "[Epoch 9-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06135(bn_acc:0.98)             vs No Batchnorm Loss(Acc): nn_loss:0.11545(nn_acc:0.97)\n",
      "[Epoch 9-VALID] Batchnorm Loss(Acc): bn_loss:0.10129(bn_acc:0.97)         vs No Batchnorm Loss(Acc): nn_loss:0.19083(nn_acc:0.96)\n",
      "\n",
      "[Epoch 10-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06157(bn_acc:0.98)             vs No Batchnorm Loss(Acc): nn_loss:0.11047(nn_acc:0.97)\n",
      "[Epoch 10-VALID] Batchnorm Loss(Acc): bn_loss:0.10219(bn_acc:0.97)         vs No Batchnorm Loss(Acc): nn_loss:0.19094(nn_acc:0.96)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save Losses and Accuracies every epoch\n",
    "# We are going to plot them later\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "train_total_batch = len(train_loader)\n",
    "test_total_batch = len(test_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    bn_model.train() # set the model to train mode\n",
    "    \n",
    "    for X, Y in train_loader:\n",
    "        # reshape input image inot [batch_size by 784]\n",
    "        # label is not one-hot encoded\n",
    "        X = X.view(-1, 28*28).to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        bn_optimizer.zero_grad()\n",
    "        bn_prediction = bn_model(X)\n",
    "        bn_loss = criterion(bn_prediction, Y)\n",
    "        bn_loss.backward()\n",
    "        bn_optimizer.step()\n",
    "        \n",
    "        nn_optimizer.zero_grad()\n",
    "        nn_prediction = nn_model(X)\n",
    "        nn_loss = criterion(nn_prediction, Y)\n",
    "        nn_loss.backward()\n",
    "        nn_optimizer.step()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        bn_model.eval() # set the model to evaluation mode\n",
    "        \n",
    "        # Test the model using train sets\n",
    "        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n",
    "        for i, (X, Y) in enumerate(train_loader):\n",
    "            X = X.view(-1, 28 * 28).to(device)\n",
    "            Y = Y.to(device)\n",
    "            \n",
    "            bn_prediction = bn_model(X)\n",
    "            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
    "            bn_loss += criterion(bn_prediction, Y)\n",
    "            bn_acc += bn_correct_prediction.float().mean()\n",
    "            \n",
    "            nn_prediction = nn_model(X)\n",
    "            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n",
    "            nn_loss += criterion(nn_prediction, Y)\n",
    "            nn_acc += nn_correct_prediction.float().mean()\n",
    "            \n",
    "        bn_loss, nn_loss, bn_acc, nn_acc = \\\n",
    "        bn_loss / train_total_batch, \\\n",
    "        nn_loss / train_total_batch, \\\n",
    "        bn_acc / train_total_batch, \\\n",
    "        nn_acc / train_total_batch\n",
    "        \n",
    "        # Save train losses/acc\n",
    "        train_losses.append([bn_loss, nn_loss])\n",
    "        train_accs.append([bn_acc, nn_acc])\n",
    "        print(\n",
    "            '[Epoch %d-TRAIN] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) \\\n",
    "            vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n",
    "            (epoch + 1), bn_loss.item(), bn_acc.item(), \n",
    "                nn_loss.item(), nn_acc.item()))\n",
    "        \n",
    "        # Test the model using test sets\n",
    "        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n",
    "        for i, (X, Y) in enumerate(test_loader):\n",
    "            X = X.view(-1, 28 * 28).to(device)\n",
    "            Y = Y.to(device)\n",
    "            \n",
    "            bn_prediction = bn_model(X)\n",
    "            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
    "            bn_loss += criterion(bn_prediction, Y)\n",
    "            bn_acc += bn_correct_prediction.float().mean()\n",
    "            \n",
    "            nn_prediction = nn_model(X)\n",
    "            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n",
    "            nn_loss += criterion(nn_prediction, Y)\n",
    "            nn_acc += nn_correct_prediction.float().mean()\n",
    "            \n",
    "        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / test_total_batch, \\\n",
    "        nn_loss / test_total_batch, \\\n",
    "        bn_acc / test_total_batch, \\\n",
    "        nn_acc / test_total_batch\n",
    "        \n",
    "        # Save valid losses / acc\n",
    "        valid_losses.append([bn_loss, nn_loss])\n",
    "        valid_accs.append([bn_loss, nn_acc])\n",
    "        print(\n",
    "        '[Epoch %d-VALID] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) \\\n",
    "        vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n",
    "                (epoch + 1), bn_loss.item(), bn_acc.item(), \n",
    "            nn_loss.item(), nn_acc.item()))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
