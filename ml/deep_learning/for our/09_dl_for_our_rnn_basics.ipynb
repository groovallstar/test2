{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encodding\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "input_data_np = np.array([[h, e, l, l, o],\n",
    "                           [e, o, l, l, l],\n",
    "                           [l, l, e, e, l]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform as torch tensor\n",
    "input_data = torch.Tensor(input_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare RNN\n",
    "rnn = torch.nn.RNN(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8810,  0.7214],\n",
      "         [ 0.7921,  0.5653],\n",
      "         [ 0.4716,  0.5663],\n",
      "         [ 0.4716,  0.5663],\n",
      "         [ 0.9255, -0.0376]],\n",
      "\n",
      "        [[ 0.3839, -0.1119],\n",
      "         [ 0.7916, -0.5840],\n",
      "         [ 0.0352,  0.1385],\n",
      "         [ 0.0352,  0.1385],\n",
      "         [ 0.3118,  0.2829]],\n",
      "\n",
      "        [[ 0.4590,  0.5002],\n",
      "         [ 0.6097,  0.5695],\n",
      "         [ 0.7526,  0.5004],\n",
      "         [ 0.7526,  0.5004],\n",
      "         [ 0.2508,  0.3455]]], grad_fn=<StackBackward>)\n",
      "<built-in method size of Tensor object at 0x7f97a9e51cd0>\n"
     ]
    }
   ],
   "source": [
    "outputs, _status = rnn(input_data)\n",
    "print(outputs)\n",
    "print(outputs.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hihello example  \n",
    "'H','i','h','e','l','l','o'를 순차적으로 예측 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "\n",
    "# list of available characters\n",
    "char_set = ['h','i','e','l','o']\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]\n",
    "\n",
    "# data setting\n",
    "# hihell 까지만 input, o가 output이므로 o가 제외됨.\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],\n",
    "              [0, 1, 0, 0, 0],\n",
    "              [1, 0, 0, 0, 0],\n",
    "              [0, 0, 1, 0, 0],\n",
    "              [0, 0, 0, 1, 0],\n",
    "              [0, 0, 0, 1, 0]]]\n",
    "y_data = [[1, 0, 2, 3, 3, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "input_size = len(char_set)\n",
    "hidden_size = len(char_set)\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform as torch tensor variable\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare RNN\n",
    "# batch_first guarantees the order of output = (B, S, F)\n",
    "rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss & optimizer setting\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.9214973449707031 prediction:  [[2 1 2 2 0 0]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  eieehh\n",
      "1 loss:  1.7136880159378052 prediction:  [[2 1 4 4 4 1]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  eioooi\n",
      "2 loss:  1.5266627073287964 prediction:  [[2 1 2 4 4 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  eieooo\n",
      "3 loss:  1.3623462915420532 prediction:  [[2 1 2 3 4 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  eieloo\n",
      "4 loss:  1.2477024793624878 prediction:  [[2 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ehello\n",
      "5 loss:  1.1417827606201172 prediction:  [[2 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ehello\n",
      "6 loss:  1.0423825979232788 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "7 loss:  0.9706668257713318 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "8 loss:  0.9190327525138855 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "9 loss:  0.8697660565376282 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "10 loss:  0.8196830749511719 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "11 loss:  0.7755098938941956 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "12 loss:  0.7396721243858337 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "13 loss:  0.7105648517608643 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "14 loss:  0.6874920725822449 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "15 loss:  0.6710364818572998 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "16 loss:  0.6600294709205627 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "17 loss:  0.6486800312995911 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "18 loss:  0.6330389380455017 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "19 loss:  0.6155431866645813 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "20 loss:  0.5981414914131165 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "21 loss:  0.5803340077400208 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "22 loss:  0.5644497275352478 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "23 loss:  0.5549392700195312 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "24 loss:  0.5480155348777771 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "25 loss:  0.5391984581947327 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "26 loss:  0.5302402377128601 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "27 loss:  0.523294985294342 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "28 loss:  0.5169376730918884 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "29 loss:  0.5101814866065979 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "30 loss:  0.505639374256134 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "31 loss:  0.5027403831481934 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "32 loss:  0.49979785084724426 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "33 loss:  0.4965515434741974 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "34 loss:  0.49403342604637146 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "35 loss:  0.4922926723957062 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "36 loss:  0.48988261818885803 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "37 loss:  0.48790833353996277 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "38 loss:  0.48696771264076233 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "39 loss:  0.4859481751918793 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "40 loss:  0.4844557046890259 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "41 loss:  0.48313263058662415 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "42 loss:  0.48220348358154297 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "43 loss:  0.48100075125694275 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "44 loss:  0.4799903333187103 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "45 loss:  0.47944164276123047 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "46 loss:  0.47883522510528564 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "47 loss:  0.47812390327453613 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "48 loss:  0.4776031970977783 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "49 loss:  0.4770379960536957 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "50 loss:  0.4763089418411255 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "51 loss:  0.47576412558555603 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "52 loss:  0.47526636719703674 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "53 loss:  0.4747176170349121 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "54 loss:  0.4743283689022064 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "55 loss:  0.4739830195903778 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "56 loss:  0.4735397398471832 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "57 loss:  0.4731919467449188 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "58 loss:  0.472823828458786 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "59 loss:  0.4723908007144928 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "60 loss:  0.472050279378891 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "61 loss:  0.4717153310775757 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "62 loss:  0.47136303782463074 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "63 loss:  0.471096396446228 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "64 loss:  0.4708133637905121 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "65 loss:  0.4705161154270172 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "66 loss:  0.47026047110557556 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "67 loss:  0.46996650099754333 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "68 loss:  0.4696773588657379 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "69 loss:  0.46941661834716797 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "70 loss:  0.46913495659828186 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "71 loss:  0.4688827693462372 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "72 loss:  0.4686462879180908 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "73 loss:  0.46839722990989685 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "74 loss:  0.46817541122436523 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "75 loss:  0.4679426848888397 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "76 loss:  0.46770909428596497 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "77 loss:  0.4674917161464691 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "78 loss:  0.46726277470588684 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "79 loss:  0.4670514166355133 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "80 loss:  0.46684351563453674 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "81 loss:  0.46663573384284973 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "82 loss:  0.46644219756126404 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "83 loss:  0.46624162793159485 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "84 loss:  0.46604931354522705 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "85 loss:  0.46585771441459656 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "86 loss:  0.4656648337841034 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 loss:  0.4654809534549713 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "88 loss:  0.4652944803237915 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "89 loss:  0.4651161730289459 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "90 loss:  0.46493974328041077 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "91 loss:  0.4647655189037323 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "92 loss:  0.46459659934043884 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "93 loss:  0.46442607045173645 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "94 loss:  0.4642610549926758 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "95 loss:  0.46409568190574646 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "96 loss:  0.4639337956905365 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "97 loss:  0.4637746512889862 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "98 loss:  0.46361684799194336 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n",
      "99 loss:  0.46346330642700195 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]] prediction_str:  ihello\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs, _status = rnn(X)\n",
    "    loss = criterion(outputs.view(-1, input_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    result = outputs.data.numpy().argmax(axis=2)\n",
    "    result_str = ''.join([char_set[c] for c in np.squeeze(result)])\n",
    "    print(i, 'loss: ', loss.item(), 'prediction: ', result, \n",
    "          'true Y: ', y_data, 'prediction_str: ', result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "charseq example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \" if you want you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o': 0, 'u': 1, 't': 2, 'y': 3, 'n': 4, 'f': 5, 'w': 6, 'a': 7, ' ': 8, 'i': 9}\n"
     ]
    }
   ],
   "source": [
    "#make dictionary\n",
    "char_set = list(set(sample))\n",
    "char_dic = {c: i for i, c in enumerate(char_set)}\n",
    "print(char_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "dic_size = len(char_dic)\n",
    "hidden_size = len(char_dic)\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data setting\n",
    "sample_idx = [char_dic[c] for c in sample]\n",
    "x_data = [sample_idx[:-1]]\n",
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
    "y_data = [sample_idx[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform as torch tensor variable\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare RNN\n",
    "# batch_first guarantees the order of output = (B, S, F)\n",
    "rnn = torch.nn.RNN(dic_size, hidden_size, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loss & optimizer setting\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  2.372743844985962 prediction:  [[0 0 7 8 0 0 5 0 5 0 0 0 0 0 0]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  ooa oofofoooooo\n",
      "1 loss:  2.0390493869781494 prediction:  [[8 0 8 8 0 1 8 0 7 1 1 8 8 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:   o  ou oauu  ou\n",
      "2 loss:  1.7754647731781006 prediction:  [[8 5 8 3 0 1 8 6 7 1 2 8 8 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:   f you waut  ou\n",
      "3 loss:  1.5837836265563965 prediction:  [[8 5 8 3 0 1 8 6 7 1 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:   f you waut you\n",
      "4 loss:  1.456066608428955 prediction:  [[8 5 8 3 0 1 8 6 7 3 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:   f you wayt you\n",
      "5 loss:  1.3734009265899658 prediction:  [[8 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:   f you want you\n",
      "6 loss:  1.2991251945495605 prediction:  [[3 5 8 3 0 1 3 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  yf youywant you\n",
      "7 loss:  1.2393826246261597 prediction:  [[9 5 8 3 0 1 3 6 7 3 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if youywayt you\n",
      "8 loss:  1.1734353303909302 prediction:  [[9 5 8 3 0 1 3 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if youywant you\n",
      "9 loss:  1.1332519054412842 prediction:  [[9 5 8 3 0 1 3 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if youywant you\n",
      "10 loss:  1.1014595031738281 prediction:  [[9 5 8 3 0 1 8 6 5 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wfnt you\n",
      "11 loss:  1.0674368143081665 prediction:  [[9 5 8 3 0 1 8 6 5 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wfnt you\n",
      "12 loss:  1.03679621219635 prediction:  [[9 5 8 3 0 1 8 6 7 8 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wa t you\n",
      "13 loss:  1.0134633779525757 prediction:  [[9 5 8 3 0 1 8 6 7 8 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wa t you\n",
      "14 loss:  0.9920076131820679 prediction:  [[9 5 8 3 0 1 8 6 7 8 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wa t you\n",
      "15 loss:  0.9736259579658508 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "16 loss:  0.9615724682807922 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "17 loss:  0.9503604173660278 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "18 loss:  0.933897078037262 prediction:  [[9 5 8 3 0 1 8 6 7 8 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wa t you\n",
      "19 loss:  0.9256830811500549 prediction:  [[9 5 8 3 0 1 8 6 7 8 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wa t you\n",
      "20 loss:  0.9167446494102478 prediction:  [[9 5 8 3 0 1 8 6 7 8 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wa t you\n",
      "21 loss:  0.9095293283462524 prediction:  [[9 5 8 3 0 1 8 6 7 8 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wa t you\n",
      "22 loss:  0.9007276892662048 prediction:  [[9 5 8 3 0 1 8 6 7 8 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wa t you\n",
      "23 loss:  0.8907324075698853 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "24 loss:  0.8861474394798279 prediction:  [[9 5 8 3 0 1 8 6 7 8 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wa t you\n",
      "25 loss:  0.8795958161354065 prediction:  [[9 5 8 3 0 1 8 6 7 8 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wa t you\n",
      "26 loss:  0.8740468621253967 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "27 loss:  0.8705518841743469 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "28 loss:  0.865888774394989 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "29 loss:  0.8628833293914795 prediction:  [[9 5 8 3 0 1 8 6 7 8 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wa t you\n",
      "30 loss:  0.8608351945877075 prediction:  [[9 5 8 3 0 1 8 6 7 8 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wa t you\n",
      "31 loss:  0.8578952550888062 prediction:  [[9 5 8 3 0 1 8 6 7 8 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wa t you\n",
      "32 loss:  0.8548533916473389 prediction:  [[9 5 8 3 0 1 8 6 7 8 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you wa t you\n",
      "33 loss:  0.8529481291770935 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "34 loss:  0.8514858484268188 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "35 loss:  0.8498152494430542 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "36 loss:  0.8481733798980713 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "37 loss:  0.8465238213539124 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "38 loss:  0.844716489315033 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "39 loss:  0.8429079055786133 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "40 loss:  0.840825617313385 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "41 loss:  0.8378851413726807 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "42 loss:  0.8336582779884338 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "43 loss:  0.8289976716041565 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "44 loss:  0.8268014788627625 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "45 loss:  0.8261265158653259 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "46 loss:  0.8245901465415955 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "47 loss:  0.823130190372467 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "48 loss:  0.8218657374382019 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n",
      "49 loss:  0.8206174969673157 prediction:  [[9 5 8 3 0 1 8 6 7 4 2 8 3 0 1]] true Y:  [[9, 5, 8, 3, 0, 1, 8, 6, 7, 4, 2, 8, 3, 0, 1]] prediction str:  if you want you\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for i in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    outputs, _status = rnn(X)\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    result = outputs.data.numpy().argmax(axis=2)\n",
    "    result_str = ''.join([char_set[c] for c in np.squeeze(result)])\n",
    "    print(i, 'loss: ', loss.item(), 'prediction: ', result, \n",
    "          \"true Y: \", y_data, \"prediction str: \", result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f980852bd50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random seed to make results deterministic and reproducible\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary\n",
    "char_set = list(set(sentence))\n",
    "char_dic = {c : i for i, c in enumerate(char_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "dic_size = len(char_dic)\n",
    "hidden_size = len(char_dic)\n",
    "sequence_length = 10 # Any arbitrary number\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 if you wan -> f you want\n",
      "1 f you want ->  you want \n",
      "2  you want  -> you want t\n",
      "3 you want t -> ou want to\n",
      "4 ou want to -> u want to \n",
      "5 u want to  ->  want to b\n",
      "6  want to b -> want to bu\n",
      "7 want to bu -> ant to bui\n",
      "8 ant to bui -> nt to buil\n",
      "9 nt to buil -> t to build\n",
      "10 t to build ->  to build \n",
      "11  to build  -> to build a\n",
      "12 to build a -> o build a \n",
      "13 o build a  ->  build a s\n",
      "14  build a s -> build a sh\n",
      "15 build a sh -> uild a shi\n",
      "16 uild a shi -> ild a ship\n",
      "17 ild a ship -> ld a ship,\n",
      "18 ld a ship, -> d a ship, \n",
      "19 d a ship,  ->  a ship, d\n",
      "20  a ship, d -> a ship, do\n",
      "21 a ship, do ->  ship, don\n",
      "22  ship, don -> ship, don'\n",
      "23 ship, don' -> hip, don't\n",
      "24 hip, don't -> ip, don't \n",
      "25 ip, don't  -> p, don't d\n",
      "26 p, don't d -> , don't dr\n",
      "27 , don't dr ->  don't dru\n",
      "28  don't dru -> don't drum\n",
      "29 don't drum -> on't drum \n",
      "30 on't drum  -> n't drum u\n",
      "31 n't drum u -> 't drum up\n",
      "32 't drum up -> t drum up \n",
      "33 t drum up  ->  drum up p\n",
      "34  drum up p -> drum up pe\n",
      "35 drum up pe -> rum up peo\n",
      "36 rum up peo -> um up peop\n",
      "37 um up peop -> m up peopl\n",
      "38 m up peopl ->  up people\n",
      "39  up people -> up people \n",
      "40 up people  -> p people t\n",
      "41 p people t ->  people to\n",
      "42  people to -> people tog\n",
      "43 people tog -> eople toge\n",
      "44 eople toge -> ople toget\n",
      "45 ople toget -> ple togeth\n",
      "46 ple togeth -> le togethe\n",
      "47 le togethe -> e together\n",
      "48 e together ->  together \n",
      "49  together  -> together t\n",
      "50 together t -> ogether to\n",
      "51 ogether to -> gether to \n",
      "52 gether to  -> ether to c\n",
      "53 ether to c -> ther to co\n",
      "54 ther to co -> her to col\n",
      "55 her to col -> er to coll\n",
      "56 er to coll -> r to colle\n",
      "57 r to colle ->  to collec\n",
      "58  to collec -> to collect\n",
      "59 to collect -> o collect \n",
      "60 o collect  ->  collect w\n",
      "61  collect w -> collect wo\n",
      "62 collect wo -> ollect woo\n",
      "63 ollect woo -> llect wood\n",
      "64 llect wood -> lect wood \n",
      "65 lect wood  -> ect wood a\n",
      "66 ect wood a -> ct wood an\n",
      "67 ct wood an -> t wood and\n",
      "68 t wood and ->  wood and \n",
      "69  wood and  -> wood and d\n",
      "70 wood and d -> ood and do\n",
      "71 ood and do -> od and don\n",
      "72 od and don -> d and don'\n",
      "73 d and don' ->  and don't\n",
      "74  and don't -> and don't \n",
      "75 and don't  -> nd don't a\n",
      "76 nd don't a -> d don't as\n",
      "77 d don't as ->  don't ass\n",
      "78  don't ass -> don't assi\n",
      "79 don't assi -> on't assig\n",
      "80 on't assig -> n't assign\n",
      "81 n't assign -> 't assign \n",
      "82 't assign  -> t assign t\n",
      "83 t assign t ->  assign th\n",
      "84  assign th -> assign the\n",
      "85 assign the -> ssign them\n",
      "86 ssign them -> sign them \n",
      "87 sign them  -> ign them t\n",
      "88 ign them t -> gn them ta\n",
      "89 gn them ta -> n them tas\n",
      "90 n them tas ->  them task\n",
      "91  them task -> them tasks\n",
      "92 them tasks -> hem tasks \n",
      "93 hem tasks  -> em tasks a\n",
      "94 em tasks a -> m tasks an\n",
      "95 m tasks an ->  tasks and\n",
      "96  tasks and -> tasks and \n",
      "97 tasks and  -> asks and w\n",
      "98 asks and w -> sks and wo\n",
      "99 sks and wo -> ks and wor\n",
      "100 ks and wor -> s and work\n",
      "101 s and work ->  and work,\n",
      "102  and work, -> and work, \n",
      "103 and work,  -> nd work, b\n",
      "104 nd work, b -> d work, bu\n",
      "105 d work, bu ->  work, but\n",
      "106  work, but -> work, but \n",
      "107 work, but  -> ork, but r\n",
      "108 ork, but r -> rk, but ra\n",
      "109 rk, but ra -> k, but rat\n",
      "110 k, but rat -> , but rath\n",
      "111 , but rath ->  but rathe\n",
      "112  but rathe -> but rather\n",
      "113 but rather -> ut rather \n",
      "114 ut rather  -> t rather t\n",
      "115 t rather t ->  rather te\n",
      "116  rather te -> rather tea\n",
      "117 rather tea -> ather teac\n",
      "118 ather teac -> ther teach\n",
      "119 ther teach -> her teach \n",
      "120 her teach  -> er teach t\n",
      "121 er teach t -> r teach th\n",
      "122 r teach th ->  teach the\n",
      "123  teach the -> teach them\n",
      "124 teach them -> each them \n",
      "125 each them  -> ach them t\n",
      "126 ach them t -> ch them to\n",
      "127 ch them to -> h them to \n",
      "128 h them to  ->  them to l\n",
      "129  them to l -> them to lo\n",
      "130 them to lo -> hem to lon\n",
      "131 hem to lon -> em to long\n",
      "132 em to long -> m to long \n",
      "133 m to long  ->  to long f\n",
      "134  to long f -> to long fo\n",
      "135 to long fo -> o long for\n",
      "136 o long for ->  long for \n",
      "137  long for  -> long for t\n",
      "138 long for t -> ong for th\n",
      "139 ong for th -> ng for the\n",
      "140 ng for the -> g for the \n",
      "141 g for the  ->  for the e\n",
      "142  for the e -> for the en\n",
      "143 for the en -> or the end\n",
      "144 or the end -> r the endl\n",
      "145 r the endl ->  the endle\n",
      "146  the endle -> the endles\n",
      "147 the endles -> he endless\n",
      "148 he endless -> e endless \n",
      "149 e endless  ->  endless i\n",
      "150  endless i -> endless im\n",
      "151 endless im -> ndless imm\n",
      "152 ndless imm -> dless imme\n",
      "153 dless imme -> less immen\n",
      "154 less immen -> ess immens\n",
      "155 ess immens -> ss immensi\n",
      "156 ss immensi -> s immensit\n",
      "157 s immensit ->  immensity\n",
      "158  immensity -> immensity \n",
      "159 immensity  -> mmensity o\n",
      "160 mmensity o -> mensity of\n",
      "161 mensity of -> ensity of \n",
      "162 ensity of  -> nsity of t\n",
      "163 nsity of t -> sity of th\n",
      "164 sity of th -> ity of the\n",
      "165 ity of the -> ty of the \n",
      "166 ty of the  -> y of the s\n",
      "167 y of the s ->  of the se\n",
      "168  of the se -> of the sea\n",
      "169 of the sea -> f the sea.\n"
     ]
    }
   ],
   "source": [
    "# data setting\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i : i + sequence_length]\n",
    "    y_str = sentence[i+1 : i + sequence_length + 1]\n",
    "    print(i, x_str, '->', y_str)\n",
    "    \n",
    "    x_data.append([char_dic[c] for c in x_str])\n",
    "    y_data.append([char_dic[c] for c in y_str])\n",
    "    \n",
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform as torch tensor variable\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare RNN + FC\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(\n",
    "            input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(dic_size, hidden_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss & optimizer setting\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'iiiieeeiieewieeiieeeeieaieieeieeaewia'ei'e'iaeiaawiaeaeaaeiieeiaiiaiaiiaeaiiaewieeaeeiaiiiawiaeeiieaeiiiaeiiaaieeiweaieeaaeiiaeaaiiaeeiieiieeewiae'iaaeiaiiaaaiweeieiiaeiwiwiaaeia\n",
      "                                                                                                                                                                                   \n",
      "ootototot ototottot tot t   totototo totottot  t dot todr tot tt to ttoto tod tot totto tt todr toto otto tot tototototodrototsottotr totoot tootototr t  tostootot sootto totrdtod\n",
      "ocb ucu uct cb ec. ya  cec.culuct u t ycnolycu yc  pcu .c.algctcy.e c  elyc.yl cu t u u uctc ccagcy   ycu ucu t u t ucu uc.  uct y tcylcbloc.bglc tcu t u tcy  c. acy  y' cy ucu rc\n",
      "  a t t thi  t e e i  t t t t i t t t iw  s  i s   d t e i  sht i s is e ths ie tht t i i s i  iysa   t s t t t thi t i t i    t t t ihs  ehs s e   e t s i i i s t i i rtit t t t \n",
      "  d e      e er i e e       d s     d  d e     e   s e  t   e  a  d  s e  t e e     e s  d   e  ther    d the     s s s e  t e  e  d  t e es ed n t ea  d e        d   e     e   s \n",
      "  dotm m tltotm m tmemtmdt ototm mdod  tl t nioemtodmnm n totm nl m  to m to mtotm mdodm t t em toem   o mto mtmto mtotm t toem etoem toeienl  otmtoemdodotm ito ttm   tt  toem  dm\n",
      "e wo mton ototh   tototo t ototototo toto ton oo to ltoenotodtonetrtoto m todowo ltoto t n toe  to m t o owo  totototo  enotoe  otoem to tonltwototoem odown tto tnd to to toemto t\n",
      "e do  to p wo to ltotot tt  tonltodo totun  o  o to etoereto tt etrt wo   tod wo dthdn  t sto p tos    o  wo  todot do  en to  to  e  to to dtuo etoepd   tn  to tnstt eto toepto t\n",
      "  do  to p tod'nd dkdod dp etoddthe  t t  p n  est rlt er'todl nerst d  d tod wod'thens   gt ep todp   od worlt dnd dor en tod  edoer todlonltforltoer  d tns  o  nstther etoer  s \n",
      "  d   a  p a d'ac d t d dps t rlthe e  t ep    est rlt er t nco ers  d u  tnd t r'thens   gt er t dp  tod aon r ert d d e  t e  et er t dlon't  r t er  d ans g  ers   en ether  st\n",
      "  d n a ndht d'ac d t dsdp  t rltha ec t epe p est  't erst e'ocle s douc tnd'tor'thans g gt en t dk    d ton r aat det e  t es  ther t d'en't or themend ass g  ensst   u them  ss\n",
      "  gon tonthto 'at d tntsdp  ton'thaoec taepeni est  ether tonlon'es  aonc tnd ton'thdns g  toem tosk  tns ton , tat det en tonc  t em ton'en' fon toemtnd ast g  enstt  ou t em  ss\n",
      "  goo tosthaou'aisd tot dif tou'thdoum taepa tleato ether to bo 'e th onc and aon'thdnsi t toem tosk  tns ton ,otat dathem tonth toem to 'on'ofon toem nd ast go  nsttheoo toem ost\n",
      " m oo tosihtog'uigd tothdif tou'thdout tpeil tleuto ethem to bo 'e todoodlandhton'thdnsigt to m tosk  dns ton , tat dother toush toem to bo gofon toem ndpest dt ensitheof toem ngt\n",
      " mdoo tosi to build tnthdmf don'thdout tpepln le to lthem to po le t don  tnd ton'thdnsign to m tosk  tni don , dut dether toust them to pengtfon toem nd ess d  ensitheof them nss\n",
      " m ooptoni to luild tnthdpf don'thdoum tp pe  le to  t em tonlo  e t don  tns won'thdmsign toem tosk  dnd wonk, dut dether tonst them to bengtfon toem nd ess do ensityeof them oss\n",
      "   oopwost wo build tnthdp, don'thdoum tp plople to  ther to 'o  e t dond ans won'thdssign toer tosks dnd wonk, dut duther tosst ther to beng fon toem ndkess tm ensity oo them ecs\n",
      "t  oopwost wo luild tnthip, don'thwoup tp people to  ther to lo le t dood and won'thassign them tosks and wonk, dut w ther tonch ther to berg fon toemendkecs tm ensity oo themencs\n",
      "t  oo wost ao luild anthip, don'thaoup tp people to  ther to lo  e t aood and ton'thassign ther tosks and aork, dut wather tonch ther to lerg fon toe end ess im ensity oo themencs\n",
      "t  of wast ao build anship, don't doum up people to ether to co lect dood tnd ton'thansign ther tosks and todk, dut rather toacm the  to berg fon toe end ess im ensity of themee t\n",
      "t  ofkwast ao build anship, don't doum up people together to co lect dood and ton'thansign ther tosks dnd aonk, dut rather toach the  to cor' fon the endless im ensity of the ee t\n",
      "t  ou want ao luild umship, don't doum up people together to co lect dood and aon't assign ther tosks dnd work, dut rather toach the  to cor' for the endless immensity of themee t\n",
      "t  o  want to luild tmship, don't doum up people together to co lect dood and won't dssign ther tosks dnd work, dut rather toach the  to lor' for the endless immensity of themee t\n",
      "pm o  want to luild amship, don't drum up people to ether to co lect dood and won't dssign ther tosks dnd work, dut rather toach the  to lorg for the endless immensity of themeeas\n",
      "em a  want to luild anship, don't arum up people to ether to co lect dood and won't assign ther tosks and work, but rather toach them to lorg for the endless immensity of themeeas\n",
      "em a  want to luild anship, don't drum up people to ether to co lect wood and won't dssign ther tosks and work, but rather toach them to long for the endless immensity of themeeas\n",
      "em au want to build anship, don't drum up people to ether to co lect wood and won't dssign the  tosks and work, but rather toach them to long for the endless immensity of themeeac\n",
      "embau want to build anship, don't arum up people together to collect wood and aon't assign them tosks and work, but rather toach them to long for the endless immensity of the eeac\n",
      "pmbou want to build amship, don't drum up people together to collect wood and won't assign them tosks and work, but rather toach the  to long for the endless immensity of the eeac\n",
      "pmbou want to build amship, don't drum up people to ether to collect wood and don't assign them tosks and work, but rather toach the  to long for the endless immensity of the eeac\n",
      "l bou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach the  to long for the endless immensity of the seac\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach the  to long for the endless immensity of the seac\n",
      "l  ou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach the  to long for the endless immensity of the seac\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach the  to long for the endless immensity of the seac\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach the  to long for the endless immensity of the seac\n",
      "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach the  to long for the sndless immensity of the seac\n",
      "t you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
      "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the eeac\n",
      "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the sndless immensity of the seat\n",
      "t you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the eeac\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = ''\n",
    "    for j, result in enumerate(results):\n",
    "        #print(i, j, ''.join([char_set[t] for t in result]), loss.item())\n",
    "        if j == 0:\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else:\n",
    "            predict_str += char_set[result[-1]]\n",
    "    \n",
    "    print(predict_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f980852bd50>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random seed to make results deterministic and reproducible\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling function for input data\n",
    "def minmax_scaler(data):\n",
    "    numerate = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    return numerate / (denominator + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset to input\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series[i : i + seq_length, :]\n",
    "        _y = time_series[i + seq_length, [-1]]\n",
    "        print(_x, '->', _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "seq_length = 7\n",
    "data_dim = 6\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learninig_rate = 0.01\n",
    "iterations = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11679644 0.10891404 0.12241071 0.1404261  0.22107676 0.1404261 ]\n",
      " [0.11568409 0.12027819 0.12508929 0.15479137 0.33496593 0.15479137]\n",
      " [0.1570634  0.1601891  0.17196429 0.1929367  0.34815613 0.1929367 ]\n",
      " [0.20053393 0.18791763 0.19410714 0.21079385 0.35350486 0.21079385]\n",
      " [0.17575083 0.15873449 0.17026786 0.18626238 0.28058924 0.18626238]\n",
      " [0.1716574  0.15682531 0.15973214 0.17233911 0.30015533 0.17233911]\n",
      " [0.16120133 0.14496113 0.13700893 0.15899045 0.31318268 0.15899045]] -> [0.14307815]\n",
      "[[0.11568409 0.12027819 0.12508929 0.15479137 0.33496593 0.15479137]\n",
      " [0.1570634  0.1601891  0.17196429 0.1929367  0.34815613 0.1929367 ]\n",
      " [0.20053393 0.18791763 0.19410714 0.21079385 0.35350486 0.21079385]\n",
      " [0.17575083 0.15873449 0.17026786 0.18626238 0.28058924 0.18626238]\n",
      " [0.1716574  0.15682531 0.15973214 0.17233911 0.30015533 0.17233911]\n",
      " [0.16120133 0.14496113 0.13700893 0.15899045 0.31318268 0.15899045]\n",
      " [0.14135706 0.12605118 0.14459821 0.14307815 0.25134031 0.14307815]] -> [0.12469059]\n",
      "[[0.1570634  0.1601891  0.17196429 0.1929367  0.34815613 0.1929367 ]\n",
      " [0.20053393 0.18791763 0.19410714 0.21079385 0.35350486 0.21079385]\n",
      " [0.17575083 0.15873449 0.17026786 0.18626238 0.28058924 0.18626238]\n",
      " [0.1716574  0.15682531 0.15973214 0.17233911 0.30015533 0.17233911]\n",
      " [0.16120133 0.14496113 0.13700893 0.15899045 0.31318268 0.15899045]\n",
      " [0.14135706 0.12605118 0.14459821 0.14307815 0.25134031 0.14307815]\n",
      " [0.11951057 0.09864085 0.11263393 0.12469059 0.25800431 0.12469059]] -> [0.08902051]\n",
      "[[0.20053393 0.18791763 0.19410714 0.21079385 0.35350486 0.21079385]\n",
      " [0.17575083 0.15873449 0.17026786 0.18626238 0.28058924 0.18626238]\n",
      " [0.1716574  0.15682531 0.15973214 0.17233911 0.30015533 0.17233911]\n",
      " [0.16120133 0.14496113 0.13700893 0.15899045 0.31318268 0.15899045]\n",
      " [0.14135706 0.12605118 0.14459821 0.14307815 0.25134031 0.14307815]\n",
      " [0.11951057 0.09864085 0.11263393 0.12469059 0.25800431 0.12469059]\n",
      " [0.09566185 0.07045775 0.08044643 0.08902051 0.36053212 0.08902051]] -> [0.08561704]\n",
      "[[0.17575083 0.15873449 0.17026786 0.18626238 0.28058924 0.18626238]\n",
      " [0.1716574  0.15682531 0.15973214 0.17233911 0.30015533 0.17233911]\n",
      " [0.16120133 0.14496113 0.13700893 0.15899045 0.31318268 0.15899045]\n",
      " [0.14135706 0.12605118 0.14459821 0.14307815 0.25134031 0.14307815]\n",
      " [0.11951057 0.09864085 0.11263393 0.12469059 0.25800431 0.12469059]\n",
      " [0.09566185 0.07045775 0.08044643 0.08902051 0.36053212 0.08902051]\n",
      " [0.08591769 0.06859403 0.08678571 0.08561704 0.27445135 0.08561704]] -> [0.08190417]\n",
      "[[0.1716574  0.15682531 0.15973214 0.17233911 0.30015533 0.17233911]\n",
      " [0.16120133 0.14496113 0.13700893 0.15899045 0.31318268 0.15899045]\n",
      " [0.14135706 0.12605118 0.14459821 0.14307815 0.25134031 0.14307815]\n",
      " [0.11951057 0.09864085 0.11263393 0.12469059 0.25800431 0.12469059]\n",
      " [0.09566185 0.07045775 0.08044643 0.08902051 0.36053212 0.08902051]\n",
      " [0.08591769 0.06859403 0.08678571 0.08561704 0.27445135 0.08561704]\n",
      " [0.0720356  0.06236647 0.06955357 0.08190417 0.3296172  0.08190417]] -> [0.07704208]\n",
      "[[0.16120133 0.14496113 0.13700893 0.15899045 0.31318268 0.15899045]\n",
      " [0.14135706 0.12605118 0.14459821 0.14307815 0.25134031 0.14307815]\n",
      " [0.11951057 0.09864085 0.11263393 0.12469059 0.25800431 0.12469059]\n",
      " [0.09566185 0.07045775 0.08044643 0.08902051 0.36053212 0.08902051]\n",
      " [0.08591769 0.06859403 0.08678571 0.08561704 0.27445135 0.08561704]\n",
      " [0.0720356  0.06236647 0.06955357 0.08190417 0.3296172  0.08190417]\n",
      " [0.06976641 0.05768444 0.07598214 0.07704208 0.27889819 0.07704208]] -> [0.10369519]\n",
      "[[0.14135706 0.12605118 0.14459821 0.14307815 0.25134031 0.14307815]\n",
      " [0.11951057 0.09864085 0.11263393 0.12469059 0.25800431 0.12469059]\n",
      " [0.09566185 0.07045775 0.08044643 0.08902051 0.36053212 0.08902051]\n",
      " [0.08591769 0.06859403 0.08678571 0.08561704 0.27445135 0.08561704]\n",
      " [0.0720356  0.06236647 0.06955357 0.08190417 0.3296172  0.08190417]\n",
      " [0.06976641 0.05768444 0.07598214 0.07704208 0.27889819 0.07704208]\n",
      " [0.08102336 0.06573026 0.08303571 0.10369519 0.42594448 0.10369519]] -> [0.04738331]\n",
      "[[0.11951057 0.09864085 0.11263393 0.12469059 0.25800431 0.12469059]\n",
      " [0.09566185 0.07045775 0.08044643 0.08902051 0.36053212 0.08902051]\n",
      " [0.08591769 0.06859403 0.08678571 0.08561704 0.27445135 0.08561704]\n",
      " [0.0720356  0.06236647 0.06955357 0.08190417 0.3296172  0.08190417]\n",
      " [0.06976641 0.05768444 0.07598214 0.07704208 0.27889819 0.07704208]\n",
      " [0.08102336 0.06573026 0.08303571 0.10369519 0.42594448 0.10369519]\n",
      " [0.07541713 0.05222965 0.02638393 0.04738331 0.40907155 0.04738331]] -> [0.04000177]\n",
      "[[0.09566185 0.07045775 0.08044643 0.08902051 0.36053212 0.08902051]\n",
      " [0.08591769 0.06859403 0.08678571 0.08561704 0.27445135 0.08561704]\n",
      " [0.0720356  0.06236647 0.06955357 0.08190417 0.3296172  0.08190417]\n",
      " [0.06976641 0.05768444 0.07598214 0.07704208 0.27889819 0.07704208]\n",
      " [0.08102336 0.06573026 0.08303571 0.10369519 0.42594448 0.10369519]\n",
      " [0.07541713 0.05222965 0.02638393 0.04738331 0.40907155 0.04738331]\n",
      " [0.02958843 0.01359153 0.02933036 0.04000177 0.25564936 0.04000177]] -> [0.05595827]\n",
      "[[0.08591769 0.06859403 0.08678571 0.08561704 0.27445135 0.08561704]\n",
      " [0.0720356  0.06236647 0.06955357 0.08190417 0.3296172  0.08190417]\n",
      " [0.06976641 0.05768444 0.07598214 0.07704208 0.27889819 0.07704208]\n",
      " [0.08102336 0.06573026 0.08303571 0.10369519 0.42594448 0.10369519]\n",
      " [0.07541713 0.05222965 0.02638393 0.04738331 0.40907155 0.04738331]\n",
      " [0.02958843 0.01359153 0.02933036 0.04000177 0.25564936 0.04000177]\n",
      " [0.03888765 0.02831947 0.04723214 0.05595827 0.20030815 0.05595827]] -> [0.08460042]\n",
      "[[0.0720356  0.06236647 0.06955357 0.08190417 0.3296172  0.08190417]\n",
      " [0.06976641 0.05768444 0.07598214 0.07704208 0.27889819 0.07704208]\n",
      " [0.08102336 0.06573026 0.08303571 0.10369519 0.42594448 0.10369519]\n",
      " [0.07541713 0.05222965 0.02638393 0.04738331 0.40907155 0.04738331]\n",
      " [0.02958843 0.01359153 0.02933036 0.04000177 0.25564936 0.04000177]\n",
      " [0.03888765 0.02831947 0.04723214 0.05595827 0.20030815 0.05595827]\n",
      " [0.03114572 0.04500205 0.04700893 0.08460042 0.27379998 0.08460042]] -> [0.12681223]\n",
      "[[0.06976641 0.05768444 0.07598214 0.07704208 0.27889819 0.07704208]\n",
      " [0.08102336 0.06573026 0.08303571 0.10369519 0.42594448 0.10369519]\n",
      " [0.07541713 0.05222965 0.02638393 0.04738331 0.40907155 0.04738331]\n",
      " [0.02958843 0.01359153 0.02933036 0.04000177 0.25564936 0.04000177]\n",
      " [0.03888765 0.02831947 0.04723214 0.05595827 0.20030815 0.05595827]\n",
      " [0.03114572 0.04500205 0.04700893 0.08460042 0.27379998 0.08460042]\n",
      " [0.06086763 0.10105005 0.07647321 0.12681223 0.3003808  0.12681223]] -> [0.12159653]\n",
      "[[0.08102336 0.06573026 0.08303571 0.10369519 0.42594448 0.10369519]\n",
      " [0.07541713 0.05222965 0.02638393 0.04738331 0.40907155 0.04738331]\n",
      " [0.02958843 0.01359153 0.02933036 0.04000177 0.25564936 0.04000177]\n",
      " [0.03888765 0.02831947 0.04723214 0.05595827 0.20030815 0.05595827]\n",
      " [0.03114572 0.04500205 0.04700893 0.08460042 0.27379998 0.08460042]\n",
      " [0.06086763 0.10105005 0.07647321 0.12681223 0.3003808  0.12681223]\n",
      " [0.11568409 0.09909541 0.11723214 0.12159653 0.16416976 0.12159653]] -> [0.11032532]\n",
      "[[0.07541713 0.05222965 0.02638393 0.04738331 0.40907155 0.04738331]\n",
      " [0.02958843 0.01359153 0.02933036 0.04000177 0.25564936 0.04000177]\n",
      " [0.03888765 0.02831947 0.04723214 0.05595827 0.20030815 0.05595827]\n",
      " [0.03114572 0.04500205 0.04700893 0.08460042 0.27379998 0.08460042]\n",
      " [0.06086763 0.10105005 0.07647321 0.12681223 0.3003808  0.12681223]\n",
      " [0.11568409 0.09909541 0.11723214 0.12159653 0.16416976 0.12159653]\n",
      " [0.10678532 0.09641347 0.09928571 0.11032532 0.21735645 0.11032532]] -> [0.0988331]\n",
      "[[0.02958843 0.01359153 0.02933036 0.04000177 0.25564936 0.04000177]\n",
      " [0.03888765 0.02831947 0.04723214 0.05595827 0.20030815 0.05595827]\n",
      " [0.03114572 0.04500205 0.04700893 0.08460042 0.27379998 0.08460042]\n",
      " [0.06086763 0.10105005 0.07647321 0.12681223 0.3003808  0.12681223]\n",
      " [0.11568409 0.09909541 0.11723214 0.12159653 0.16416976 0.12159653]\n",
      " [0.10678532 0.09641347 0.09928571 0.11032532 0.21735645 0.11032532]\n",
      " [0.10767519 0.09273149 0.09897321 0.0988331  0.18189448 0.0988331 ]] -> [0.05242221]\n",
      "[[0.03888765 0.02831947 0.04723214 0.05595827 0.20030815 0.05595827]\n",
      " [0.03114572 0.04500205 0.04700893 0.08460042 0.27379998 0.08460042]\n",
      " [0.06086763 0.10105005 0.07647321 0.12681223 0.3003808  0.12681223]\n",
      " [0.11568409 0.09909541 0.11723214 0.12159653 0.16416976 0.12159653]\n",
      " [0.10678532 0.09641347 0.09928571 0.11032532 0.21735645 0.11032532]\n",
      " [0.10767519 0.09273149 0.09897321 0.0988331  0.18189448 0.0988331 ]\n",
      " [0.06856507 0.04277467 0.04834821 0.05242221 0.20779888 0.05242221]] -> [0.0573727]\n",
      "[[0.03114572 0.04500205 0.04700893 0.08460042 0.27379998 0.08460042]\n",
      " [0.06086763 0.10105005 0.07647321 0.12681223 0.3003808  0.12681223]\n",
      " [0.11568409 0.09909541 0.11723214 0.12159653 0.16416976 0.12159653]\n",
      " [0.10678532 0.09641347 0.09928571 0.11032532 0.21735645 0.11032532]\n",
      " [0.10767519 0.09273149 0.09897321 0.0988331  0.18189448 0.0988331 ]\n",
      " [0.06856507 0.04277467 0.04834821 0.05242221 0.20779888 0.05242221]\n",
      " [0.02892102 0.02109187 0.02535714 0.0573727  0.1758568  0.0573727 ]] -> [0.]\n",
      "[[0.06086763 0.10105005 0.07647321 0.12681223 0.3003808  0.12681223]\n",
      " [0.11568409 0.09909541 0.11723214 0.12159653 0.16416976 0.12159653]\n",
      " [0.10678532 0.09641347 0.09928571 0.11032532 0.21735645 0.11032532]\n",
      " [0.10767519 0.09273149 0.09897321 0.0988331  0.18189448 0.0988331 ]\n",
      " [0.06856507 0.04277467 0.04834821 0.05242221 0.20779888 0.05242221]\n",
      " [0.02892102 0.02109187 0.02535714 0.0573727  0.1758568  0.0573727 ]\n",
      " [0.03666296 0.01450066 0.         0.         0.30070648 0.        ]] -> [0.0259017]\n",
      "[[0.11568409 0.09909541 0.11723214 0.12159653 0.16416976 0.12159653]\n",
      " [0.10678532 0.09641347 0.09928571 0.11032532 0.21735645 0.11032532]\n",
      " [0.10767519 0.09273149 0.09897321 0.0988331  0.18189448 0.0988331 ]\n",
      " [0.06856507 0.04277467 0.04834821 0.05242221 0.20779888 0.05242221]\n",
      " [0.02892102 0.02109187 0.02535714 0.0573727  0.1758568  0.0573727 ]\n",
      " [0.03666296 0.01450066 0.         0.         0.30070648 0.        ]\n",
      " [0.         0.         0.00928571 0.0259017  0.32349183 0.0259017 ]] -> [0.06082037]\n",
      "[[0.10678532 0.09641347 0.09928571 0.11032532 0.21735645 0.11032532]\n",
      " [0.10767519 0.09273149 0.09897321 0.0988331  0.18189448 0.0988331 ]\n",
      " [0.06856507 0.04277467 0.04834821 0.05242221 0.20779888 0.05242221]\n",
      " [0.02892102 0.02109187 0.02535714 0.0573727  0.1758568  0.0573727 ]\n",
      " [0.03666296 0.01450066 0.         0.         0.30070648 0.        ]\n",
      " [0.         0.         0.00928571 0.0259017  0.32349183 0.0259017 ]\n",
      " [0.02723026 0.03841084 0.04383929 0.06082037 0.30274827 0.06082037]] -> [0.07854491]\n",
      "[[0.10767519 0.09273149 0.09897321 0.0988331  0.18189448 0.0988331 ]\n",
      " [0.06856507 0.04277467 0.04834821 0.05242221 0.20779888 0.05242221]\n",
      " [0.02892102 0.02109187 0.02535714 0.0573727  0.1758568  0.0573727 ]\n",
      " [0.03666296 0.01450066 0.         0.         0.30070648 0.        ]\n",
      " [0.         0.         0.00928571 0.0259017  0.32349183 0.0259017 ]\n",
      " [0.02723026 0.03841084 0.04383929 0.06082037 0.30274827 0.06082037]\n",
      " [0.04262514 0.05168417 0.05678571 0.07854491 0.25903147 0.07854491]] -> [0.12230375]\n",
      "[[0.06856507 0.04277467 0.04834821 0.05242221 0.20779888 0.05242221]\n",
      " [0.02892102 0.02109187 0.02535714 0.0573727  0.1758568  0.0573727 ]\n",
      " [0.03666296 0.01450066 0.         0.         0.30070648 0.        ]\n",
      " [0.         0.         0.00928571 0.0259017  0.32349183 0.0259017 ]\n",
      " [0.02723026 0.03841084 0.04383929 0.06082037 0.30274827 0.06082037]\n",
      " [0.04262514 0.05168417 0.05678571 0.07854491 0.25903147 0.07854491]\n",
      " [0.07194661 0.08418564 0.08571429 0.12230375 0.39921335 0.12230375]] -> [0.12402758]\n",
      "[[0.02892102 0.02109187 0.02535714 0.0573727  0.1758568  0.0573727 ]\n",
      " [0.03666296 0.01450066 0.         0.         0.30070648 0.        ]\n",
      " [0.         0.         0.00928571 0.0259017  0.32349183 0.0259017 ]\n",
      " [0.02723026 0.03841084 0.04383929 0.06082037 0.30274827 0.06082037]\n",
      " [0.04262514 0.05168417 0.05678571 0.07854491 0.25903147 0.07854491]\n",
      " [0.07194661 0.08418564 0.08571429 0.12230375 0.39921335 0.12230375]\n",
      " [0.10549499 0.09423156 0.11111607 0.12402758 0.25517336 0.12402758]] -> [0.18361033]\n",
      "[[0.03666296 0.01450066 0.         0.         0.30070648 0.        ]\n",
      " [0.         0.         0.00928571 0.0259017  0.32349183 0.0259017 ]\n",
      " [0.02723026 0.03841084 0.04383929 0.06082037 0.30274827 0.06082037]\n",
      " [0.04262514 0.05168417 0.05678571 0.07854491 0.25903147 0.07854491]\n",
      " [0.07194661 0.08418564 0.08571429 0.12230375 0.39921335 0.12230375]\n",
      " [0.10549499 0.09423156 0.11111607 0.12402758 0.25517336 0.12402758]\n",
      " [0.13196885 0.14855221 0.14513393 0.18361033 0.36555517 0.18361033]] -> [0.2031913]\n",
      "[[0.         0.         0.00928571 0.0259017  0.32349183 0.0259017 ]\n",
      " [0.02723026 0.03841084 0.04383929 0.06082037 0.30274827 0.06082037]\n",
      " [0.04262514 0.05168417 0.05678571 0.07854491 0.25903147 0.07854491]\n",
      " [0.07194661 0.08418564 0.08571429 0.12230375 0.39921335 0.12230375]\n",
      " [0.10549499 0.09423156 0.11111607 0.12402758 0.25517336 0.12402758]\n",
      " [0.13196885 0.14855221 0.14513393 0.18361033 0.36555517 0.18361033]\n",
      " [0.16320356 0.18069003 0.17017857 0.2031913  0.71347329 0.2031913 ]] -> [0.14860325]\n",
      "[[0.02723026 0.03841084 0.04383929 0.06082037 0.30274827 0.06082037]\n",
      " [0.04262514 0.05168417 0.05678571 0.07854491 0.25903147 0.07854491]\n",
      " [0.07194661 0.08418564 0.08571429 0.12230375 0.39921335 0.12230375]\n",
      " [0.10549499 0.09423156 0.11111607 0.12402758 0.25517336 0.12402758]\n",
      " [0.13196885 0.14855221 0.14513393 0.18361033 0.36555517 0.18361033]\n",
      " [0.16320356 0.18069003 0.17017857 0.2031913  0.71347329 0.2031913 ]\n",
      " [0.14682981 0.13400609 0.13714286 0.14860325 0.7510021  0.14860325]] -> [0.14824965]\n",
      "[[0.04262514 0.05168417 0.05678571 0.07854491 0.25903147 0.07854491]\n",
      " [0.07194661 0.08418564 0.08571429 0.12230375 0.39921335 0.12230375]\n",
      " [0.10549499 0.09423156 0.11111607 0.12402758 0.25517336 0.12402758]\n",
      " [0.13196885 0.14855221 0.14513393 0.18361033 0.36555517 0.18361033]\n",
      " [0.16320356 0.18069003 0.17017857 0.2031913  0.71347329 0.2031913 ]\n",
      " [0.14682981 0.13400609 0.13714286 0.14860325 0.7510021  0.14860325]\n",
      " [0.13294772 0.122051   0.13513393 0.14824965 0.28745365 0.14824965]] -> [0.13821605]\n",
      "[[0.07194661 0.08418564 0.08571429 0.12230375 0.39921335 0.12230375]\n",
      " [0.10549499 0.09423156 0.11111607 0.12402758 0.25517336 0.12402758]\n",
      " [0.13196885 0.14855221 0.14513393 0.18361033 0.36555517 0.18361033]\n",
      " [0.16320356 0.18069003 0.17017857 0.2031913  0.71347329 0.2031913 ]\n",
      " [0.14682981 0.13400609 0.13714286 0.14860325 0.7510021  0.14860325]\n",
      " [0.13294772 0.122051   0.13513393 0.14824965 0.28745365 0.14824965]\n",
      " [0.13766407 0.11773262 0.13276786 0.13821605 0.26424241 0.13821605]] -> [0.13728784]\n",
      "[[0.10549499 0.09423156 0.11111607 0.12402758 0.25517336 0.12402758]\n",
      " [0.13196885 0.14855221 0.14513393 0.18361033 0.36555517 0.18361033]\n",
      " [0.16320356 0.18069003 0.17017857 0.2031913  0.71347329 0.2031913 ]\n",
      " [0.14682981 0.13400609 0.13714286 0.14860325 0.7510021  0.14860325]\n",
      " [0.13294772 0.122051   0.13513393 0.14824965 0.28745365 0.14824965]\n",
      " [0.13766407 0.11773262 0.13276786 0.13821605 0.26424241 0.13821605]\n",
      " [0.12458287 0.110005   0.12285714 0.13728784 0.21645455 0.13728784]] -> [0.17994165]\n",
      "[[0.13196885 0.14855221 0.14513393 0.18361033 0.36555517 0.18361033]\n",
      " [0.16320356 0.18069003 0.17017857 0.2031913  0.71347329 0.2031913 ]\n",
      " [0.14682981 0.13400609 0.13714286 0.14860325 0.7510021  0.14860325]\n",
      " [0.13294772 0.122051   0.13513393 0.14824965 0.28745365 0.14824965]\n",
      " [0.13766407 0.11773262 0.13276786 0.13821605 0.26424241 0.13821605]\n",
      " [0.12458287 0.110005   0.12285714 0.13728784 0.21645455 0.13728784]\n",
      " [0.12760845 0.16005273 0.13178571 0.17994165 0.32812657 0.17994165]] -> [0.22140205]\n",
      "[[0.16320356 0.18069003 0.17017857 0.2031913  0.71347329 0.2031913 ]\n",
      " [0.14682981 0.13400609 0.13714286 0.14860325 0.7510021  0.14860325]\n",
      " [0.13294772 0.122051   0.13513393 0.14824965 0.28745365 0.14824965]\n",
      " [0.13766407 0.11773262 0.13276786 0.13821605 0.26424241 0.13821605]\n",
      " [0.12458287 0.110005   0.12285714 0.13728784 0.21645455 0.13728784]\n",
      " [0.12760845 0.16005273 0.13178571 0.17994165 0.32812657 0.17994165]\n",
      " [0.15933259 0.20223647 0.17419643 0.22140205 0.34675318 0.22140205]] -> [0.21291549]\n",
      "[[0.14682981 0.13400609 0.13714286 0.14860325 0.7510021  0.14860325]\n",
      " [0.13294772 0.122051   0.13513393 0.14824965 0.28745365 0.14824965]\n",
      " [0.13766407 0.11773262 0.13276786 0.13821605 0.26424241 0.13821605]\n",
      " [0.12458287 0.110005   0.12285714 0.13728784 0.21645455 0.13728784]\n",
      " [0.12760845 0.16005273 0.13178571 0.17994165 0.32812657 0.17994165]\n",
      " [0.15933259 0.20223647 0.17419643 0.22140205 0.34675318 0.22140205]\n",
      " [0.20484983 0.18523569 0.18584821 0.21291549 0.20614541 0.21291549]] -> [0.19103607]\n",
      "[[0.13294772 0.122051   0.13513393 0.14824965 0.28745365 0.14824965]\n",
      " [0.13766407 0.11773262 0.13276786 0.13821605 0.26424241 0.13821605]\n",
      " [0.12458287 0.110005   0.12285714 0.13728784 0.21645455 0.13728784]\n",
      " [0.12760845 0.16005273 0.13178571 0.17994165 0.32812657 0.17994165]\n",
      " [0.15933259 0.20223647 0.17419643 0.22140205 0.34675318 0.22140205]\n",
      " [0.20484983 0.18523569 0.18584821 0.21291549 0.20614541 0.21291549]\n",
      " [0.18242492 0.16732579 0.17910714 0.19103607 0.2096653  0.19103607]] -> [0.17507956]\n",
      "[[0.13766407 0.11773262 0.13276786 0.13821605 0.26424241 0.13821605]\n",
      " [0.12458287 0.110005   0.12285714 0.13728784 0.21645455 0.13728784]\n",
      " [0.12760845 0.16005273 0.13178571 0.17994165 0.32812657 0.17994165]\n",
      " [0.15933259 0.20223647 0.17419643 0.22140205 0.34675318 0.22140205]\n",
      " [0.20484983 0.18523569 0.18584821 0.21291549 0.20614541 0.21291549]\n",
      " [0.18242492 0.16732579 0.17910714 0.19103607 0.2096653  0.19103607]\n",
      " [0.16565072 0.14455203 0.1603125  0.17507956 0.1413343  0.17507956]] -> [0.21662836]\n",
      "[[0.12458287 0.110005   0.12285714 0.13728784 0.21645455 0.13728784]\n",
      " [0.12760845 0.16005273 0.13178571 0.17994165 0.32812657 0.17994165]\n",
      " [0.15933259 0.20223647 0.17419643 0.22140205 0.34675318 0.22140205]\n",
      " [0.20484983 0.18523569 0.18584821 0.21291549 0.20614541 0.21291549]\n",
      " [0.18242492 0.16732579 0.17910714 0.19103607 0.2096653  0.19103607]\n",
      " [0.16565072 0.14455203 0.1603125  0.17507956 0.1413343  0.17507956]\n",
      " [0.18958843 0.20669121 0.20816964 0.21662836 0.29310302 0.21662836]] -> [0.20518034]\n",
      "[[0.12760845 0.16005273 0.13178571 0.17994165 0.32812657 0.17994165]\n",
      " [0.15933259 0.20223647 0.17419643 0.22140205 0.34675318 0.22140205]\n",
      " [0.20484983 0.18523569 0.18584821 0.21291549 0.20614541 0.21291549]\n",
      " [0.18242492 0.16732579 0.17910714 0.19103607 0.2096653  0.19103607]\n",
      " [0.16565072 0.14455203 0.1603125  0.17507956 0.1413343  0.17507956]\n",
      " [0.18958843 0.20669121 0.20816964 0.21662836 0.29310302 0.21662836]\n",
      " [0.22238042 0.21228238 0.20736607 0.20518034 0.25073905 0.20518034]] -> [0.24566832]\n",
      "[[0.15933259 0.20223647 0.17419643 0.22140205 0.34675318 0.22140205]\n",
      " [0.20484983 0.18523569 0.18584821 0.21291549 0.20614541 0.21291549]\n",
      " [0.18242492 0.16732579 0.17910714 0.19103607 0.2096653  0.19103607]\n",
      " [0.16565072 0.14455203 0.1603125  0.17507956 0.1413343  0.17507956]\n",
      " [0.18958843 0.20669121 0.20816964 0.21662836 0.29310302 0.21662836]\n",
      " [0.22238042 0.21228238 0.20736607 0.20518034 0.25073905 0.20518034]\n",
      " [0.21686318 0.21773717 0.230625   0.24566832 0.2169055  0.24566832]] -> [0.25238685]\n",
      "[[0.20484983 0.18523569 0.18584821 0.21291549 0.20614541 0.21291549]\n",
      " [0.18242492 0.16732579 0.17910714 0.19103607 0.2096653  0.19103607]\n",
      " [0.16565072 0.14455203 0.1603125  0.17507956 0.1413343  0.17507956]\n",
      " [0.18958843 0.20669121 0.20816964 0.21662836 0.29310302 0.21662836]\n",
      " [0.22238042 0.21228238 0.20736607 0.20518034 0.25073905 0.20518034]\n",
      " [0.21686318 0.21773717 0.230625   0.24566832 0.2169055  0.24566832]\n",
      " [0.2196663  0.21819174 0.23415179 0.25238685 0.19179026 0.25238685]] -> [0.24093883]\n",
      "[[0.18242492 0.16732579 0.17910714 0.19103607 0.2096653  0.19103607]\n",
      " [0.16565072 0.14455203 0.1603125  0.17507956 0.1413343  0.17507956]\n",
      " [0.18958843 0.20669121 0.20816964 0.21662836 0.29310302 0.21662836]\n",
      " [0.22238042 0.21228238 0.20736607 0.20518034 0.25073905 0.20518034]\n",
      " [0.21686318 0.21773717 0.230625   0.24566832 0.2169055  0.24566832]\n",
      " [0.2196663  0.21819174 0.23415179 0.25238685 0.19179026 0.25238685]\n",
      " [0.24916574 0.23141961 0.23169643 0.24093883 0.18580269 0.24093883]] -> [0.23749116]\n",
      "[[0.16565072 0.14455203 0.1603125  0.17507956 0.1413343  0.17507956]\n",
      " [0.18958843 0.20669121 0.20816964 0.21662836 0.29310302 0.21662836]\n",
      " [0.22238042 0.21228238 0.20736607 0.20518034 0.25073905 0.20518034]\n",
      " [0.21686318 0.21773717 0.230625   0.24566832 0.2169055  0.24566832]\n",
      " [0.2196663  0.21819174 0.23415179 0.25238685 0.19179026 0.25238685]\n",
      " [0.24916574 0.23141961 0.23169643 0.24093883 0.18580269 0.24093883]\n",
      " [0.24160178 0.22219192 0.23459821 0.23749116 0.15685439 0.23749116]] -> [0.26728253]\n",
      "[[0.18958843 0.20669121 0.20816964 0.21662836 0.29310302 0.21662836]\n",
      " [0.22238042 0.21228238 0.20736607 0.20518034 0.25073905 0.20518034]\n",
      " [0.21686318 0.21773717 0.230625   0.24566832 0.2169055  0.24566832]\n",
      " [0.2196663  0.21819174 0.23415179 0.25238685 0.19179026 0.25238685]\n",
      " [0.24916574 0.23141961 0.23169643 0.24093883 0.18580269 0.24093883]\n",
      " [0.24160178 0.22219192 0.23459821 0.23749116 0.15685439 0.23749116]\n",
      " [0.24769744 0.24282922 0.2634375  0.26728253 0.21099309 0.26728253]] -> [0.26511669]\n",
      "[[0.22238042 0.21228238 0.20736607 0.20518034 0.25073905 0.20518034]\n",
      " [0.21686318 0.21773717 0.230625   0.24566832 0.2169055  0.24566832]\n",
      " [0.2196663  0.21819174 0.23415179 0.25238685 0.19179026 0.25238685]\n",
      " [0.24916574 0.23141961 0.23169643 0.24093883 0.18580269 0.24093883]\n",
      " [0.24160178 0.22219192 0.23459821 0.23749116 0.15685439 0.23749116]\n",
      " [0.24769744 0.24282922 0.2634375  0.26728253 0.21099309 0.26728253]\n",
      " [0.24533927 0.2382381  0.25808036 0.26511669 0.11064485 0.26511669]] -> [0.25331506]\n",
      "[[0.21686318 0.21773717 0.230625   0.24566832 0.2169055  0.24566832]\n",
      " [0.2196663  0.21819174 0.23415179 0.25238685 0.19179026 0.25238685]\n",
      " [0.24916574 0.23141961 0.23169643 0.24093883 0.18580269 0.24093883]\n",
      " [0.24160178 0.22219192 0.23459821 0.23749116 0.15685439 0.23749116]\n",
      " [0.24769744 0.24282922 0.2634375  0.26728253 0.21099309 0.26728253]\n",
      " [0.24533927 0.2382381  0.25808036 0.26511669 0.11064485 0.26511669]\n",
      " [0.23879867 0.23287422 0.25178571 0.25331506 0.10694959 0.25331506]] -> [0.27382426]\n",
      "[[0.2196663  0.21819174 0.23415179 0.25238685 0.19179026 0.25238685]\n",
      " [0.24916574 0.23141961 0.23169643 0.24093883 0.18580269 0.24093883]\n",
      " [0.24160178 0.22219192 0.23459821 0.23749116 0.15685439 0.23749116]\n",
      " [0.24769744 0.24282922 0.2634375  0.26728253 0.21099309 0.26728253]\n",
      " [0.24533927 0.2382381  0.25808036 0.26511669 0.11064485 0.26511669]\n",
      " [0.23879867 0.23287422 0.25178571 0.25331506 0.10694959 0.25331506]\n",
      " [0.2465406  0.25342061 0.263125   0.27382426 0.18572753 0.27382426]] -> [0.29017857]\n",
      "[[0.24916574 0.23141961 0.23169643 0.24093883 0.18580269 0.24093883]\n",
      " [0.24160178 0.22219192 0.23459821 0.23749116 0.15685439 0.23749116]\n",
      " [0.24769744 0.24282922 0.2634375  0.26728253 0.21099309 0.26728253]\n",
      " [0.24533927 0.2382381  0.25808036 0.26511669 0.11064485 0.26511669]\n",
      " [0.23879867 0.23287422 0.25178571 0.25331506 0.10694959 0.25331506]\n",
      " [0.2465406  0.25342061 0.263125   0.27382426 0.18572753 0.27382426]\n",
      " [0.27652948 0.26692122 0.2809375  0.29017857 0.14236146 0.29017857]] -> [0.28010078]\n",
      "[[0.24160178 0.22219192 0.23459821 0.23749116 0.15685439 0.23749116]\n",
      " [0.24769744 0.24282922 0.2634375  0.26728253 0.21099309 0.26728253]\n",
      " [0.24533927 0.2382381  0.25808036 0.26511669 0.11064485 0.26511669]\n",
      " [0.23879867 0.23287422 0.25178571 0.25331506 0.10694959 0.25331506]\n",
      " [0.2465406  0.25342061 0.263125   0.27382426 0.18572753 0.27382426]\n",
      " [0.27652948 0.26692122 0.2809375  0.29017857 0.14236146 0.29017857]\n",
      " [0.27933259 0.25992091 0.27191964 0.28010078 0.10226476 0.28010078]] -> [0.2133133]\n",
      "[[0.24769744 0.24282922 0.2634375  0.26728253 0.21099309 0.26728253]\n",
      " [0.24533927 0.2382381  0.25808036 0.26511669 0.11064485 0.26511669]\n",
      " [0.23879867 0.23287422 0.25178571 0.25331506 0.10694959 0.25331506]\n",
      " [0.2465406  0.25342061 0.263125   0.27382426 0.18572753 0.27382426]\n",
      " [0.27652948 0.26692122 0.2809375  0.29017857 0.14236146 0.29017857]\n",
      " [0.27933259 0.25992091 0.27191964 0.28010078 0.10226476 0.28010078]\n",
      " [0.22914349 0.20905496 0.21044643 0.2133133  0.22065087 0.2133133 ]] -> [0.21503713]\n",
      "[[0.24533927 0.2382381  0.25808036 0.26511669 0.11064485 0.26511669]\n",
      " [0.23879867 0.23287422 0.25178571 0.25331506 0.10694959 0.25331506]\n",
      " [0.2465406  0.25342061 0.263125   0.27382426 0.18572753 0.27382426]\n",
      " [0.27652948 0.26692122 0.2809375  0.29017857 0.14236146 0.29017857]\n",
      " [0.27933259 0.25992091 0.27191964 0.28010078 0.10226476 0.28010078]\n",
      " [0.22914349 0.20905496 0.21044643 0.2133133  0.22065087 0.2133133 ]\n",
      " [0.20066741 0.18955407 0.206875   0.21503713 0.18694258 0.21503713]] -> [0.20924682]\n",
      "[[0.23879867 0.23287422 0.25178571 0.25331506 0.10694959 0.25331506]\n",
      " [0.2465406  0.25342061 0.263125   0.27382426 0.18572753 0.27382426]\n",
      " [0.27652948 0.26692122 0.2809375  0.29017857 0.14236146 0.29017857]\n",
      " [0.27933259 0.25992091 0.27191964 0.28010078 0.10226476 0.28010078]\n",
      " [0.22914349 0.20905496 0.21044643 0.2133133  0.22065087 0.2133133 ]\n",
      " [0.20066741 0.18955407 0.206875   0.21503713 0.18694258 0.21503713]\n",
      " [0.17793103 0.17728079 0.19008929 0.20924682 0.1749048  0.20924682]] -> [0.28191301]\n",
      "[[0.2465406  0.25342061 0.263125   0.27382426 0.18572753 0.27382426]\n",
      " [0.27652948 0.26692122 0.2809375  0.29017857 0.14236146 0.29017857]\n",
      " [0.27933259 0.25992091 0.27191964 0.28010078 0.10226476 0.28010078]\n",
      " [0.22914349 0.20905496 0.21044643 0.2133133  0.22065087 0.2133133 ]\n",
      " [0.20066741 0.18955407 0.206875   0.21503713 0.18694258 0.21503713]\n",
      " [0.17793103 0.17728079 0.19008929 0.20924682 0.1749048  0.20924682]\n",
      " [0.23136819 0.25537524 0.24665179 0.28191301 0.39335104 0.28191301]] -> [0.30326202]\n",
      "[[0.27652948 0.26692122 0.2809375  0.29017857 0.14236146 0.29017857]\n",
      " [0.27933259 0.25992091 0.27191964 0.28010078 0.10226476 0.28010078]\n",
      " [0.22914349 0.20905496 0.21044643 0.2133133  0.22065087 0.2133133 ]\n",
      " [0.20066741 0.18955407 0.206875   0.21503713 0.18694258 0.21503713]\n",
      " [0.17793103 0.17728079 0.19008929 0.20924682 0.1749048  0.20924682]\n",
      " [0.23136819 0.25537524 0.24665179 0.28191301 0.39335104 0.28191301]\n",
      " [0.2916129  0.27314878 0.29763393 0.30326202 0.33843572 0.30326202]] -> [0.31868812]\n",
      "[[0.27933259 0.25992091 0.27191964 0.28010078 0.10226476 0.28010078]\n",
      " [0.22914349 0.20905496 0.21044643 0.2133133  0.22065087 0.2133133 ]\n",
      " [0.20066741 0.18955407 0.206875   0.21503713 0.18694258 0.21503713]\n",
      " [0.17793103 0.17728079 0.19008929 0.20924682 0.1749048  0.20924682]\n",
      " [0.23136819 0.25537524 0.24665179 0.28191301 0.39335104 0.28191301]\n",
      " [0.2916129  0.27314878 0.29763393 0.30326202 0.33843572 0.30326202]\n",
      " [0.2996663  0.29133142 0.30553571 0.31868812 0.19975699 0.31868812]] -> [0.33035714]\n",
      "[[0.22914349 0.20905496 0.21044643 0.2133133  0.22065087 0.2133133 ]\n",
      " [0.20066741 0.18955407 0.206875   0.21503713 0.18694258 0.21503713]\n",
      " [0.17793103 0.17728079 0.19008929 0.20924682 0.1749048  0.20924682]\n",
      " [0.23136819 0.25537524 0.24665179 0.28191301 0.39335104 0.28191301]\n",
      " [0.2916129  0.27314878 0.29763393 0.30326202 0.33843572 0.30326202]\n",
      " [0.2996663  0.29133142 0.30553571 0.31868812 0.19975699 0.31868812]\n",
      " [0.30758621 0.31069594 0.32473214 0.33035714 0.18567742 0.33035714]] -> [0.31546146]\n",
      "[[0.20066741 0.18955407 0.206875   0.21503713 0.18694258 0.21503713]\n",
      " [0.17793103 0.17728079 0.19008929 0.20924682 0.1749048  0.20924682]\n",
      " [0.23136819 0.25537524 0.24665179 0.28191301 0.39335104 0.28191301]\n",
      " [0.2916129  0.27314878 0.29763393 0.30326202 0.33843572 0.30326202]\n",
      " [0.2996663  0.29133142 0.30553571 0.31868812 0.19975699 0.31868812]\n",
      " [0.30758621 0.31069594 0.32473214 0.33035714 0.18567742 0.33035714]\n",
      " [0.32373749 0.3045593  0.31571429 0.31546146 0.14143451 0.31546146]] -> [0.30688649]\n",
      "[[0.17793103 0.17728079 0.19008929 0.20924682 0.1749048  0.20924682]\n",
      " [0.23136819 0.25537524 0.24665179 0.28191301 0.39335104 0.28191301]\n",
      " [0.2916129  0.27314878 0.29763393 0.30326202 0.33843572 0.30326202]\n",
      " [0.2996663  0.29133142 0.30553571 0.31868812 0.19975699 0.31868812]\n",
      " [0.30758621 0.31069594 0.32473214 0.33035714 0.18567742 0.33035714]\n",
      " [0.32373749 0.3045593  0.31571429 0.31546146 0.14143451 0.31546146]\n",
      " [0.30513904 0.28446748 0.2890625  0.30688649 0.14282493 0.30688649]] -> [0.30109618]\n",
      "[[0.23136819 0.25537524 0.24665179 0.28191301 0.39335104 0.28191301]\n",
      " [0.2916129  0.27314878 0.29763393 0.30326202 0.33843572 0.30326202]\n",
      " [0.2996663  0.29133142 0.30553571 0.31868812 0.19975699 0.31868812]\n",
      " [0.30758621 0.31069594 0.32473214 0.33035714 0.18567742 0.33035714]\n",
      " [0.32373749 0.3045593  0.31571429 0.31546146 0.14143451 0.31546146]\n",
      " [0.30513904 0.28446748 0.2890625  0.30688649 0.14282493 0.30688649]\n",
      " [0.30816463 0.30169553 0.30089286 0.30109618 0.11435264 0.30109618]] -> [0.28748232]\n",
      "[[0.2916129  0.27314878 0.29763393 0.30326202 0.33843572 0.30326202]\n",
      " [0.2996663  0.29133142 0.30553571 0.31868812 0.19975699 0.31868812]\n",
      " [0.30758621 0.31069594 0.32473214 0.33035714 0.18567742 0.33035714]\n",
      " [0.32373749 0.3045593  0.31571429 0.31546146 0.14143451 0.31546146]\n",
      " [0.30513904 0.28446748 0.2890625  0.30688649 0.14282493 0.30688649]\n",
      " [0.30816463 0.30169553 0.30089286 0.30109618 0.11435264 0.30109618]\n",
      " [0.26602892 0.25392063 0.276875   0.28748232 0.13768915 0.28748232]] -> [0.26135962]\n",
      "[[0.2996663  0.29133142 0.30553571 0.31868812 0.19975699 0.31868812]\n",
      " [0.30758621 0.31069594 0.32473214 0.33035714 0.18567742 0.33035714]\n",
      " [0.32373749 0.3045593  0.31571429 0.31546146 0.14143451 0.31546146]\n",
      " [0.30513904 0.28446748 0.2890625  0.30688649 0.14282493 0.30688649]\n",
      " [0.30816463 0.30169553 0.30089286 0.30109618 0.11435264 0.30109618]\n",
      " [0.26602892 0.25392063 0.276875   0.28748232 0.13768915 0.28748232]\n",
      " [0.26553949 0.27646711 0.26089286 0.26135962 0.21748171 0.26135962]] -> [0.24920438]\n",
      "[[0.30758621 0.31069594 0.32473214 0.33035714 0.18567742 0.33035714]\n",
      " [0.32373749 0.3045593  0.31571429 0.31546146 0.14143451 0.31546146]\n",
      " [0.30513904 0.28446748 0.2890625  0.30688649 0.14282493 0.30688649]\n",
      " [0.30816463 0.30169553 0.30089286 0.30109618 0.11435264 0.30109618]\n",
      " [0.26602892 0.25392063 0.276875   0.28748232 0.13768915 0.28748232]\n",
      " [0.26553949 0.27646711 0.26089286 0.26135962 0.21748171 0.26135962]\n",
      " [0.24836485 0.23787445 0.25272321 0.24920438 0.11851137 0.24920438]] -> [0.26913897]\n",
      "[[0.32373749 0.3045593  0.31571429 0.31546146 0.14143451 0.31546146]\n",
      " [0.30513904 0.28446748 0.2890625  0.30688649 0.14282493 0.30688649]\n",
      " [0.30816463 0.30169553 0.30089286 0.30109618 0.11435264 0.30109618]\n",
      " [0.26602892 0.25392063 0.276875   0.28748232 0.13768915 0.28748232]\n",
      " [0.26553949 0.27646711 0.26089286 0.26135962 0.21748171 0.26135962]\n",
      " [0.24836485 0.23787445 0.25272321 0.24920438 0.11851137 0.24920438]\n",
      " [0.24836485 0.2375108  0.26348214 0.26913897 0.09864465 0.26913897]] -> [0.28584689]\n",
      "[[0.30513904 0.28446748 0.2890625  0.30688649 0.14282493 0.30688649]\n",
      " [0.30816463 0.30169553 0.30089286 0.30109618 0.11435264 0.30109618]\n",
      " [0.26602892 0.25392063 0.276875   0.28748232 0.13768915 0.28748232]\n",
      " [0.26553949 0.27646711 0.26089286 0.26135962 0.21748171 0.26135962]\n",
      " [0.24836485 0.23787445 0.25272321 0.24920438 0.11851137 0.24920438]\n",
      " [0.24836485 0.2375108  0.26348214 0.26913897 0.09864465 0.26913897]\n",
      " [0.25615128 0.25728442 0.26696429 0.28584689 0.08023099 0.28584689]] -> [0.27400106]\n",
      "[[0.30816463 0.30169553 0.30089286 0.30109618 0.11435264 0.30109618]\n",
      " [0.26602892 0.25392063 0.276875   0.28748232 0.13768915 0.28748232]\n",
      " [0.26553949 0.27646711 0.26089286 0.26135962 0.21748171 0.26135962]\n",
      " [0.24836485 0.23787445 0.25272321 0.24920438 0.11851137 0.24920438]\n",
      " [0.24836485 0.2375108  0.26348214 0.26913897 0.09864465 0.26913897]\n",
      " [0.25615128 0.25728442 0.26696429 0.28584689 0.08023099 0.28584689]\n",
      " [0.28605117 0.27723987 0.26723214 0.27400106 0.22522297 0.27400106]] -> [0.29764851]\n",
      "[[0.26602892 0.25392063 0.276875   0.28748232 0.13768915 0.28748232]\n",
      " [0.26553949 0.27646711 0.26089286 0.26135962 0.21748171 0.26135962]\n",
      " [0.24836485 0.23787445 0.25272321 0.24920438 0.11851137 0.24920438]\n",
      " [0.24836485 0.2375108  0.26348214 0.26913897 0.09864465 0.26913897]\n",
      " [0.25615128 0.25728442 0.26696429 0.28584689 0.08023099 0.28584689]\n",
      " [0.28605117 0.27723987 0.26723214 0.27400106 0.22522297 0.27400106]\n",
      " [0.26278087 0.273967   0.28044643 0.29764851 0.16743912 0.29764851]] -> [0.32845651]\n",
      "[[0.26553949 0.27646711 0.26089286 0.26135962 0.21748171 0.26135962]\n",
      " [0.24836485 0.23787445 0.25272321 0.24920438 0.11851137 0.24920438]\n",
      " [0.24836485 0.2375108  0.26348214 0.26913897 0.09864465 0.26913897]\n",
      " [0.25615128 0.25728442 0.26696429 0.28584689 0.08023099 0.28584689]\n",
      " [0.28605117 0.27723987 0.26723214 0.27400106 0.22522297 0.27400106]\n",
      " [0.26278087 0.273967   0.28044643 0.29764851 0.16743912 0.29764851]\n",
      " [0.29655172 0.29519524 0.29473214 0.32845651 0.20992835 0.32845651]] -> [0.33376061]\n",
      "[[0.24836485 0.23787445 0.25272321 0.24920438 0.11851137 0.24920438]\n",
      " [0.24836485 0.2375108  0.26348214 0.26913897 0.09864465 0.26913897]\n",
      " [0.25615128 0.25728442 0.26696429 0.28584689 0.08023099 0.28584689]\n",
      " [0.28605117 0.27723987 0.26723214 0.27400106 0.22522297 0.27400106]\n",
      " [0.26278087 0.273967   0.28044643 0.29764851 0.16743912 0.29764851]\n",
      " [0.29655172 0.29519524 0.29473214 0.32845651 0.20992835 0.32845651]\n",
      " [0.31323693 0.305923   0.31986607 0.33376061 0.13079968 0.33376061]] -> [0.34693246]\n",
      "[[0.24836485 0.2375108  0.26348214 0.26913897 0.09864465 0.26913897]\n",
      " [0.25615128 0.25728442 0.26696429 0.28584689 0.08023099 0.28584689]\n",
      " [0.28605117 0.27723987 0.26723214 0.27400106 0.22522297 0.27400106]\n",
      " [0.26278087 0.273967   0.28044643 0.29764851 0.16743912 0.29764851]\n",
      " [0.29655172 0.29519524 0.29473214 0.32845651 0.20992835 0.32845651]\n",
      " [0.31323693 0.305923   0.31986607 0.33376061 0.13079968 0.33376061]\n",
      " [0.31372636 0.32183281 0.33066964 0.34693246 0.13991883 0.34693246]] -> [0.35762907]\n",
      "[[0.25615128 0.25728442 0.26696429 0.28584689 0.08023099 0.28584689]\n",
      " [0.28605117 0.27723987 0.26723214 0.27400106 0.22522297 0.27400106]\n",
      " [0.26278087 0.273967   0.28044643 0.29764851 0.16743912 0.29764851]\n",
      " [0.29655172 0.29519524 0.29473214 0.32845651 0.20992835 0.32845651]\n",
      " [0.31323693 0.305923   0.31986607 0.33376061 0.13079968 0.33376061]\n",
      " [0.31372636 0.32183281 0.33066964 0.34693246 0.13991883 0.34693246]\n",
      " [0.33406007 0.33142416 0.34254464 0.35762907 0.19282994 0.35762907]] -> [0.40514498]\n",
      "[[0.28605117 0.27723987 0.26723214 0.27400106 0.22522297 0.27400106]\n",
      " [0.26278087 0.273967   0.28044643 0.29764851 0.16743912 0.29764851]\n",
      " [0.29655172 0.29519524 0.29473214 0.32845651 0.20992835 0.32845651]\n",
      " [0.31323693 0.305923   0.31986607 0.33376061 0.13079968 0.33376061]\n",
      " [0.31372636 0.32183281 0.33066964 0.34693246 0.13991883 0.34693246]\n",
      " [0.33406007 0.33142416 0.34254464 0.35762907 0.19282994 0.35762907]\n",
      " [0.35506118 0.37897177 0.36830357 0.40514498 0.21627919 0.40514498]] -> [0.42030587]\n",
      "[[0.26278087 0.273967   0.28044643 0.29764851 0.16743912 0.29764851]\n",
      " [0.29655172 0.29519524 0.29473214 0.32845651 0.20992835 0.32845651]\n",
      " [0.31323693 0.305923   0.31986607 0.33376061 0.13079968 0.33376061]\n",
      " [0.31372636 0.32183281 0.33066964 0.34693246 0.13991883 0.34693246]\n",
      " [0.33406007 0.33142416 0.34254464 0.35762907 0.19282994 0.35762907]\n",
      " [0.35506118 0.37897177 0.36830357 0.40514498 0.21627919 0.40514498]\n",
      " [0.40298109 0.41533706 0.40700893 0.42030587 0.45400341 0.42030587]] -> [0.41915665]\n",
      "[[0.29655172 0.29519524 0.29473214 0.32845651 0.20992835 0.32845651]\n",
      " [0.31323693 0.305923   0.31986607 0.33376061 0.13079968 0.33376061]\n",
      " [0.31372636 0.32183281 0.33066964 0.34693246 0.13991883 0.34693246]\n",
      " [0.33406007 0.33142416 0.34254464 0.35762907 0.19282994 0.35762907]\n",
      " [0.35506118 0.37897177 0.36830357 0.40514498 0.21627919 0.40514498]\n",
      " [0.40298109 0.41533706 0.40700893 0.42030587 0.45400341 0.42030587]\n",
      " [0.4305673  0.41356425 0.42522321 0.41915665 0.30373785 0.41915665]] -> [0.44364392]\n",
      "[[0.31323693 0.305923   0.31986607 0.33376061 0.13079968 0.33376061]\n",
      " [0.31372636 0.32183281 0.33066964 0.34693246 0.13991883 0.34693246]\n",
      " [0.33406007 0.33142416 0.34254464 0.35762907 0.19282994 0.35762907]\n",
      " [0.35506118 0.37897177 0.36830357 0.40514498 0.21627919 0.40514498]\n",
      " [0.40298109 0.41533706 0.40700893 0.42030587 0.45400341 0.42030587]\n",
      " [0.4305673  0.41356425 0.42522321 0.41915665 0.30373785 0.41915665]\n",
      " [0.39038932 0.42229192 0.40285714 0.44364392 0.15754334 0.44364392]] -> [0.45274929]\n",
      "[[0.31372636 0.32183281 0.33066964 0.34693246 0.13991883 0.34693246]\n",
      " [0.33406007 0.33142416 0.34254464 0.35762907 0.19282994 0.35762907]\n",
      " [0.35506118 0.37897177 0.36830357 0.40514498 0.21627919 0.40514498]\n",
      " [0.40298109 0.41533706 0.40700893 0.42030587 0.45400341 0.42030587]\n",
      " [0.4305673  0.41356425 0.42522321 0.41915665 0.30373785 0.41915665]\n",
      " [0.39038932 0.42229192 0.40285714 0.44364392 0.15754334 0.44364392]\n",
      " [0.44903226 0.43633802 0.45459821 0.45274929 0.27338661 0.45274929]] -> [0.45009724]\n",
      "[[0.33406007 0.33142416 0.34254464 0.35762907 0.19282994 0.35762907]\n",
      " [0.35506118 0.37897177 0.36830357 0.40514498 0.21627919 0.40514498]\n",
      " [0.40298109 0.41533706 0.40700893 0.42030587 0.45400341 0.42030587]\n",
      " [0.4305673  0.41356425 0.42522321 0.41915665 0.30373785 0.41915665]\n",
      " [0.39038932 0.42229192 0.40285714 0.44364392 0.15754334 0.44364392]\n",
      " [0.44903226 0.43633802 0.45459821 0.45274929 0.27338661 0.45274929]\n",
      " [0.44840934 0.45911178 0.45415179 0.45009724 0.23117296 0.45009724]] -> [0.4426273]\n",
      "[[0.35506118 0.37897177 0.36830357 0.40514498 0.21627919 0.40514498]\n",
      " [0.40298109 0.41533706 0.40700893 0.42030587 0.45400341 0.42030587]\n",
      " [0.4305673  0.41356425 0.42522321 0.41915665 0.30373785 0.41915665]\n",
      " [0.39038932 0.42229192 0.40285714 0.44364392 0.15754334 0.44364392]\n",
      " [0.44903226 0.43633802 0.45459821 0.45274929 0.27338661 0.45274929]\n",
      " [0.44840934 0.45911178 0.45415179 0.45009724 0.23117296 0.45009724]\n",
      " [0.44703003 0.43370153 0.433125   0.4426273  0.20905151 0.4426273 ]] -> [0.42366513]\n",
      "[[0.40298109 0.41533706 0.40700893 0.42030587 0.45400341 0.42030587]\n",
      " [0.4305673  0.41356425 0.42522321 0.41915665 0.30373785 0.41915665]\n",
      " [0.39038932 0.42229192 0.40285714 0.44364392 0.15754334 0.44364392]\n",
      " [0.44903226 0.43633802 0.45459821 0.45274929 0.27338661 0.45274929]\n",
      " [0.44840934 0.45911178 0.45415179 0.45009724 0.23117296 0.45009724]\n",
      " [0.44703003 0.43370153 0.433125   0.4426273  0.20905151 0.4426273 ]\n",
      " [0.41953281 0.42697395 0.42866071 0.42366513 0.14917577 0.42366513]] -> [0.45040665]\n",
      "[[0.4305673  0.41356425 0.42522321 0.41915665 0.30373785 0.41915665]\n",
      " [0.39038932 0.42229192 0.40285714 0.44364392 0.15754334 0.44364392]\n",
      " [0.44903226 0.43633802 0.45459821 0.45274929 0.27338661 0.45274929]\n",
      " [0.44840934 0.45911178 0.45415179 0.45009724 0.23117296 0.45009724]\n",
      " [0.44703003 0.43370153 0.433125   0.4426273  0.20905151 0.4426273 ]\n",
      " [0.41953281 0.42697395 0.42866071 0.42366513 0.14917577 0.42366513]\n",
      " [0.42197998 0.43411064 0.43446429 0.45040665 0.12283295 0.45040665]] -> [0.45040665]\n",
      "[[0.39038932 0.42229192 0.40285714 0.44364392 0.15754334 0.44364392]\n",
      " [0.44903226 0.43633802 0.45459821 0.45274929 0.27338661 0.45274929]\n",
      " [0.44840934 0.45911178 0.45415179 0.45009724 0.23117296 0.45009724]\n",
      " [0.44703003 0.43370153 0.433125   0.4426273  0.20905151 0.4426273 ]\n",
      " [0.41953281 0.42697395 0.42866071 0.42366513 0.14917577 0.42366513]\n",
      " [0.42197998 0.43411064 0.43446429 0.45040665 0.12283295 0.45040665]\n",
      " [0.44284761 0.42615573 0.43316964 0.45040665 0.15538882 0.45040665]] -> [0.43856082]\n",
      "[[0.44903226 0.43633802 0.45459821 0.45274929 0.27338661 0.45274929]\n",
      " [0.44840934 0.45911178 0.45415179 0.45009724 0.23117296 0.45009724]\n",
      " [0.44703003 0.43370153 0.433125   0.4426273  0.20905151 0.4426273 ]\n",
      " [0.41953281 0.42697395 0.42866071 0.42366513 0.14917577 0.42366513]\n",
      " [0.42197998 0.43411064 0.43446429 0.45040665 0.12283295 0.45040665]\n",
      " [0.44284761 0.42615573 0.43316964 0.45040665 0.15538882 0.45040665]\n",
      " [0.44493882 0.42792854 0.40741071 0.43856082 0.28597555 0.43856082]] -> [0.40032709]\n",
      "[[0.44840934 0.45911178 0.45415179 0.45009724 0.23117296 0.45009724]\n",
      " [0.44703003 0.43370153 0.433125   0.4426273  0.20905151 0.4426273 ]\n",
      " [0.41953281 0.42697395 0.42866071 0.42366513 0.14917577 0.42366513]\n",
      " [0.42197998 0.43411064 0.43446429 0.45040665 0.12283295 0.45040665]\n",
      " [0.44284761 0.42615573 0.43316964 0.45040665 0.15538882 0.45040665]\n",
      " [0.44493882 0.42792854 0.40741071 0.43856082 0.28597555 0.43856082]\n",
      " [0.4137931  0.41124597 0.40633929 0.40032709 0.24482664 0.40032709]] -> [0.38874646]\n",
      "[[0.44703003 0.43370153 0.433125   0.4426273  0.20905151 0.4426273 ]\n",
      " [0.41953281 0.42697395 0.42866071 0.42366513 0.14917577 0.42366513]\n",
      " [0.42197998 0.43411064 0.43446429 0.45040665 0.12283295 0.45040665]\n",
      " [0.44284761 0.42615573 0.43316964 0.45040665 0.15538882 0.45040665]\n",
      " [0.44493882 0.42792854 0.40741071 0.43856082 0.28597555 0.43856082]\n",
      " [0.4137931  0.41124597 0.40633929 0.40032709 0.24482664 0.40032709]\n",
      " [0.37259177 0.38974499 0.38669643 0.38874646 0.21822076 0.38874646]] -> [0.4061616]\n",
      "[[0.41953281 0.42697395 0.42866071 0.42366513 0.14917577 0.42366513]\n",
      " [0.42197998 0.43411064 0.43446429 0.45040665 0.12283295 0.45040665]\n",
      " [0.44284761 0.42615573 0.43316964 0.45040665 0.15538882 0.45040665]\n",
      " [0.44493882 0.42792854 0.40741071 0.43856082 0.28597555 0.43856082]\n",
      " [0.4137931  0.41124597 0.40633929 0.40032709 0.24482664 0.40032709]\n",
      " [0.37259177 0.38974499 0.38669643 0.38874646 0.21822076 0.38874646]\n",
      " [0.38998888 0.39288149 0.39       0.4061616  0.16107576 0.4061616 ]] -> [0.45133487]\n",
      "[[0.42197998 0.43411064 0.43446429 0.45040665 0.12283295 0.45040665]\n",
      " [0.44284761 0.42615573 0.43316964 0.45040665 0.15538882 0.45040665]\n",
      " [0.44493882 0.42792854 0.40741071 0.43856082 0.28597555 0.43856082]\n",
      " [0.4137931  0.41124597 0.40633929 0.40032709 0.24482664 0.40032709]\n",
      " [0.37259177 0.38974499 0.38669643 0.38874646 0.21822076 0.38874646]\n",
      " [0.38998888 0.39288149 0.39       0.4061616  0.16107576 0.4061616 ]\n",
      " [0.4080089  0.42442838 0.42732143 0.45133487 0.23469285 0.45133487]] -> [0.53443246]\n",
      "[[0.44284761 0.42615573 0.43316964 0.45040665 0.15538882 0.45040665]\n",
      " [0.44493882 0.42792854 0.40741071 0.43856082 0.28597555 0.43856082]\n",
      " [0.4137931  0.41124597 0.40633929 0.40032709 0.24482664 0.40032709]\n",
      " [0.37259177 0.38974499 0.38669643 0.38874646 0.21822076 0.38874646]\n",
      " [0.38998888 0.39288149 0.39       0.4061616  0.16107576 0.4061616 ]\n",
      " [0.4080089  0.42442838 0.42732143 0.45133487 0.23469285 0.45133487]\n",
      " [0.44048943 0.51361425 0.45459821 0.53443246 0.50299379 0.53443246]] -> [0.51958098]\n",
      "[[0.44493882 0.42792854 0.40741071 0.43856082 0.28597555 0.43856082]\n",
      " [0.4137931  0.41124597 0.40633929 0.40032709 0.24482664 0.40032709]\n",
      " [0.37259177 0.38974499 0.38669643 0.38874646 0.21822076 0.38874646]\n",
      " [0.38998888 0.39288149 0.39       0.4061616  0.16107576 0.4061616 ]\n",
      " [0.4080089  0.42442838 0.42732143 0.45133487 0.23469285 0.45133487]\n",
      " [0.44048943 0.51361425 0.45459821 0.53443246 0.50299379 0.53443246]\n",
      " [0.53201335 0.53297877 0.52392857 0.51958098 0.43154374 0.51958098]] -> [0.52873055]\n",
      "[[0.4137931  0.41124597 0.40633929 0.40032709 0.24482664 0.40032709]\n",
      " [0.37259177 0.38974499 0.38669643 0.38874646 0.21822076 0.38874646]\n",
      " [0.38998888 0.39288149 0.39       0.4061616  0.16107576 0.4061616 ]\n",
      " [0.4080089  0.42442838 0.42732143 0.45133487 0.23469285 0.45133487]\n",
      " [0.44048943 0.51361425 0.45459821 0.53443246 0.50299379 0.53443246]\n",
      " [0.53201335 0.53297877 0.52392857 0.51958098 0.43154374 0.51958098]\n",
      " [0.51902113 0.52506932 0.52910714 0.52873055 0.23541938 0.52873055]] -> [0.56316301]\n",
      "[[0.37259177 0.38974499 0.38669643 0.38874646 0.21822076 0.38874646]\n",
      " [0.38998888 0.39288149 0.39       0.4061616  0.16107576 0.4061616 ]\n",
      " [0.4080089  0.42442838 0.42732143 0.45133487 0.23469285 0.45133487]\n",
      " [0.44048943 0.51361425 0.45459821 0.53443246 0.50299379 0.53443246]\n",
      " [0.53201335 0.53297877 0.52392857 0.51958098 0.43154374 0.51958098]\n",
      " [0.51902113 0.52506932 0.52910714 0.52873055 0.23541938 0.52873055]\n",
      " [0.54914349 0.54438838 0.550625   0.56316301 0.30858553 0.56316301]] -> [0.57231259]\n",
      "[[0.38998888 0.39288149 0.39       0.4061616  0.16107576 0.4061616 ]\n",
      " [0.4080089  0.42442838 0.42732143 0.45133487 0.23469285 0.45133487]\n",
      " [0.44048943 0.51361425 0.45459821 0.53443246 0.50299379 0.53443246]\n",
      " [0.53201335 0.53297877 0.52392857 0.51958098 0.43154374 0.51958098]\n",
      " [0.51902113 0.52506932 0.52910714 0.52873055 0.23541938 0.52873055]\n",
      " [0.54914349 0.54438838 0.550625   0.56316301 0.30858553 0.56316301]\n",
      " [0.5534594  0.55211601 0.55982143 0.57231259 0.27291061 0.57231259]] -> [0.61302157]\n",
      "[[0.4080089  0.42442838 0.42732143 0.45133487 0.23469285 0.45133487]\n",
      " [0.44048943 0.51361425 0.45459821 0.53443246 0.50299379 0.53443246]\n",
      " [0.53201335 0.53297877 0.52392857 0.51958098 0.43154374 0.51958098]\n",
      " [0.51902113 0.52506932 0.52910714 0.52873055 0.23541938 0.52873055]\n",
      " [0.54914349 0.54438838 0.550625   0.56316301 0.30858553 0.56316301]\n",
      " [0.5534594  0.55211601 0.55982143 0.57231259 0.27291061 0.57231259]\n",
      " [0.58936596 0.58902677 0.59830357 0.61302157 0.30059375 0.61302157]] -> [0.58910891]\n",
      "[[0.44048943 0.51361425 0.45459821 0.53443246 0.50299379 0.53443246]\n",
      " [0.53201335 0.53297877 0.52392857 0.51958098 0.43154374 0.51958098]\n",
      " [0.51902113 0.52506932 0.52910714 0.52873055 0.23541938 0.52873055]\n",
      " [0.54914349 0.54438838 0.550625   0.56316301 0.30858553 0.56316301]\n",
      " [0.5534594  0.55211601 0.55982143 0.57231259 0.27291061 0.57231259]\n",
      " [0.58936596 0.58902677 0.59830357 0.61302157 0.30059375 0.61302157]\n",
      " [0.59510567 0.59502705 0.58611607 0.58910891 0.65659134 0.58910891]] -> [0.6772454]\n",
      "[[0.53201335 0.53297877 0.52392857 0.51958098 0.43154374 0.51958098]\n",
      " [0.51902113 0.52506932 0.52910714 0.52873055 0.23541938 0.52873055]\n",
      " [0.54914349 0.54438838 0.550625   0.56316301 0.30858553 0.56316301]\n",
      " [0.5534594  0.55211601 0.55982143 0.57231259 0.27291061 0.57231259]\n",
      " [0.58936596 0.58902677 0.59830357 0.61302157 0.30059375 0.61302157]\n",
      " [0.59510567 0.59502705 0.58611607 0.58910891 0.65659134 0.58910891]\n",
      " [0.65552836 0.6761671  0.66316964 0.6772454  1.         0.6772454 ]] -> [0.68714639]\n",
      "[[0.51902113 0.52506932 0.52910714 0.52873055 0.23541938 0.52873055]\n",
      " [0.54914349 0.54438838 0.550625   0.56316301 0.30858553 0.56316301]\n",
      " [0.5534594  0.55211601 0.55982143 0.57231259 0.27291061 0.57231259]\n",
      " [0.58936596 0.58902677 0.59830357 0.61302157 0.30059375 0.61302157]\n",
      " [0.59510567 0.59502705 0.58611607 0.58910891 0.65659134 0.58910891]\n",
      " [0.65552836 0.6761671  0.66316964 0.6772454  1.         0.6772454 ]\n",
      " [0.67937709 0.67093959 0.68191964 0.68714639 0.29552059 0.68714639]] -> [0.68551096]\n",
      "[[0.54914349 0.54438838 0.550625   0.56316301 0.30858553 0.56316301]\n",
      " [0.5534594  0.55211601 0.55982143 0.57231259 0.27291061 0.57231259]\n",
      " [0.58936596 0.58902677 0.59830357 0.61302157 0.30059375 0.61302157]\n",
      " [0.59510567 0.59502705 0.58611607 0.58910891 0.65659134 0.58910891]\n",
      " [0.65552836 0.6761671  0.66316964 0.6772454  1.         0.6772454 ]\n",
      " [0.67937709 0.67093959 0.68191964 0.68714639 0.29552059 0.68714639]\n",
      " [0.67470523 0.66798491 0.64607143 0.68551096 0.39896282 0.68551096]] -> [0.68277051]\n",
      "[[0.5534594  0.55211601 0.55982143 0.57231259 0.27291061 0.57231259]\n",
      " [0.58936596 0.58902677 0.59830357 0.61302157 0.30059375 0.61302157]\n",
      " [0.59510567 0.59502705 0.58611607 0.58910891 0.65659134 0.58910891]\n",
      " [0.65552836 0.6761671  0.66316964 0.6772454  1.         0.6772454 ]\n",
      " [0.67937709 0.67093959 0.68191964 0.68714639 0.29552059 0.68714639]\n",
      " [0.67470523 0.66798491 0.64607143 0.68551096 0.39896282 0.68551096]\n",
      " [0.66700779 0.69707714 0.683125   0.68277051 0.35222718 0.68277051]] -> [0.69598656]\n",
      "[[0.58936596 0.58902677 0.59830357 0.61302157 0.30059375 0.61302157]\n",
      " [0.59510567 0.59502705 0.58611607 0.58910891 0.65659134 0.58910891]\n",
      " [0.65552836 0.6761671  0.66316964 0.6772454  1.         0.6772454 ]\n",
      " [0.67937709 0.67093959 0.68191964 0.68714639 0.29552059 0.68714639]\n",
      " [0.67470523 0.66798491 0.64607143 0.68551096 0.39896282 0.68551096]\n",
      " [0.66700779 0.69707714 0.683125   0.68277051 0.35222718 0.68277051]\n",
      " [0.66740823 0.67730351 0.67866071 0.69598656 0.18511374 0.69598656]] -> [0.69421853]\n",
      "[[0.59510567 0.59502705 0.58611607 0.58910891 0.65659134 0.58910891]\n",
      " [0.65552836 0.6761671  0.66316964 0.6772454  1.         0.6772454 ]\n",
      " [0.67937709 0.67093959 0.68191964 0.68714639 0.29552059 0.68714639]\n",
      " [0.67470523 0.66798491 0.64607143 0.68551096 0.39896282 0.68551096]\n",
      " [0.66700779 0.69707714 0.683125   0.68277051 0.35222718 0.68277051]\n",
      " [0.66740823 0.67730351 0.67866071 0.69598656 0.18511374 0.69598656]\n",
      " [0.69076752 0.69044047 0.69294643 0.69421853 0.19215352 0.69421853]] -> [0.69651697]\n",
      "[[0.65552836 0.6761671  0.66316964 0.6772454  1.         0.6772454 ]\n",
      " [0.67937709 0.67093959 0.68191964 0.68714639 0.29552059 0.68714639]\n",
      " [0.67470523 0.66798491 0.64607143 0.68551096 0.39896282 0.68551096]\n",
      " [0.66700779 0.69707714 0.683125   0.68277051 0.35222718 0.68277051]\n",
      " [0.66740823 0.67730351 0.67866071 0.69598656 0.18511374 0.69598656]\n",
      " [0.69076752 0.69044047 0.69294643 0.69421853 0.19215352 0.69421853]\n",
      " [0.69299221 0.7074867  0.6915625  0.69651697 0.26451799 0.69651697]] -> [0.67035007]\n",
      "[[0.67937709 0.67093959 0.68191964 0.68714639 0.29552059 0.68714639]\n",
      " [0.67470523 0.66798491 0.64607143 0.68551096 0.39896282 0.68551096]\n",
      " [0.66700779 0.69707714 0.683125   0.68277051 0.35222718 0.68277051]\n",
      " [0.66740823 0.67730351 0.67866071 0.69598656 0.18511374 0.69598656]\n",
      " [0.69076752 0.69044047 0.69294643 0.69421853 0.19215352 0.69421853]\n",
      " [0.69299221 0.7074867  0.6915625  0.69651697 0.26451799 0.69651697]\n",
      " [0.67172414 0.67530342 0.66151786 0.67035007 0.29534522 0.67035007]] -> [0.63503359]\n",
      "[[0.67470523 0.66798491 0.64607143 0.68551096 0.39896282 0.68551096]\n",
      " [0.66700779 0.69707714 0.683125   0.68277051 0.35222718 0.68277051]\n",
      " [0.66740823 0.67730351 0.67866071 0.69598656 0.18511374 0.69598656]\n",
      " [0.69076752 0.69044047 0.69294643 0.69421853 0.19215352 0.69421853]\n",
      " [0.69299221 0.7074867  0.6915625  0.69651697 0.26451799 0.69651697]\n",
      " [0.67172414 0.67530342 0.66151786 0.67035007 0.29534522 0.67035007]\n",
      " [0.65793103 0.65457521 0.63513393 0.63503359 0.21408708 0.63503359]] -> [0.6825495]\n",
      "[[0.66700779 0.69707714 0.683125   0.68277051 0.35222718 0.68277051]\n",
      " [0.66740823 0.67730351 0.67866071 0.69598656 0.18511374 0.69598656]\n",
      " [0.69076752 0.69044047 0.69294643 0.69421853 0.19215352 0.69421853]\n",
      " [0.69299221 0.7074867  0.6915625  0.69651697 0.26451799 0.69651697]\n",
      " [0.67172414 0.67530342 0.66151786 0.67035007 0.29534522 0.67035007]\n",
      " [0.65793103 0.65457521 0.63513393 0.63503359 0.21408708 0.63503359]\n",
      " [0.63630701 0.66289377 0.64741071 0.6825495  0.20842519 0.6825495 ]] -> [0.61655764]\n",
      "[[0.66740823 0.67730351 0.67866071 0.69598656 0.18511374 0.69598656]\n",
      " [0.69076752 0.69044047 0.69294643 0.69421853 0.19215352 0.69421853]\n",
      " [0.69299221 0.7074867  0.6915625  0.69651697 0.26451799 0.69651697]\n",
      " [0.67172414 0.67530342 0.66151786 0.67035007 0.29534522 0.67035007]\n",
      " [0.65793103 0.65457521 0.63513393 0.63503359 0.21408708 0.63503359]\n",
      " [0.63630701 0.66289377 0.64741071 0.6825495  0.20842519 0.6825495 ]\n",
      " [0.66740823 0.65534797 0.61723214 0.61655764 0.32691151 0.61655764]] -> [0.60714286]\n",
      "[[0.69076752 0.69044047 0.69294643 0.69421853 0.19215352 0.69421853]\n",
      " [0.69299221 0.7074867  0.6915625  0.69651697 0.26451799 0.69651697]\n",
      " [0.67172414 0.67530342 0.66151786 0.67035007 0.29534522 0.67035007]\n",
      " [0.65793103 0.65457521 0.63513393 0.63503359 0.21408708 0.63503359]\n",
      " [0.63630701 0.66289377 0.64741071 0.6825495  0.20842519 0.6825495 ]\n",
      " [0.66740823 0.65534797 0.61723214 0.61655764 0.32691151 0.61655764]\n",
      " [0.60992214 0.60666394 0.59044643 0.60714286 0.29354144 0.60714286]] -> [0.62172914]\n",
      "[[0.69299221 0.7074867  0.6915625  0.69651697 0.26451799 0.69651697]\n",
      " [0.67172414 0.67530342 0.66151786 0.67035007 0.29534522 0.67035007]\n",
      " [0.65793103 0.65457521 0.63513393 0.63503359 0.21408708 0.63503359]\n",
      " [0.63630701 0.66289377 0.64741071 0.6825495  0.20842519 0.6825495 ]\n",
      " [0.66740823 0.65534797 0.61723214 0.61655764 0.32691151 0.61655764]\n",
      " [0.60992214 0.60666394 0.59044643 0.60714286 0.29354144 0.60714286]\n",
      " [0.57846496 0.59775444 0.59071429 0.62172914 0.19058773 0.62172914]] -> [0.63516619]\n",
      "[[0.67172414 0.67530342 0.66151786 0.67035007 0.29534522 0.67035007]\n",
      " [0.65793103 0.65457521 0.63513393 0.63503359 0.21408708 0.63503359]\n",
      " [0.63630701 0.66289377 0.64741071 0.6825495  0.20842519 0.6825495 ]\n",
      " [0.66740823 0.65534797 0.61723214 0.61655764 0.32691151 0.61655764]\n",
      " [0.60992214 0.60666394 0.59044643 0.60714286 0.29354144 0.60714286]\n",
      " [0.57846496 0.59775444 0.59071429 0.62172914 0.19058773 0.62172914]\n",
      " [0.62647386 0.63411973 0.62696429 0.63516619 0.18463774 0.63516619]] -> [0.6719413]\n",
      "[[0.65793103 0.65457521 0.63513393 0.63503359 0.21408708 0.63503359]\n",
      " [0.63630701 0.66289377 0.64741071 0.6825495  0.20842519 0.6825495 ]\n",
      " [0.66740823 0.65534797 0.61723214 0.61655764 0.32691151 0.61655764]\n",
      " [0.60992214 0.60666394 0.59044643 0.60714286 0.29354144 0.60714286]\n",
      " [0.57846496 0.59775444 0.59071429 0.62172914 0.19058773 0.62172914]\n",
      " [0.62647386 0.63411973 0.62696429 0.63516619 0.18463774 0.63516619]\n",
      " [0.63844271 0.65352971 0.65482143 0.6719413  0.12388516 0.6719413 ]] -> [0.68277051]\n",
      "[[0.63630701 0.66289377 0.64741071 0.6825495  0.20842519 0.6825495 ]\n",
      " [0.66740823 0.65534797 0.61723214 0.61655764 0.32691151 0.61655764]\n",
      " [0.60992214 0.60666394 0.59044643 0.60714286 0.29354144 0.60714286]\n",
      " [0.57846496 0.59775444 0.59071429 0.62172914 0.19058773 0.62172914]\n",
      " [0.62647386 0.63411973 0.62696429 0.63516619 0.18463774 0.63516619]\n",
      " [0.63844271 0.65352971 0.65482143 0.6719413  0.12388516 0.6719413 ]\n",
      " [0.65726363 0.66266648 0.6675     0.68277051 0.12125463 0.68277051]] -> [0.73320368]\n",
      "[[0.66740823 0.65534797 0.61723214 0.61655764 0.32691151 0.61655764]\n",
      " [0.60992214 0.60666394 0.59044643 0.60714286 0.29354144 0.60714286]\n",
      " [0.57846496 0.59775444 0.59071429 0.62172914 0.19058773 0.62172914]\n",
      " [0.62647386 0.63411973 0.62696429 0.63516619 0.18463774 0.63516619]\n",
      " [0.63844271 0.65352971 0.65482143 0.6719413  0.12388516 0.6719413 ]\n",
      " [0.65726363 0.66266648 0.6675     0.68277051 0.12125463 0.68277051]\n",
      " [0.6916574  0.71175963 0.70647321 0.73320368 0.22437118 0.73320368]] -> [0.75198904]\n",
      "[[0.60992214 0.60666394 0.59044643 0.60714286 0.29354144 0.60714286]\n",
      " [0.57846496 0.59775444 0.59071429 0.62172914 0.19058773 0.62172914]\n",
      " [0.62647386 0.63411973 0.62696429 0.63516619 0.18463774 0.63516619]\n",
      " [0.63844271 0.65352971 0.65482143 0.6719413  0.12388516 0.6719413 ]\n",
      " [0.65726363 0.66266648 0.6675     0.68277051 0.12125463 0.68277051]\n",
      " [0.6916574  0.71175963 0.70647321 0.73320368 0.22437118 0.73320368]\n",
      " [0.72404894 0.73994272 0.74116071 0.75198904 0.17187343 0.75198904]] -> [0.76878536]\n",
      "[[0.57846496 0.59775444 0.59071429 0.62172914 0.19058773 0.62172914]\n",
      " [0.62647386 0.63411973 0.62696429 0.63516619 0.18463774 0.63516619]\n",
      " [0.63844271 0.65352971 0.65482143 0.6719413  0.12388516 0.6719413 ]\n",
      " [0.65726363 0.66266648 0.6675     0.68277051 0.12125463 0.68277051]\n",
      " [0.6916574  0.71175963 0.70647321 0.73320368 0.22437118 0.73320368]\n",
      " [0.72404894 0.73994272 0.74116071 0.75198904 0.17187343 0.75198904]\n",
      " [0.7585317  0.76139825 0.75839286 0.76878536 0.18300932 0.76878536]] -> [0.75680693]\n",
      "[[0.62647386 0.63411973 0.62696429 0.63516619 0.18463774 0.63516619]\n",
      " [0.63844271 0.65352971 0.65482143 0.6719413  0.12388516 0.6719413 ]\n",
      " [0.65726363 0.66266648 0.6675     0.68277051 0.12125463 0.68277051]\n",
      " [0.6916574  0.71175963 0.70647321 0.73320368 0.22437118 0.73320368]\n",
      " [0.72404894 0.73994272 0.74116071 0.75198904 0.17187343 0.75198904]\n",
      " [0.7585317  0.76139825 0.75839286 0.76878536 0.18300932 0.76878536]\n",
      " [0.75443826 0.75867085 0.75678571 0.75680693 0.12872031 0.75680693]] -> [0.77537129]\n",
      "[[0.63844271 0.65352971 0.65482143 0.6719413  0.12388516 0.6719413 ]\n",
      " [0.65726363 0.66266648 0.6675     0.68277051 0.12125463 0.68277051]\n",
      " [0.6916574  0.71175963 0.70647321 0.73320368 0.22437118 0.73320368]\n",
      " [0.72404894 0.73994272 0.74116071 0.75198904 0.17187343 0.75198904]\n",
      " [0.7585317  0.76139825 0.75839286 0.76878536 0.18300932 0.76878536]\n",
      " [0.75443826 0.75867085 0.75678571 0.75680693 0.12872031 0.75680693]\n",
      " [0.75323693 0.75689804 0.76174107 0.77537129 0.10122507 0.77537129]] -> [0.79406825]\n",
      "[[0.65726363 0.66266648 0.6675     0.68277051 0.12125463 0.68277051]\n",
      " [0.6916574  0.71175963 0.70647321 0.73320368 0.22437118 0.73320368]\n",
      " [0.72404894 0.73994272 0.74116071 0.75198904 0.17187343 0.75198904]\n",
      " [0.7585317  0.76139825 0.75839286 0.76878536 0.18300932 0.76878536]\n",
      " [0.75443826 0.75867085 0.75678571 0.75680693 0.12872031 0.75680693]\n",
      " [0.75323693 0.75689804 0.76174107 0.77537129 0.10122507 0.77537129]\n",
      " [0.77864294 0.7772626  0.78928571 0.79406825 0.16791512 0.79406825]] -> [0.79941655]\n",
      "[[0.6916574  0.71175963 0.70647321 0.73320368 0.22437118 0.73320368]\n",
      " [0.72404894 0.73994272 0.74116071 0.75198904 0.17187343 0.75198904]\n",
      " [0.7585317  0.76139825 0.75839286 0.76878536 0.18300932 0.76878536]\n",
      " [0.75443826 0.75867085 0.75678571 0.75680693 0.12872031 0.75680693]\n",
      " [0.75323693 0.75689804 0.76174107 0.77537129 0.10122507 0.77537129]\n",
      " [0.77864294 0.7772626  0.78928571 0.79406825 0.16791512 0.79406825]\n",
      " [0.77806452 0.77958089 0.79348214 0.79941655 0.13252831 0.79941655]] -> [0.79570368]\n",
      "[[0.72404894 0.73994272 0.74116071 0.75198904 0.17187343 0.75198904]\n",
      " [0.7585317  0.76139825 0.75839286 0.76878536 0.18300932 0.76878536]\n",
      " [0.75443826 0.75867085 0.75678571 0.75680693 0.12872031 0.75680693]\n",
      " [0.75323693 0.75689804 0.76174107 0.77537129 0.10122507 0.77537129]\n",
      " [0.77864294 0.7772626  0.78928571 0.79406825 0.16791512 0.79406825]\n",
      " [0.77806452 0.77958089 0.79348214 0.79941655 0.13252831 0.79941655]\n",
      " [0.78598443 0.78539934 0.78611607 0.79570368 0.08654424 0.79570368]] -> [0.77952617]\n",
      "[[0.7585317  0.76139825 0.75839286 0.76878536 0.18300932 0.76878536]\n",
      " [0.75443826 0.75867085 0.75678571 0.75680693 0.12872031 0.75680693]\n",
      " [0.75323693 0.75689804 0.76174107 0.77537129 0.10122507 0.77537129]\n",
      " [0.77864294 0.7772626  0.78928571 0.79406825 0.16791512 0.79406825]\n",
      " [0.77806452 0.77958089 0.79348214 0.79941655 0.13252831 0.79941655]\n",
      " [0.78598443 0.78539934 0.78611607 0.79570368 0.08654424 0.79570368]\n",
      " [0.77317019 0.76367108 0.776875   0.77952617 0.16400691 0.77952617]] -> [0.76613331]\n",
      "[[0.75443826 0.75867085 0.75678571 0.75680693 0.12872031 0.75680693]\n",
      " [0.75323693 0.75689804 0.76174107 0.77537129 0.10122507 0.77537129]\n",
      " [0.77864294 0.7772626  0.78928571 0.79406825 0.16791512 0.79406825]\n",
      " [0.77806452 0.77958089 0.79348214 0.79941655 0.13252831 0.79941655]\n",
      " [0.78598443 0.78539934 0.78611607 0.79570368 0.08654424 0.79570368]\n",
      " [0.77317019 0.76367108 0.776875   0.77952617 0.16400691 0.77952617]\n",
      " [0.7541713  0.75276149 0.77419643 0.76613331 0.14373935 0.76613331]] -> [0.82089816]\n",
      "[[0.75323693 0.75689804 0.76174107 0.77537129 0.10122507 0.77537129]\n",
      " [0.77864294 0.7772626  0.78928571 0.79406825 0.16791512 0.79406825]\n",
      " [0.77806452 0.77958089 0.79348214 0.79941655 0.13252831 0.79941655]\n",
      " [0.78598443 0.78539934 0.78611607 0.79570368 0.08654424 0.79570368]\n",
      " [0.77317019 0.76367108 0.776875   0.77952617 0.16400691 0.77952617]\n",
      " [0.7541713  0.75276149 0.77419643 0.76613331 0.14373935 0.76613331]\n",
      " [0.78527252 0.82094641 0.80303571 0.82089816 0.21154424 0.82089816]] -> [0.82416902]\n",
      "[[0.77864294 0.7772626  0.78928571 0.79406825 0.16791512 0.79406825]\n",
      " [0.77806452 0.77958089 0.79348214 0.79941655 0.13252831 0.79941655]\n",
      " [0.78598443 0.78539934 0.78611607 0.79570368 0.08654424 0.79570368]\n",
      " [0.77317019 0.76367108 0.776875   0.77952617 0.16400691 0.77952617]\n",
      " [0.7541713  0.75276149 0.77419643 0.76613331 0.14373935 0.76613331]\n",
      " [0.78527252 0.82094641 0.80303571 0.82089816 0.21154424 0.82089816]\n",
      " [0.81210234 0.81044593 0.80607143 0.82416902 0.09350887 0.82416902]] -> [0.83588225]\n",
      "[[0.77806452 0.77958089 0.79348214 0.79941655 0.13252831 0.79941655]\n",
      " [0.78598443 0.78539934 0.78611607 0.79570368 0.08654424 0.79570368]\n",
      " [0.77317019 0.76367108 0.776875   0.77952617 0.16400691 0.77952617]\n",
      " [0.7541713  0.75276149 0.77419643 0.76613331 0.14373935 0.76613331]\n",
      " [0.78527252 0.82094641 0.80303571 0.82089816 0.21154424 0.82089816]\n",
      " [0.81210234 0.81044593 0.80607143 0.82416902 0.09350887 0.82416902]\n",
      " [0.82941046 0.82303741 0.83352679 0.83588225 0.07547099 0.83588225]] -> [0.80945014]\n",
      "[[0.78598443 0.78539934 0.78611607 0.79570368 0.08654424 0.79570368]\n",
      " [0.77317019 0.76367108 0.776875   0.77952617 0.16400691 0.77952617]\n",
      " [0.7541713  0.75276149 0.77419643 0.76613331 0.14373935 0.76613331]\n",
      " [0.78527252 0.82094641 0.80303571 0.82089816 0.21154424 0.82089816]\n",
      " [0.81210234 0.81044593 0.80607143 0.82416902 0.09350887 0.82416902]\n",
      " [0.82941046 0.82303741 0.83352679 0.83588225 0.07547099 0.83588225]\n",
      " [0.76529477 0.80212737 0.78120536 0.80945014 0.06587584 0.80945014]] -> [0.82377122]\n",
      "[[0.77317019 0.76367108 0.776875   0.77952617 0.16400691 0.77952617]\n",
      " [0.7541713  0.75276149 0.77419643 0.76613331 0.14373935 0.76613331]\n",
      " [0.78527252 0.82094641 0.80303571 0.82089816 0.21154424 0.82089816]\n",
      " [0.81210234 0.81044593 0.80607143 0.82416902 0.09350887 0.82416902]\n",
      " [0.82941046 0.82303741 0.83352679 0.83588225 0.07547099 0.83588225]\n",
      " [0.76529477 0.80212737 0.78120536 0.80945014 0.06587584 0.80945014]\n",
      " [0.80369299 0.80762762 0.80861607 0.82377122 0.10852791 0.82377122]] -> [0.8541372]\n",
      "[[0.7541713  0.75276149 0.77419643 0.76613331 0.14373935 0.76613331]\n",
      " [0.78527252 0.82094641 0.80303571 0.82089816 0.21154424 0.82089816]\n",
      " [0.81210234 0.81044593 0.80607143 0.82416902 0.09350887 0.82416902]\n",
      " [0.82941046 0.82303741 0.83352679 0.83588225 0.07547099 0.83588225]\n",
      " [0.76529477 0.80212737 0.78120536 0.80945014 0.06587584 0.80945014]\n",
      " [0.80369299 0.80762762 0.80861607 0.82377122 0.10852791 0.82377122]\n",
      " [0.8370634  0.84194736 0.83491071 0.8541372  0.18308448 0.8541372 ]] -> [0.8437058]\n",
      "[[0.78527252 0.82094641 0.80303571 0.82089816 0.21154424 0.82089816]\n",
      " [0.81210234 0.81044593 0.80607143 0.82416902 0.09350887 0.82416902]\n",
      " [0.82941046 0.82303741 0.83352679 0.83588225 0.07547099 0.83588225]\n",
      " [0.76529477 0.80212737 0.78120536 0.80945014 0.06587584 0.80945014]\n",
      " [0.80369299 0.80762762 0.80861607 0.82377122 0.10852791 0.82377122]\n",
      " [0.8370634  0.84194736 0.83491071 0.8541372  0.18308448 0.8541372 ]\n",
      " [0.84983315 0.85008409 0.84928571 0.8437058  0.10077413 0.8437058 ]] -> [0.83588225]\n",
      "[[0.81210234 0.81044593 0.80607143 0.82416902 0.09350887 0.82416902]\n",
      " [0.82941046 0.82303741 0.83352679 0.83588225 0.07547099 0.83588225]\n",
      " [0.76529477 0.80212737 0.78120536 0.80945014 0.06587584 0.80945014]\n",
      " [0.80369299 0.80762762 0.80861607 0.82377122 0.10852791 0.82377122]\n",
      " [0.8370634  0.84194736 0.83491071 0.8541372  0.18308448 0.8541372 ]\n",
      " [0.84983315 0.85008409 0.84928571 0.8437058  0.10077413 0.8437058 ]\n",
      " [0.84111235 0.84299286 0.84383929 0.83588225 0.07139994 0.83588225]] -> [0.83265559]\n",
      "[[0.82941046 0.82303741 0.83352679 0.83588225 0.07547099 0.83588225]\n",
      " [0.76529477 0.80212737 0.78120536 0.80945014 0.06587584 0.80945014]\n",
      " [0.80369299 0.80762762 0.80861607 0.82377122 0.10852791 0.82377122]\n",
      " [0.8370634  0.84194736 0.83491071 0.8541372  0.18308448 0.8541372 ]\n",
      " [0.84983315 0.85008409 0.84928571 0.8437058  0.10077413 0.8437058 ]\n",
      " [0.84111235 0.84299286 0.84383929 0.83588225 0.07139994 0.83588225]\n",
      " [0.85882091 0.8583572  0.81785714 0.83265559 0.20729783 0.83265559]] -> [0.83813649]\n",
      "[[0.76529477 0.80212737 0.78120536 0.80945014 0.06587584 0.80945014]\n",
      " [0.80369299 0.80762762 0.80861607 0.82377122 0.10852791 0.82377122]\n",
      " [0.8370634  0.84194736 0.83491071 0.8541372  0.18308448 0.8541372 ]\n",
      " [0.84983315 0.85008409 0.84928571 0.8437058  0.10077413 0.8437058 ]\n",
      " [0.84111235 0.84299286 0.84383929 0.83588225 0.07139994 0.83588225]\n",
      " [0.85882091 0.8583572  0.81785714 0.83265559 0.20729783 0.83265559]\n",
      " [0.8196218  0.83044684 0.82598214 0.83813649 0.09739202 0.83813649]] -> [0.84167256]\n",
      "[[0.80369299 0.80762762 0.80861607 0.82377122 0.10852791 0.82377122]\n",
      " [0.8370634  0.84194736 0.83491071 0.8541372  0.18308448 0.8541372 ]\n",
      " [0.84983315 0.85008409 0.84928571 0.8437058  0.10077413 0.8437058 ]\n",
      " [0.84111235 0.84299286 0.84383929 0.83588225 0.07139994 0.83588225]\n",
      " [0.85882091 0.8583572  0.81785714 0.83265559 0.20729783 0.83265559]\n",
      " [0.8196218  0.83044684 0.82598214 0.83813649 0.09739202 0.83813649]\n",
      " [0.81646274 0.83940179 0.83044643 0.84167256 0.08333751 0.84167256]] -> [0.85038013]\n",
      "[[0.8370634  0.84194736 0.83491071 0.8541372  0.18308448 0.8541372 ]\n",
      " [0.84983315 0.85008409 0.84928571 0.8437058  0.10077413 0.8437058 ]\n",
      " [0.84111235 0.84299286 0.84383929 0.83588225 0.07139994 0.83588225]\n",
      " [0.85882091 0.8583572  0.81785714 0.83265559 0.20729783 0.83265559]\n",
      " [0.8196218  0.83044684 0.82598214 0.83813649 0.09739202 0.83813649]\n",
      " [0.81646274 0.83940179 0.83044643 0.84167256 0.08333751 0.84167256]\n",
      " [0.83426029 0.83335606 0.8375     0.85038013 0.11553011 0.85038013]] -> [0.86134194]\n",
      "[[0.84983315 0.85008409 0.84928571 0.8437058  0.10077413 0.8437058 ]\n",
      " [0.84111235 0.84299286 0.84383929 0.83588225 0.07139994 0.83588225]\n",
      " [0.85882091 0.8583572  0.81785714 0.83265559 0.20729783 0.83265559]\n",
      " [0.8196218  0.83044684 0.82598214 0.83813649 0.09739202 0.83813649]\n",
      " [0.81646274 0.83940179 0.83044643 0.84167256 0.08333751 0.84167256]\n",
      " [0.83426029 0.83335606 0.8375     0.85038013 0.11553011 0.85038013]\n",
      " [0.84734149 0.85781172 0.86799107 0.86134194 0.10141297 0.86134194]] -> [0.85696605]\n",
      "[[0.84111235 0.84299286 0.84383929 0.83588225 0.07139994 0.83588225]\n",
      " [0.85882091 0.8583572  0.81785714 0.83265559 0.20729783 0.83265559]\n",
      " [0.8196218  0.83044684 0.82598214 0.83813649 0.09739202 0.83813649]\n",
      " [0.81646274 0.83940179 0.83044643 0.84167256 0.08333751 0.84167256]\n",
      " [0.83426029 0.83335606 0.8375     0.85038013 0.11553011 0.85038013]\n",
      " [0.84734149 0.85781172 0.86799107 0.86134194 0.10141297 0.86134194]\n",
      " [0.8662069  0.85799354 0.85602679 0.85696605 0.10788907 0.85696605]] -> [0.8800389]\n",
      "[[0.85882091 0.8583572  0.81785714 0.83265559 0.20729783 0.83265559]\n",
      " [0.8196218  0.83044684 0.82598214 0.83813649 0.09739202 0.83813649]\n",
      " [0.81646274 0.83940179 0.83044643 0.84167256 0.08333751 0.84167256]\n",
      " [0.83426029 0.83335606 0.8375     0.85038013 0.11553011 0.85038013]\n",
      " [0.84734149 0.85781172 0.86799107 0.86134194 0.10141297 0.86134194]\n",
      " [0.8662069  0.85799354 0.85602679 0.85696605 0.10788907 0.85696605]\n",
      " [0.86918799 0.86963044 0.8778125  0.8800389  0.13211494 0.8800389 ]] -> [0.86859088]\n",
      "[[0.8196218  0.83044684 0.82598214 0.83813649 0.09739202 0.83813649]\n",
      " [0.81646274 0.83940179 0.83044643 0.84167256 0.08333751 0.84167256]\n",
      " [0.83426029 0.83335606 0.8375     0.85038013 0.11553011 0.85038013]\n",
      " [0.84734149 0.85781172 0.86799107 0.86134194 0.10141297 0.86134194]\n",
      " [0.8662069  0.85799354 0.85602679 0.85696605 0.10788907 0.85696605]\n",
      " [0.86918799 0.86963044 0.8778125  0.8800389  0.13211494 0.8800389 ]\n",
      " [0.86006674 0.86540297 0.87058036 0.86859088 0.17808648 0.86859088]] -> [0.88901167]\n",
      "[[0.81646274 0.83940179 0.83044643 0.84167256 0.08333751 0.84167256]\n",
      " [0.83426029 0.83335606 0.8375     0.85038013 0.11553011 0.85038013]\n",
      " [0.84734149 0.85781172 0.86799107 0.86134194 0.10141297 0.86134194]\n",
      " [0.8662069  0.85799354 0.85602679 0.85696605 0.10788907 0.85696605]\n",
      " [0.86918799 0.86963044 0.8778125  0.8800389  0.13211494 0.8800389 ]\n",
      " [0.86006674 0.86540297 0.87058036 0.86859088 0.17808648 0.86859088]\n",
      " [0.8836485  0.88354016 0.89580357 0.88901167 0.24435064 0.88901167]] -> [0.87212694]\n",
      "[[0.83426029 0.83335606 0.8375     0.85038013 0.11553011 0.85038013]\n",
      " [0.84734149 0.85781172 0.86799107 0.86134194 0.10141297 0.86134194]\n",
      " [0.8662069  0.85799354 0.85602679 0.85696605 0.10788907 0.85696605]\n",
      " [0.86918799 0.86963044 0.8778125  0.8800389  0.13211494 0.8800389 ]\n",
      " [0.86006674 0.86540297 0.87058036 0.86859088 0.17808648 0.86859088]\n",
      " [0.8836485  0.88354016 0.89580357 0.88901167 0.24435064 0.88901167]\n",
      " [0.87403782 0.87113051 0.88294643 0.87212694 0.22298076 0.87212694]] -> [0.88308875]\n",
      "[[0.84734149 0.85781172 0.86799107 0.86134194 0.10141297 0.86134194]\n",
      " [0.8662069  0.85799354 0.85602679 0.85696605 0.10788907 0.85696605]\n",
      " [0.86918799 0.86963044 0.8778125  0.8800389  0.13211494 0.8800389 ]\n",
      " [0.86006674 0.86540297 0.87058036 0.86859088 0.17808648 0.86859088]\n",
      " [0.8836485  0.88354016 0.89580357 0.88901167 0.24435064 0.88901167]\n",
      " [0.87403782 0.87113051 0.88294643 0.87212694 0.22298076 0.87212694]\n",
      " [0.87221357 0.87699441 0.88848214 0.88308875 0.33480309 0.88308875]] -> [0.89307815]\n",
      "[[0.8662069  0.85799354 0.85602679 0.85696605 0.10788907 0.85696605]\n",
      " [0.86918799 0.86963044 0.8778125  0.8800389  0.13211494 0.8800389 ]\n",
      " [0.86006674 0.86540297 0.87058036 0.86859088 0.17808648 0.86859088]\n",
      " [0.8836485  0.88354016 0.89580357 0.88901167 0.24435064 0.88901167]\n",
      " [0.87403782 0.87113051 0.88294643 0.87212694 0.22298076 0.87212694]\n",
      " [0.87221357 0.87699441 0.88848214 0.88308875 0.33480309 0.88308875]\n",
      " [0.8792436  0.88113096 0.89147321 0.89307815 0.21451298 0.89307815]] -> [0.90386315]\n",
      "[[0.86918799 0.86963044 0.8778125  0.8800389  0.13211494 0.8800389 ]\n",
      " [0.86006674 0.86540297 0.87058036 0.86859088 0.17808648 0.86859088]\n",
      " [0.8836485  0.88354016 0.89580357 0.88901167 0.24435064 0.88901167]\n",
      " [0.87403782 0.87113051 0.88294643 0.87212694 0.22298076 0.87212694]\n",
      " [0.87221357 0.87699441 0.88848214 0.88308875 0.33480309 0.88308875]\n",
      " [0.8792436  0.88113096 0.89147321 0.89307815 0.21451298 0.89307815]\n",
      " [0.89583982 0.88867676 0.90566964 0.90386315 0.12803137 0.90386315]] -> [0.95053925]\n",
      "[[0.86006674 0.86540297 0.87058036 0.86859088 0.17808648 0.86859088]\n",
      " [0.8836485  0.88354016 0.89580357 0.88901167 0.24435064 0.88901167]\n",
      " [0.87403782 0.87113051 0.88294643 0.87212694 0.22298076 0.87212694]\n",
      " [0.87221357 0.87699441 0.88848214 0.88308875 0.33480309 0.88308875]\n",
      " [0.8792436  0.88113096 0.89147321 0.89307815 0.21451298 0.89307815]\n",
      " [0.89583982 0.88867676 0.90566964 0.90386315 0.12803137 0.90386315]\n",
      " [0.9054505  0.94036093 0.92352679 0.95053925 0.15206935 0.95053925]] -> [0.98059583]\n",
      "[[0.8836485  0.88354016 0.89580357 0.88901167 0.24435064 0.88901167]\n",
      " [0.87403782 0.87113051 0.88294643 0.87212694 0.22298076 0.87212694]\n",
      " [0.87221357 0.87699441 0.88848214 0.88308875 0.33480309 0.88308875]\n",
      " [0.8792436  0.88113096 0.89147321 0.89307815 0.21451298 0.89307815]\n",
      " [0.89583982 0.88867676 0.90566964 0.90386315 0.12803137 0.90386315]\n",
      " [0.9054505  0.94036093 0.92352679 0.95053925 0.15206935 0.95053925]\n",
      " [0.94740823 0.97058957 0.96558036 0.98059583 0.         0.98059583]] -> [1.]\n",
      "[[0.87403782 0.87113051 0.88294643 0.87212694 0.22298076 0.87212694]\n",
      " [0.87221357 0.87699441 0.88848214 0.88308875 0.33480309 0.88308875]\n",
      " [0.8792436  0.88113096 0.89147321 0.89307815 0.21451298 0.89307815]\n",
      " [0.89583982 0.88867676 0.90566964 0.90386315 0.12803137 0.90386315]\n",
      " [0.9054505  0.94036093 0.92352679 0.95053925 0.15206935 0.95053925]\n",
      " [0.94740823 0.97058957 0.96558036 0.98059583 0.         0.98059583]\n",
      " [0.98625139 1.         0.99330357 1.         0.10512075 1.        ]] -> [0.98466231]\n",
      "[[0.87221357 0.87699441 0.88848214 0.88308875 0.33480309 0.88308875]\n",
      " [0.8792436  0.88113096 0.89147321 0.89307815 0.21451298 0.89307815]\n",
      " [0.89583982 0.88867676 0.90566964 0.90386315 0.12803137 0.90386315]\n",
      " [0.9054505  0.94036093 0.92352679 0.95053925 0.15206935 0.95053925]\n",
      " [0.94740823 0.97058957 0.96558036 0.98059583 0.         0.98059583]\n",
      " [0.98625139 1.         0.99330357 1.         0.10512075 1.        ]\n",
      " [0.99995551 0.99477249 0.99245536 0.98466231 0.07091141 0.98466231]] -> [0.99938119]\n",
      "[[0.8792436  0.88113096 0.89147321 0.89307815 0.21451298 0.89307815]\n",
      " [0.89583982 0.88867676 0.90566964 0.90386315 0.12803137 0.90386315]\n",
      " [0.9054505  0.94036093 0.92352679 0.95053925 0.15206935 0.95053925]\n",
      " [0.94740823 0.97058957 0.96558036 0.98059583 0.         0.98059583]\n",
      " [0.98625139 1.         0.99330357 1.         0.10512075 1.        ]\n",
      " [0.99995551 0.99477249 0.99245536 0.98466231 0.07091141 0.98466231]\n",
      " [0.97219132 0.98518114 0.99120536 0.99938119 0.07602215 0.99938119]] -> [0.98722595]\n",
      "[[0.74175495 0.72272817 0.75642747 0.74451754 0.16085424 0.74451754]\n",
      " [0.76334199 0.83528014 0.79480046 0.8497807  0.19105474 0.8497807 ]\n",
      " [0.85758545 0.90110869 0.88516884 0.91756379 0.         0.91756379]\n",
      " [0.9448331  0.96515541 0.9447429  0.96132376 0.13207013 0.96132376]\n",
      " [0.97561463 0.95377153 0.94292018 0.92673445 0.08909068 0.92673445]\n",
      " [0.91325205 0.93288458 0.94023407 0.95992823 0.09551163 0.95992823]\n",
      " [0.97571457 0.95931499 0.95913277 0.93251595 0.05682855 0.93251595]] -> [1.]\n",
      "[[0.76334199 0.83528014 0.79480046 0.8497807  0.19105474 0.8497807 ]\n",
      " [0.85758545 0.90110869 0.88516884 0.91756379 0.         0.91756379]\n",
      " [0.9448331  0.96515541 0.9447429  0.96132376 0.13207013 0.96132376]\n",
      " [0.97561463 0.95377153 0.94292018 0.92673445 0.08909068 0.92673445]\n",
      " [0.91325205 0.93288458 0.94023407 0.95992823 0.09551163 0.95992823]\n",
      " [0.97571457 0.95931499 0.95913277 0.93251595 0.05682855 0.93251595]\n",
      " [0.99770138 1.         1.         1.         0.17272041 1.        ]] -> [0.97248804]\n",
      "[[0.85758545 0.90110869 0.88516884 0.91756379 0.         0.91756379]\n",
      " [0.9448331  0.96515541 0.9447429  0.96132376 0.13207013 0.96132376]\n",
      " [0.97561463 0.95377153 0.94292018 0.92673445 0.08909068 0.92673445]\n",
      " [0.91325205 0.93288458 0.94023407 0.95992823 0.09551163 0.95992823]\n",
      " [0.97571457 0.95931499 0.95913277 0.93251595 0.05682855 0.93251595]\n",
      " [0.99770138 1.         1.         1.         0.17272041 1.        ]\n",
      " [1.         0.98346862 0.97409823 0.97248804 0.33772938 0.97248804]] -> [0.81568979]\n",
      "[[0.9448331  0.96515541 0.9447429  0.96132376 0.13207013 0.96132376]\n",
      " [0.97561463 0.95377153 0.94292018 0.92673445 0.08909068 0.92673445]\n",
      " [0.91325205 0.93288458 0.94023407 0.95992823 0.09551163 0.95992823]\n",
      " [0.97571457 0.95931499 0.95913277 0.93251595 0.05682855 0.93251595]\n",
      " [0.99770138 1.         1.         1.         0.17272041 1.        ]\n",
      " [1.         0.98346862 0.97409823 0.97248804 0.33772938 0.97248804]\n",
      " [0.98680791 0.96386854 0.82847275 0.81568979 0.49101382 0.81568979]] -> [0.67454147]\n",
      "[[0.97561463 0.95377153 0.94292018 0.92673445 0.08909068 0.92673445]\n",
      " [0.91325205 0.93288458 0.94023407 0.95992823 0.09551163 0.95992823]\n",
      " [0.97571457 0.95931499 0.95913277 0.93251595 0.05682855 0.93251595]\n",
      " [0.99770138 1.         1.         1.         0.17272041 1.        ]\n",
      " [1.         0.98346862 0.97409823 0.97248804 0.33772938 0.97248804]\n",
      " [0.98680791 0.96386854 0.82847275 0.81568979 0.49101382 0.81568979]\n",
      " [0.82230662 0.80686993 0.69694935 0.67454147 0.87431935 0.67454147]] -> [0.75348883]\n",
      "[[0.91325205 0.93288458 0.94023407 0.95992823 0.09551163 0.95992823]\n",
      " [0.97571457 0.95931499 0.95913277 0.93251595 0.05682855 0.93251595]\n",
      " [0.99770138 1.         1.         1.         0.17272041 1.        ]\n",
      " [1.         0.98346862 0.97409823 0.97248804 0.33772938 0.97248804]\n",
      " [0.98680791 0.96386854 0.82847275 0.81568979 0.49101382 0.81568979]\n",
      " [0.82230662 0.80686993 0.69694935 0.67454147 0.87431935 0.67454147]\n",
      " [0.64841095 0.74005147 0.66298925 0.75348883 0.60835353 0.75348883]] -> [0.74441786]\n",
      "[[0.97571457 0.95931499 0.95913277 0.93251595 0.05682855 0.93251595]\n",
      " [0.99770138 1.         1.         1.         0.17272041 1.        ]\n",
      " [1.         0.98346862 0.97409823 0.97248804 0.33772938 0.97248804]\n",
      " [0.98680791 0.96386854 0.82847275 0.81568979 0.49101382 0.81568979]\n",
      " [0.82230662 0.80686993 0.69694935 0.67454147 0.87431935 0.67454147]\n",
      " [0.64841095 0.74005147 0.66298925 0.75348883 0.60835353 0.75348883]\n",
      " [0.77293624 0.7520293  0.71028396 0.74441786 1.         0.74441786]] -> [0.63845694]\n",
      "[[0.99770138 1.         1.         1.         0.17272041 1.        ]\n",
      " [1.         0.98346862 0.97409823 0.97248804 0.33772938 0.97248804]\n",
      " [0.98680791 0.96386854 0.82847275 0.81568979 0.49101382 0.81568979]\n",
      " [0.82230662 0.80686993 0.69694935 0.67454147 0.87431935 0.67454147]\n",
      " [0.64841095 0.74005147 0.66298925 0.75348883 0.60835353 0.75348883]\n",
      " [0.77293624 0.7520293  0.71028396 0.74441786 1.         0.74441786]\n",
      " [0.70487707 0.68966541 0.65234075 0.63845694 0.62865506 0.63845694]] -> [0.60466507]\n",
      "[[1.         0.98346862 0.97409823 0.97248804 0.33772938 0.97248804]\n",
      " [0.98680791 0.96386854 0.82847275 0.81568979 0.49101382 0.81568979]\n",
      " [0.82230662 0.80686993 0.69694935 0.67454147 0.87431935 0.67454147]\n",
      " [0.64841095 0.74005147 0.66298925 0.75348883 0.60835353 0.75348883]\n",
      " [0.77293624 0.7520293  0.71028396 0.74441786 1.         0.74441786]\n",
      " [0.70487707 0.68966541 0.65234075 0.63845694 0.62865506 0.63845694]\n",
      " [0.49340396 0.5928529  0.51707598 0.60466507 0.88727141 0.60466507]] -> [0.63217703]\n",
      "[[0.98680791 0.96386854 0.82847275 0.81568979 0.49101382 0.81568979]\n",
      " [0.82230662 0.80686993 0.69694935 0.67454147 0.87431935 0.67454147]\n",
      " [0.64841095 0.74005147 0.66298925 0.75348883 0.60835353 0.75348883]\n",
      " [0.77293624 0.7520293  0.71028396 0.74441786 1.         0.74441786]\n",
      " [0.70487707 0.68966541 0.65234075 0.63845694 0.62865506 0.63845694]\n",
      " [0.49340396 0.5928529  0.51707598 0.60466507 0.88727141 0.60466507]\n",
      " [0.56745952 0.65046525 0.6025518  0.63217703 0.53215196 0.63217703]] -> [0.53399123]\n",
      "[[0.82230662 0.80686993 0.69694935 0.67454147 0.87431935 0.67454147]\n",
      " [0.64841095 0.74005147 0.66298925 0.75348883 0.60835353 0.75348883]\n",
      " [0.77293624 0.7520293  0.71028396 0.74441786 1.         0.74441786]\n",
      " [0.70487707 0.68966541 0.65234075 0.63845694 0.62865506 0.63845694]\n",
      " [0.49340396 0.5928529  0.51707598 0.60466507 0.88727141 0.60466507]\n",
      " [0.56745952 0.65046525 0.6025518  0.63217703 0.53215196 0.63217703]\n",
      " [0.66180292 0.64412987 0.55679202 0.53399123 0.72012212 0.53399123]] -> [0.60994817]\n",
      "[[0.64841095 0.74005147 0.66298925 0.75348883 0.60835353 0.75348883]\n",
      " [0.77293624 0.7520293  0.71028396 0.74441786 1.         0.74441786]\n",
      " [0.70487707 0.68966541 0.65234075 0.63845694 0.62865506 0.63845694]\n",
      " [0.49340396 0.5928529  0.51707598 0.60466507 0.88727141 0.60466507]\n",
      " [0.56745952 0.65046525 0.6025518  0.63217703 0.53215196 0.63217703]\n",
      " [0.66180292 0.64412987 0.55679202 0.53399123 0.72012212 0.53399123]\n",
      " [0.54047571 0.61304692 0.54000384 0.60994817 0.54420698 0.60994817]] -> [0.53807815]\n",
      "[[0.77293624 0.7520293  0.71028396 0.74441786 1.         0.74441786]\n",
      " [0.70487707 0.68966541 0.65234075 0.63845694 0.62865506 0.63845694]\n",
      " [0.49340396 0.5928529  0.51707598 0.60466507 0.88727141 0.60466507]\n",
      " [0.56745952 0.65046525 0.6025518  0.63217703 0.53215196 0.63217703]\n",
      " [0.66180292 0.64412987 0.55679202 0.53399123 0.72012212 0.53399123]\n",
      " [0.54047571 0.61304692 0.54000384 0.60994817 0.54420698 0.60994817]\n",
      " [0.58824705 0.56909523 0.53041059 0.53807815 0.37644393 0.53807815]] -> [0.56369617]\n",
      "[[0.70487707 0.68966541 0.65234075 0.63845694 0.62865506 0.63845694]\n",
      " [0.49340396 0.5928529  0.51707598 0.60466507 0.88727141 0.60466507]\n",
      " [0.56745952 0.65046525 0.6025518  0.63217703 0.53215196 0.63217703]\n",
      " [0.66180292 0.64412987 0.55679202 0.53399123 0.72012212 0.53399123]\n",
      " [0.54047571 0.61304692 0.54000384 0.60994817 0.54420698 0.60994817]\n",
      " [0.58824705 0.56909523 0.53041059 0.53807815 0.37644393 0.53807815]\n",
      " [0.56286228 0.57741041 0.50124712 0.56369617 0.85754304 0.56369617]] -> [0.23504785]\n",
      "[[0.49340396 0.5928529  0.51707598 0.60466507 0.88727141 0.60466507]\n",
      " [0.56745952 0.65046525 0.6025518  0.63217703 0.53215196 0.63217703]\n",
      " [0.66180292 0.64412987 0.55679202 0.53399123 0.72012212 0.53399123]\n",
      " [0.54047571 0.61304692 0.54000384 0.60994817 0.54420698 0.60994817]\n",
      " [0.58824705 0.56909523 0.53041059 0.53807815 0.37644393 0.53807815]\n",
      " [0.56286228 0.57741041 0.50124712 0.56369617 0.85754304 0.56369617]\n",
      " [0.37357585 0.41684815 0.14255564 0.23504785 0.93508231 0.23504785]] -> [0.13526715]\n",
      "[[0.56745952 0.65046525 0.6025518  0.63217703 0.53215196 0.63217703]\n",
      " [0.66180292 0.64412987 0.55679202 0.53399123 0.72012212 0.53399123]\n",
      " [0.54047571 0.61304692 0.54000384 0.60994817 0.54420698 0.60994817]\n",
      " [0.58824705 0.56909523 0.53041059 0.53807815 0.37644393 0.53807815]\n",
      " [0.56286228 0.57741041 0.50124712 0.56369617 0.85754304 0.56369617]\n",
      " [0.37357585 0.41684815 0.14255564 0.23504785 0.93508231 0.23504785]\n",
      " [0.19458325 0.21174025 0.14879125 0.13526715 0.56044821 0.13526715]] -> [0.15938995]\n",
      "[[0.66180292 0.64412987 0.55679202 0.53399123 0.72012212 0.53399123]\n",
      " [0.54047571 0.61304692 0.54000384 0.60994817 0.54420698 0.60994817]\n",
      " [0.58824705 0.56909523 0.53041059 0.53807815 0.37644393 0.53807815]\n",
      " [0.56286228 0.57741041 0.50124712 0.56369617 0.85754304 0.56369617]\n",
      " [0.37357585 0.41684815 0.14255564 0.23504785 0.93508231 0.23504785]\n",
      " [0.19458325 0.21174025 0.14879125 0.13526715 0.56044821 0.13526715]\n",
      " [0.10843494 0.20896852 0.15627398 0.15938995 0.55031318 0.15938995]] -> [0.15620016]\n",
      "[[0.54047571 0.61304692 0.54000384 0.60994817 0.54420698 0.60994817]\n",
      " [0.58824705 0.56909523 0.53041059 0.53807815 0.37644393 0.53807815]\n",
      " [0.56286228 0.57741041 0.50124712 0.56369617 0.85754304 0.56369617]\n",
      " [0.37357585 0.41684815 0.14255564 0.23504785 0.93508231 0.23504785]\n",
      " [0.19458325 0.21174025 0.14879125 0.13526715 0.56044821 0.13526715]\n",
      " [0.10843494 0.20896852 0.15627398 0.15938995 0.55031318 0.15938995]\n",
      " [0.14141515 0.18966541 0.1468726  0.15620016 0.48876334 0.15620016]] -> [0.07834928]\n",
      "[[0.58824705 0.56909523 0.53041059 0.53807815 0.37644393 0.53807815]\n",
      " [0.56286228 0.57741041 0.50124712 0.56369617 0.85754304 0.56369617]\n",
      " [0.37357585 0.41684815 0.14255564 0.23504785 0.93508231 0.23504785]\n",
      " [0.19458325 0.21174025 0.14879125 0.13526715 0.56044821 0.13526715]\n",
      " [0.10843494 0.20896852 0.15627398 0.15938995 0.55031318 0.15938995]\n",
      " [0.14141515 0.18966541 0.1468726  0.15620016 0.48876334 0.15620016]\n",
      " [0.17359584 0.18323104 0.10168841 0.07834928 0.3730446  0.07834928]] -> [0.03498804]\n",
      "[[0.56286228 0.57741041 0.50124712 0.56369617 0.85754304 0.56369617]\n",
      " [0.37357585 0.41684815 0.14255564 0.23504785 0.93508231 0.23504785]\n",
      " [0.19458325 0.21174025 0.14879125 0.13526715 0.56044821 0.13526715]\n",
      " [0.10843494 0.20896852 0.15627398 0.15938995 0.55031318 0.15938995]\n",
      " [0.14141515 0.18966541 0.1468726  0.15620016 0.48876334 0.15620016]\n",
      " [0.17359584 0.18323104 0.10168841 0.07834928 0.3730446  0.07834928]\n",
      " [0.11363182 0.12373787 0.05381811 0.03498804 0.51647729 0.03498804]] -> [0.06568979]\n",
      "[[0.37357585 0.41684815 0.14255564 0.23504785 0.93508231 0.23504785]\n",
      " [0.19458325 0.21174025 0.14879125 0.13526715 0.56044821 0.13526715]\n",
      " [0.10843494 0.20896852 0.15627398 0.15938995 0.55031318 0.15938995]\n",
      " [0.14141515 0.18966541 0.1468726  0.15620016 0.48876334 0.15620016]\n",
      " [0.17359584 0.18323104 0.10168841 0.07834928 0.3730446  0.07834928]\n",
      " [0.11363182 0.12373787 0.05381811 0.03498804 0.51647729 0.03498804]\n",
      " [0.07475515 0.0724609  0.09881044 0.06568979 0.21908344 0.06568979]] -> [0.0467504]\n",
      "[[0.19458325 0.21174025 0.14879125 0.13526715 0.56044821 0.13526715]\n",
      " [0.10843494 0.20896852 0.15627398 0.15938995 0.55031318 0.15938995]\n",
      " [0.14141515 0.18966541 0.1468726  0.15620016 0.48876334 0.15620016]\n",
      " [0.17359584 0.18323104 0.10168841 0.07834928 0.3730446  0.07834928]\n",
      " [0.11363182 0.12373787 0.05381811 0.03498804 0.51647729 0.03498804]\n",
      " [0.07475515 0.0724609  0.09881044 0.06568979 0.21908344 0.06568979]\n",
      " [0.07835299 0.06404672 0.0730046  0.0467504  0.50950552 0.0467504 ]] -> [0.14344099]\n",
      "[[0.10843494 0.20896852 0.15627398 0.15938995 0.55031318 0.15938995]\n",
      " [0.14141515 0.18966541 0.1468726  0.15620016 0.48876334 0.15620016]\n",
      " [0.17359584 0.18323104 0.10168841 0.07834928 0.3730446  0.07834928]\n",
      " [0.11363182 0.12373787 0.05381811 0.03498804 0.51647729 0.03498804]\n",
      " [0.07475515 0.0724609  0.09881044 0.06568979 0.21908344 0.06568979]\n",
      " [0.07835299 0.06404672 0.0730046  0.0467504  0.50950552 0.0467504 ]\n",
      " [0.01549071 0.1347258  0.07895242 0.14344099 0.33684807 0.14344099]] -> [0.00348884]\n",
      "[[0.14141515 0.18966541 0.1468726  0.15620016 0.48876334 0.15620016]\n",
      " [0.17359584 0.18323104 0.10168841 0.07834928 0.3730446  0.07834928]\n",
      " [0.11363182 0.12373787 0.05381811 0.03498804 0.51647729 0.03498804]\n",
      " [0.07475515 0.0724609  0.09881044 0.06568979 0.21908344 0.06568979]\n",
      " [0.07835299 0.06404672 0.0730046  0.0467504  0.50950552 0.0467504 ]\n",
      " [0.01549071 0.1347258  0.07895242 0.14344099 0.33684807 0.14344099]\n",
      " [0.09874076 0.09413977 0.05333845 0.00348884 0.39643071 0.00348884]] -> [0.04844498]\n",
      "[[0.17359584 0.18323104 0.10168841 0.07834928 0.3730446  0.07834928]\n",
      " [0.11363182 0.12373787 0.05381811 0.03498804 0.51647729 0.03498804]\n",
      " [0.07475515 0.0724609  0.09881044 0.06568979 0.21908344 0.06568979]\n",
      " [0.07835299 0.06404672 0.0730046  0.0467504  0.50950552 0.0467504 ]\n",
      " [0.01549071 0.1347258  0.07895242 0.14344099 0.33684807 0.14344099]\n",
      " [0.09874076 0.09413977 0.05333845 0.00348884 0.39643071 0.00348884]\n",
      " [0.01279232 0.04959414 0.02359939 0.04844498 0.3615404  0.04844498]] -> [0.07017544]\n",
      "[[0.11363182 0.12373787 0.05381811 0.03498804 0.51647729 0.03498804]\n",
      " [0.07475515 0.0724609  0.09881044 0.06568979 0.21908344 0.06568979]\n",
      " [0.07835299 0.06404672 0.0730046  0.0467504  0.50950552 0.0467504 ]\n",
      " [0.01549071 0.1347258  0.07895242 0.14344099 0.33684807 0.14344099]\n",
      " [0.09874076 0.09413977 0.05333845 0.00348884 0.39643071 0.00348884]\n",
      " [0.01279232 0.04959414 0.02359939 0.04844498 0.3615404  0.04844498]\n",
      " [0.05376774 0.13373589 0.11061013 0.07017544 0.2888798  0.07017544]] -> [0.0997807]\n",
      "[[0.07475515 0.0724609  0.09881044 0.06568979 0.21908344 0.06568979]\n",
      " [0.07835299 0.06404672 0.0730046  0.0467504  0.50950552 0.0467504 ]\n",
      " [0.01549071 0.1347258  0.07895242 0.14344099 0.33684807 0.14344099]\n",
      " [0.09874076 0.09413977 0.05333845 0.00348884 0.39643071 0.00348884]\n",
      " [0.01279232 0.04959414 0.02359939 0.04844498 0.3615404  0.04844498]\n",
      " [0.05376774 0.13373589 0.11061013 0.07017544 0.2888798  0.07017544]\n",
      " [0.12412552 0.12908335 0.14457022 0.0997807  0.30859904 0.0997807 ]] -> [0.07994418]\n",
      "[[0.07835299 0.06404672 0.0730046  0.0467504  0.50950552 0.0467504 ]\n",
      " [0.01549071 0.1347258  0.07895242 0.14344099 0.33684807 0.14344099]\n",
      " [0.09874076 0.09413977 0.05333845 0.00348884 0.39643071 0.00348884]\n",
      " [0.01279232 0.04959414 0.02359939 0.04844498 0.3615404  0.04844498]\n",
      " [0.05376774 0.13373589 0.11061013 0.07017544 0.2888798  0.07017544]\n",
      " [0.12412552 0.12908335 0.14457022 0.0997807  0.30859904 0.0997807 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [0.06945832 0.09206098 0.07377206 0.07994418 0.28576375 0.07994418]] -> [0.09938198]\n",
      "[[0.01549071 0.1347258  0.07895242 0.14344099 0.33684807 0.14344099]\n",
      " [0.09874076 0.09413977 0.05333845 0.00348884 0.39643071 0.00348884]\n",
      " [0.01279232 0.04959414 0.02359939 0.04844498 0.3615404  0.04844498]\n",
      " [0.05376774 0.13373589 0.11061013 0.07017544 0.2888798  0.07017544]\n",
      " [0.12412552 0.12908335 0.14457022 0.0997807  0.30859904 0.0997807 ]\n",
      " [0.06945832 0.09206098 0.07377206 0.07994418 0.28576375 0.07994418]\n",
      " [0.06196282 0.11878836 0.09113584 0.09938198 0.24413774 0.09938198]] -> [0.0666866]\n",
      "[[0.09874076 0.09413977 0.05333845 0.00348884 0.39643071 0.00348884]\n",
      " [0.01279232 0.04959414 0.02359939 0.04844498 0.3615404  0.04844498]\n",
      " [0.05376774 0.13373589 0.11061013 0.07017544 0.2888798  0.07017544]\n",
      " [0.12412552 0.12908335 0.14457022 0.0997807  0.30859904 0.0997807 ]\n",
      " [0.06945832 0.09206098 0.07377206 0.07994418 0.28576375 0.07994418]\n",
      " [0.06196282 0.11878836 0.09113584 0.09938198 0.24413774 0.09938198]\n",
      " [0.05846492 0.0857256  0.10072909 0.0666866  0.22363161 0.0666866 ]] -> [0.14822568]\n",
      "[[0.01279232 0.04959414 0.02359939 0.04844498 0.3615404  0.04844498]\n",
      " [0.05376774 0.13373589 0.11061013 0.07017544 0.2888798  0.07017544]\n",
      " [0.12412552 0.12908335 0.14457022 0.0997807  0.30859904 0.0997807 ]\n",
      " [0.06945832 0.09206098 0.07377206 0.07994418 0.28576375 0.07994418]\n",
      " [0.06196282 0.11878836 0.09113584 0.09938198 0.24413774 0.09938198]\n",
      " [0.05846492 0.0857256  0.10072909 0.0666866  0.22363161 0.0666866 ]\n",
      " [0.09744153 0.15482083 0.13718342 0.14822568 0.43997671 0.14822568]] -> [0.11742424]\n",
      "[[0.05376774 0.13373589 0.11061013 0.07017544 0.2888798  0.07017544]\n",
      " [0.12412552 0.12908335 0.14457022 0.0997807  0.30859904 0.0997807 ]\n",
      " [0.06945832 0.09206098 0.07377206 0.07994418 0.28576375 0.07994418]\n",
      " [0.06196282 0.11878836 0.09113584 0.09938198 0.24413774 0.09938198]\n",
      " [0.05846492 0.0857256  0.10072909 0.0666866  0.22363161 0.0666866 ]\n",
      " [0.09744153 0.15482083 0.13718342 0.14822568 0.43997671 0.14822568]\n",
      " [0.14871077 0.14759454 0.16893707 0.11742424 0.18428756 0.11742424]] -> [0.16736443]\n",
      "[[0.12412552 0.12908335 0.14457022 0.0997807  0.30859904 0.0997807 ]\n",
      " [0.06945832 0.09206098 0.07377206 0.07994418 0.28576375 0.07994418]\n",
      " [0.06196282 0.11878836 0.09113584 0.09938198 0.24413774 0.09938198]\n",
      " [0.05846492 0.0857256  0.10072909 0.0666866  0.22363161 0.0666866 ]\n",
      " [0.09744153 0.15482083 0.13718342 0.14822568 0.43997671 0.14822568]\n",
      " [0.14871077 0.14759454 0.16893707 0.11742424 0.18428756 0.11742424]\n",
      " [0.10413752 0.16353197 0.154835   0.16736443 0.23263353 0.16736443]] -> [0.1428429]\n",
      "[[0.06945832 0.09206098 0.07377206 0.07994418 0.28576375 0.07994418]\n",
      " [0.06196282 0.11878836 0.09113584 0.09938198 0.24413774 0.09938198]\n",
      " [0.05846492 0.0857256  0.10072909 0.0666866  0.22363161 0.0666866 ]\n",
      " [0.09744153 0.15482083 0.13718342 0.14822568 0.43997671 0.14822568]\n",
      " [0.14871077 0.14759454 0.16893707 0.11742424 0.18428756 0.11742424]\n",
      " [0.10413752 0.16353197 0.154835   0.16736443 0.23263353 0.16736443]\n",
      " [0.13401959 0.15383092 0.18898695 0.1428429  0.26665827 0.1428429 ]] -> [0.16317783]\n",
      "[[0.06196282 0.11878836 0.09113584 0.09938198 0.24413774 0.09938198]\n",
      " [0.05846492 0.0857256  0.10072909 0.0666866  0.22363161 0.0666866 ]\n",
      " [0.09744153 0.15482083 0.13718342 0.14822568 0.43997671 0.14822568]\n",
      " [0.14871077 0.14759454 0.16893707 0.11742424 0.18428756 0.11742424]\n",
      " [0.10413752 0.16353197 0.154835   0.16736443 0.23263353 0.16736443]\n",
      " [0.13401959 0.15383092 0.18898695 0.1428429  0.26665827 0.1428429 ]\n",
      " [0.20217869 0.18818056 0.20145817 0.16317783 0.20237009 0.16317783]] -> [0.0861244]\n",
      "[[0.05846492 0.0857256  0.10072909 0.0666866  0.22363161 0.0666866 ]\n",
      " [0.09744153 0.15482083 0.13718342 0.14822568 0.43997671 0.14822568]\n",
      " [0.14871077 0.14759454 0.16893707 0.11742424 0.18428756 0.11742424]\n",
      " [0.10413752 0.16353197 0.154835   0.16736443 0.23263353 0.16736443]\n",
      " [0.13401959 0.15383092 0.18898695 0.1428429  0.26665827 0.1428429 ]\n",
      " [0.20217869 0.18818056 0.20145817 0.16317783 0.20237009 0.16317783]\n",
      " [0.15870478 0.14987131 0.117901   0.0861244  0.31693998 0.0861244 ]] -> [0.05023923]\n",
      "[[0.09744153 0.15482083 0.13718342 0.14822568 0.43997671 0.14822568]\n",
      " [0.14871077 0.14759454 0.16893707 0.11742424 0.18428756 0.11742424]\n",
      " [0.10413752 0.16353197 0.154835   0.16736443 0.23263353 0.16736443]\n",
      " [0.13401959 0.15383092 0.18898695 0.1428429  0.26665827 0.1428429 ]\n",
      " [0.20217869 0.18818056 0.20145817 0.16317783 0.20237009 0.16317783]\n",
      " [0.15870478 0.14987131 0.117901   0.0861244  0.31693998 0.0861244 ]\n",
      " [0.06855886 0.09849535 0.10082502 0.05023923 0.23101256 0.05023923]] -> [0.]\n",
      "[[0.14871077 0.14759454 0.16893707 0.11742424 0.18428756 0.11742424]\n",
      " [0.10413752 0.16353197 0.154835   0.16736443 0.23263353 0.16736443]\n",
      " [0.13401959 0.15383092 0.18898695 0.1428429  0.26665827 0.1428429 ]\n",
      " [0.20217869 0.18818056 0.20145817 0.16317783 0.20237009 0.16317783]\n",
      " [0.15870478 0.14987131 0.117901   0.0861244  0.31693998 0.0861244 ]\n",
      " [0.06855886 0.09849535 0.10082502 0.05023923 0.23101256 0.05023923]\n",
      " [0.         0.         0.         0.         0.38565044 0.        ]] -> [0.0036882]\n",
      "[[0.10413752 0.16353197 0.154835   0.16736443 0.23263353 0.16736443]\n",
      " [0.13401959 0.15383092 0.18898695 0.1428429  0.26665827 0.1428429 ]\n",
      " [0.20217869 0.18818056 0.20145817 0.16317783 0.20237009 0.16317783]\n",
      " [0.15870478 0.14987131 0.117901   0.0861244  0.31693998 0.0861244 ]\n",
      " [0.06855886 0.09849535 0.10082502 0.05023923 0.23101256 0.05023923]\n",
      " [0.         0.         0.         0.         0.38565044 0.        ]\n",
      " [0.0029982  0.03227084 0.0333845  0.0036882  0.18737213 0.0036882 ]] -> [0.06240032]\n",
      "[[0.13401959 0.15383092 0.18898695 0.1428429  0.26665827 0.1428429 ]\n",
      " [0.20217869 0.18818056 0.20145817 0.16317783 0.20237009 0.16317783]\n",
      " [0.15870478 0.14987131 0.117901   0.0861244  0.31693998 0.0861244 ]\n",
      " [0.06855886 0.09849535 0.10082502 0.05023923 0.23101256 0.05023923]\n",
      " [0.         0.         0.         0.         0.38565044 0.        ]\n",
      " [0.0029982  0.03227084 0.0333845  0.0036882  0.18737213 0.0036882 ]\n",
      " [0.02078753 0.04751534 0.07425173 0.06240032 0.21705329 0.06240032]] -> [0.14583333]\n",
      "[[0.20217869 0.18818056 0.20145817 0.16317783 0.20237009 0.16317783]\n",
      " [0.15870478 0.14987131 0.117901   0.0861244  0.31693998 0.0861244 ]\n",
      " [0.06855886 0.09849535 0.10082502 0.05023923 0.23101256 0.05023923]\n",
      " [0.         0.         0.         0.         0.38565044 0.        ]\n",
      " [0.0029982  0.03227084 0.0333845  0.0036882  0.18737213 0.0036882 ]\n",
      " [0.02078753 0.04751534 0.07425173 0.06240032 0.21705329 0.06240032]\n",
      " [0.08354987 0.16996634 0.14399463 0.14583333 0.55051777 0.14583333]] -> [0.18829745]\n",
      "[[0.15870478 0.14987131 0.117901   0.0861244  0.31693998 0.0861244 ]\n",
      " [0.06855886 0.09849535 0.10082502 0.05023923 0.23101256 0.05023923]\n",
      " [0.         0.         0.         0.         0.38565044 0.        ]\n",
      " [0.0029982  0.03227084 0.0333845  0.0036882  0.18737213 0.0036882 ]\n",
      " [0.02078753 0.04751534 0.07425173 0.06240032 0.21705329 0.06240032]\n",
      " [0.08354987 0.16996634 0.14399463 0.14583333 0.55051777 0.14583333]\n",
      " [0.15230861 0.19431796 0.18466999 0.18829745 0.35105914 0.18829745]] -> [0.28070175]\n",
      "[[0.06855886 0.09849535 0.10082502 0.05023923 0.23101256 0.05023923]\n",
      " [0.         0.         0.         0.         0.38565044 0.        ]\n",
      " [0.0029982  0.03227084 0.0333845  0.0036882  0.18737213 0.0036882 ]\n",
      " [0.02078753 0.04751534 0.07425173 0.06240032 0.21705329 0.06240032]\n",
      " [0.08354987 0.16996634 0.14399463 0.14583333 0.55051777 0.14583333]\n",
      " [0.15230861 0.19431796 0.18466999 0.18829745 0.35105914 0.18829745]\n",
      " [0.19368379 0.27360919 0.25134305 0.28070175 0.36591546 0.28070175]] -> [0.3765949]\n",
      "[[0.         0.         0.         0.         0.38565044 0.        ]\n",
      " [0.0029982  0.03227084 0.0333845  0.0036882  0.18737213 0.0036882 ]\n",
      " [0.02078753 0.04751534 0.07425173 0.06240032 0.21705329 0.06240032]\n",
      " [0.08354987 0.16996634 0.14399463 0.14583333 0.55051777 0.14583333]\n",
      " [0.15230861 0.19431796 0.18466999 0.18829745 0.35105914 0.18829745]\n",
      " [0.19368379 0.27360919 0.25134305 0.28070175 0.36591546 0.28070175]\n",
      " [0.34209474 0.38784399 0.38277053 0.3765949  0.4805483  0.3765949 ]] -> [0.35935008]\n",
      "[[0.0029982  0.03227084 0.0333845  0.0036882  0.18737213 0.0036882 ]\n",
      " [0.02078753 0.04751534 0.07425173 0.06240032 0.21705329 0.06240032]\n",
      " [0.08354987 0.16996634 0.14399463 0.14583333 0.55051777 0.14583333]\n",
      " [0.15230861 0.19431796 0.18466999 0.18829745 0.35105914 0.18829745]\n",
      " [0.19368379 0.27360919 0.25134305 0.28070175 0.36591546 0.28070175]\n",
      " [0.34209474 0.38784399 0.38277053 0.3765949  0.4805483  0.3765949 ]\n",
      " [0.37637418 0.36319541 0.39341903 0.35935008 0.24042366 0.35935008]] -> [0.33652313]\n",
      "[[0.02078753 0.04751534 0.07425173 0.06240032 0.21705329 0.06240032]\n",
      " [0.08354987 0.16996634 0.14399463 0.14583333 0.55051777 0.14583333]\n",
      " [0.15230861 0.19431796 0.18466999 0.18829745 0.35105914 0.18829745]\n",
      " [0.19368379 0.27360919 0.25134305 0.28070175 0.36591546 0.28070175]\n",
      " [0.34209474 0.38784399 0.38277053 0.3765949  0.4805483  0.3765949 ]\n",
      " [0.37637418 0.36319541 0.39341903 0.35935008 0.24042366 0.35935008]\n",
      " [0.32710374 0.35804791 0.35015349 0.33652313 0.36478235 0.33652313]] -> [0.49860447]\n",
      "[[0.08354987 0.16996634 0.14399463 0.14583333 0.55051777 0.14583333]\n",
      " [0.15230861 0.19431796 0.18466999 0.18829745 0.35105914 0.18829745]\n",
      " [0.19368379 0.27360919 0.25134305 0.28070175 0.36591546 0.28070175]\n",
      " [0.34209474 0.38784399 0.38277053 0.3765949  0.4805483  0.3765949 ]\n",
      " [0.37637418 0.36319541 0.39341903 0.35935008 0.24042366 0.35935008]\n",
      " [0.32710374 0.35804791 0.35015349 0.33652313 0.36478235 0.33652313]\n",
      " [0.36618029 0.49505048 0.40493093 0.49860447 0.75480784 0.49860447]] -> [0.54535486]\n",
      "[[0.15230861 0.19431796 0.18466999 0.18829745 0.35105914 0.18829745]\n",
      " [0.19368379 0.27360919 0.25134305 0.28070175 0.36591546 0.28070175]\n",
      " [0.34209474 0.38784399 0.38277053 0.3765949  0.4805483  0.3765949 ]\n",
      " [0.37637418 0.36319541 0.39341903 0.35935008 0.24042366 0.35935008]\n",
      " [0.32710374 0.35804791 0.35015349 0.33652313 0.36478235 0.33652313]\n",
      " [0.36618029 0.49505048 0.40493093 0.49860447 0.75480784 0.49860447]\n",
      " [0.47111733 0.571372   0.51995395 0.54535486 0.5310818  0.54535486]] -> [0.5294059]\n",
      "[[0.19368379 0.27360919 0.25134305 0.28070175 0.36591546 0.28070175]\n",
      " [0.34209474 0.38784399 0.38277053 0.3765949  0.4805483  0.3765949 ]\n",
      " [0.37637418 0.36319541 0.39341903 0.35935008 0.24042366 0.35935008]\n",
      " [0.32710374 0.35804791 0.35015349 0.33652313 0.36478235 0.33652313]\n",
      " [0.36618029 0.49505048 0.40493093 0.49860447 0.75480784 0.49860447]\n",
      " [0.47111733 0.571372   0.51995395 0.54535486 0.5310818  0.54535486]\n",
      " [0.6098341  0.591863   0.56753645 0.5294059  0.29818073 0.5294059 ]] -> [0.36632775]\n",
      "[[0.34209474 0.38784399 0.38277053 0.3765949  0.4805483  0.3765949 ]\n",
      " [0.37637418 0.36319541 0.39341903 0.35935008 0.24042366 0.35935008]\n",
      " [0.32710374 0.35804791 0.35015349 0.33652313 0.36478235 0.33652313]\n",
      " [0.36618029 0.49505048 0.40493093 0.49860447 0.75480784 0.49860447]\n",
      " [0.47111733 0.571372   0.51995395 0.54535486 0.5310818  0.54535486]\n",
      " [0.6098341  0.591863   0.56753645 0.5294059  0.29818073 0.5294059 ]\n",
      " [0.39536278 0.40546426 0.34535687 0.36632775 0.59727424 0.36632775]] -> [0.38646332]\n",
      "[[0.37637418 0.36319541 0.39341903 0.35935008 0.24042366 0.35935008]\n",
      " [0.32710374 0.35804791 0.35015349 0.33652313 0.36478235 0.33652313]\n",
      " [0.36618029 0.49505048 0.40493093 0.49860447 0.75480784 0.49860447]\n",
      " [0.47111733 0.571372   0.51995395 0.54535486 0.5310818  0.54535486]\n",
      " [0.6098341  0.591863   0.56753645 0.5294059  0.29818073 0.5294059 ]\n",
      " [0.39536278 0.40546426 0.34535687 0.36632775 0.59727424 0.36632775]\n",
      " [0.34689186 0.39526826 0.39102072 0.38646332 0.40489755 0.38646332]] -> [0.39005183]\n",
      "[[0.32710374 0.35804791 0.35015349 0.33652313 0.36478235 0.33652313]\n",
      " [0.36618029 0.49505048 0.40493093 0.49860447 0.75480784 0.49860447]\n",
      " [0.47111733 0.571372   0.51995395 0.54535486 0.5310818  0.54535486]\n",
      " [0.6098341  0.591863   0.56753645 0.5294059  0.29818073 0.5294059 ]\n",
      " [0.39536278 0.40546426 0.34535687 0.36632775 0.59727424 0.36632775]\n",
      " [0.34689186 0.39526826 0.39102072 0.38646332 0.40489755 0.38646332]\n",
      " [0.41155307 0.42526232 0.42450115 0.39005183 0.3876019  0.39005183]] -> [0.39842504]\n",
      "[[0.36618029 0.49505048 0.40493093 0.49860447 0.75480784 0.49860447]\n",
      " [0.47111733 0.571372   0.51995395 0.54535486 0.5310818  0.54535486]\n",
      " [0.6098341  0.591863   0.56753645 0.5294059  0.29818073 0.5294059 ]\n",
      " [0.39536278 0.40546426 0.34535687 0.36632775 0.59727424 0.36632775]\n",
      " [0.34689186 0.39526826 0.39102072 0.38646332 0.40489755 0.38646332]\n",
      " [0.41155307 0.42526232 0.42450115 0.39005183 0.3876019  0.39005183]\n",
      " [0.37577453 0.39546624 0.41212586 0.39842504 0.14455006 0.39842504]] -> [0.33462919]\n",
      "[[0.47111733 0.571372   0.51995395 0.54535486 0.5310818  0.54535486]\n",
      " [0.6098341  0.591863   0.56753645 0.5294059  0.29818073 0.5294059 ]\n",
      " [0.39536278 0.40546426 0.34535687 0.36632775 0.59727424 0.36632775]\n",
      " [0.34689186 0.39526826 0.39102072 0.38646332 0.40489755 0.38646332]\n",
      " [0.41155307 0.42526232 0.42450115 0.39005183 0.3876019  0.39005183]\n",
      " [0.37577453 0.39546624 0.41212586 0.39842504 0.14455006 0.39842504]\n",
      " [0.39086548 0.39111067 0.35763622 0.33462919 0.61907085 0.33462919]] -> [0.30970893]\n",
      "[[0.6098341  0.591863   0.56753645 0.5294059  0.29818073 0.5294059 ]\n",
      " [0.39536278 0.40546426 0.34535687 0.36632775 0.59727424 0.36632775]\n",
      " [0.34689186 0.39526826 0.39102072 0.38646332 0.40489755 0.38646332]\n",
      " [0.41155307 0.42526232 0.42450115 0.39005183 0.3876019  0.39005183]\n",
      " [0.37577453 0.39546624 0.41212586 0.39842504 0.14455006 0.39842504]\n",
      " [0.39086548 0.39111067 0.35763622 0.33462919 0.61907085 0.33462919]\n",
      " [0.28972616 0.37972679 0.32885648 0.30970893 0.49512134 0.30970893]] -> [0.22498006]\n",
      "[[0.39536278 0.40546426 0.34535687 0.36632775 0.59727424 0.36632775]\n",
      " [0.34689186 0.39526826 0.39102072 0.38646332 0.40489755 0.38646332]\n",
      " [0.41155307 0.42526232 0.42450115 0.39005183 0.3876019  0.39005183]\n",
      " [0.37577453 0.39546624 0.41212586 0.39842504 0.14455006 0.39842504]\n",
      " [0.39086548 0.39111067 0.35763622 0.33462919 0.61907085 0.33462919]\n",
      " [0.28972616 0.37972679 0.32885648 0.30970893 0.49512134 0.30970893]\n",
      " [0.29902059 0.29518907 0.21105142 0.22498006 0.73063486 0.22498006]] -> [0.30801435]\n",
      "[[0.34689186 0.39526826 0.39102072 0.38646332 0.40489755 0.38646332]\n",
      " [0.41155307 0.42526232 0.42450115 0.39005183 0.3876019  0.39005183]\n",
      " [0.37577453 0.39546624 0.41212586 0.39842504 0.14455006 0.39842504]\n",
      " [0.39086548 0.39111067 0.35763622 0.33462919 0.61907085 0.33462919]\n",
      " [0.28972616 0.37972679 0.32885648 0.30970893 0.49512134 0.30970893]\n",
      " [0.29902059 0.29518907 0.21105142 0.22498006 0.73063486 0.22498006]\n",
      " [0.18378973 0.31043358 0.18898695 0.30801435 0.89806742 0.30801435]] -> [0.36333732]\n",
      "[[0.41155307 0.42526232 0.42450115 0.39005183 0.3876019  0.39005183]\n",
      " [0.37577453 0.39546624 0.41212586 0.39842504 0.14455006 0.39842504]\n",
      " [0.39086548 0.39111067 0.35763622 0.33462919 0.61907085 0.33462919]\n",
      " [0.28972616 0.37972679 0.32885648 0.30970893 0.49512134 0.30970893]\n",
      " [0.29902059 0.29518907 0.21105142 0.22498006 0.73063486 0.22498006]\n",
      " [0.18378973 0.31043358 0.18898695 0.30801435 0.89806742 0.30801435]\n",
      " [0.31880871 0.43070679 0.3708749  0.36333732 0.48349123 0.36333732]] -> [0.36144338]\n",
      "[[0.37577453 0.39546624 0.41212586 0.39842504 0.14455006 0.39842504]\n",
      " [0.39086548 0.39111067 0.35763622 0.33462919 0.61907085 0.33462919]\n",
      " [0.28972616 0.37972679 0.32885648 0.30970893 0.49512134 0.30970893]\n",
      " [0.29902059 0.29518907 0.21105142 0.22498006 0.73063486 0.22498006]\n",
      " [0.18378973 0.31043358 0.18898695 0.30801435 0.89806742 0.30801435]\n",
      " [0.31880871 0.43070679 0.3708749  0.36333732 0.48349123 0.36333732]\n",
      " [0.38127124 0.38507226 0.38392172 0.36144338 0.28917881 0.36144338]] -> [0.35905104]\n",
      "[[0.39086548 0.39111067 0.35763622 0.33462919 0.61907085 0.33462919]\n",
      " [0.28972616 0.37972679 0.32885648 0.30970893 0.49512134 0.30970893]\n",
      " [0.29902059 0.29518907 0.21105142 0.22498006 0.73063486 0.22498006]\n",
      " [0.18378973 0.31043358 0.18898695 0.30801435 0.89806742 0.30801435]\n",
      " [0.31880871 0.43070679 0.3708749  0.36333732 0.48349123 0.36333732]\n",
      " [0.38127124 0.38507226 0.38392172 0.36144338 0.28917881 0.36144338]\n",
      " [0.35858485 0.35864185 0.389198   0.35905104 0.35343552 0.35905104]] -> [0.40151515]\n",
      "[[0.28972616 0.37972679 0.32885648 0.30970893 0.49512134 0.30970893]\n",
      " [0.29902059 0.29518907 0.21105142 0.22498006 0.73063486 0.22498006]\n",
      " [0.18378973 0.31043358 0.18898695 0.30801435 0.89806742 0.30801435]\n",
      " [0.31880871 0.43070679 0.3708749  0.36333732 0.48349123 0.36333732]\n",
      " [0.38127124 0.38507226 0.38392172 0.36144338 0.28917881 0.36144338]\n",
      " [0.35858485 0.35864185 0.389198   0.35905104 0.35343552 0.35905104]\n",
      " [0.35688587 0.38744803 0.38641596 0.40151515 0.17613547 0.40151515]] -> [0.40560207]\n",
      "[[0.29902059 0.29518907 0.21105142 0.22498006 0.73063486 0.22498006]\n",
      " [0.18378973 0.31043358 0.18898695 0.30801435 0.89806742 0.30801435]\n",
      " [0.31880871 0.43070679 0.3708749  0.36333732 0.48349123 0.36333732]\n",
      " [0.38127124 0.38507226 0.38392172 0.36144338 0.28917881 0.36144338]\n",
      " [0.35858485 0.35864185 0.389198   0.35905104 0.35343552 0.35905104]\n",
      " [0.35688587 0.38744803 0.38641596 0.40151515 0.17613547 0.40151515]\n",
      " [0.37907256 0.40833498 0.41068688 0.40560207 0.3418369  0.40560207]] -> [0.42234848]\n",
      "[[0.18378973 0.31043358 0.18898695 0.30801435 0.89806742 0.30801435]\n",
      " [0.31880871 0.43070679 0.3708749  0.36333732 0.48349123 0.36333732]\n",
      " [0.38127124 0.38507226 0.38392172 0.36144338 0.28917881 0.36144338]\n",
      " [0.35858485 0.35864185 0.389198   0.35905104 0.35343552 0.35905104]\n",
      " [0.35688587 0.38744803 0.38641596 0.40151515 0.17613547 0.40151515]\n",
      " [0.37907256 0.40833498 0.41068688 0.40560207 0.3418369  0.40560207]\n",
      " [0.44203478 0.44506038 0.43697237 0.42234848 0.19577602 0.42234848]] -> [0.44437799]\n",
      "[[0.31880871 0.43070679 0.3708749  0.36333732 0.48349123 0.36333732]\n",
      " [0.38127124 0.38507226 0.38392172 0.36144338 0.28917881 0.36144338]\n",
      " [0.35858485 0.35864185 0.389198   0.35905104 0.35343552 0.35905104]\n",
      " [0.35688587 0.38744803 0.38641596 0.40151515 0.17613547 0.40151515]\n",
      " [0.37907256 0.40833498 0.41068688 0.40560207 0.3418369  0.40560207]\n",
      " [0.44203478 0.44506038 0.43697237 0.42234848 0.19577602 0.42234848]\n",
      " [0.43753748 0.459216   0.47006907 0.44437799 0.16411193 0.44437799]] -> [0.41656699]\n",
      "[[0.38127124 0.38507226 0.38392172 0.36144338 0.28917881 0.36144338]\n",
      " [0.35858485 0.35864185 0.389198   0.35905104 0.35343552 0.35905104]\n",
      " [0.35688587 0.38744803 0.38641596 0.40151515 0.17613547 0.40151515]\n",
      " [0.37907256 0.40833498 0.41068688 0.40560207 0.3418369  0.40560207]\n",
      " [0.44203478 0.44506038 0.43697237 0.42234848 0.19577602 0.42234848]\n",
      " [0.43753748 0.459216   0.47006907 0.44437799 0.16411193 0.44437799]\n",
      " [0.42314611 0.41971887 0.43553338 0.41656699 0.1890718  0.41656699]] -> [0.36991627]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "xy = np.loadtxt('data-02-stock_daily.csv', delimiter=',')\n",
    "xy = xy[::-1] # reverse order\n",
    "\n",
    "# split train-test set\n",
    "train_size = int(len(xy) * 0.7)\n",
    "train_set = xy[0:train_size]\n",
    "test_set = xy[train_size - seq_length:]\n",
    "\n",
    "# scaling data\n",
    "train_set = minmax_scaler(train_set)\n",
    "test_set = minmax_scaler(test_set)\n",
    "\n",
    "# make train-test dataset to input\n",
    "trainX, trainY = build_dataset(train_set, seq_length)\n",
    "testX, testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "# conver to tensor\n",
    "trainX_tensor = torch.FloatTensor(trainX)\n",
    "trainY_tensor = torch.FloatTensor(trainY)\n",
    "\n",
    "testX_tensor = torch.FloatTensor(testX)\n",
    "testY_tensor = torch.FloatTensor(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x[:, -1])\n",
    "        return x\n",
    "\n",
    "net = Net(data_dim, hidden_dim, output_dim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss & optimizer setting\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.45878687500953674\n",
      "1 0.041176360100507736\n",
      "2 0.16878177225589752\n",
      "3 0.030692802742123604\n",
      "4 0.029835352674126625\n",
      "5 0.040026962757110596\n",
      "6 0.014339081011712551\n",
      "7 0.005779946688562632\n",
      "8 0.014186447486281395\n",
      "9 0.021570591256022453\n",
      "10 0.019041478633880615\n",
      "11 0.010993008501827717\n",
      "12 0.00539082707837224\n",
      "13 0.00544100534170866\n",
      "14 0.008740758523344994\n",
      "15 0.011100833304226398\n",
      "16 0.010259130969643593\n",
      "17 0.006960432510823011\n",
      "18 0.0036984123289585114\n",
      "19 0.002570732729509473\n",
      "20 0.0037071763072162867\n",
      "21 0.005364039447158575\n",
      "22 0.005630201660096645\n",
      "23 0.004148378502577543\n",
      "24 0.002231847494840622\n",
      "25 0.0014504700666293502\n",
      "26 0.0021470242645591497\n",
      "27 0.0032568168826401234\n",
      "28 0.0034927234519273043\n",
      "29 0.0026076030917465687\n",
      "30 0.0014942933339625597\n",
      "31 0.0011651075910776854\n",
      "32 0.0017172356601804495\n",
      "33 0.0023382746148854494\n",
      "34 0.002272464334964752\n",
      "35 0.0016157918144017458\n",
      "36 0.0011011044261977077\n",
      "37 0.0012121498584747314\n",
      "38 0.001666691736318171\n",
      "39 0.0018355980282649398\n",
      "40 0.0015229720156639814\n",
      "41 0.001130361924879253\n",
      "42 0.0010904398513957858\n",
      "43 0.0013513806043192744\n",
      "44 0.0015171836130321026\n",
      "45 0.001375164370983839\n",
      "46 0.0011198228457942605\n",
      "47 0.0010522661032155156\n",
      "48 0.0011961478739976883\n",
      "49 0.0013131549349054694\n",
      "50 0.0012409004848450422\n",
      "51 0.0010816523572430015\n",
      "52 0.0010300091234967113\n",
      "53 0.0011133331572636962\n",
      "54 0.00118494953494519\n",
      "55 0.0011395557085052133\n",
      "56 0.0010410809190943837\n",
      "57 0.0010120646329596639\n",
      "58 0.001064957119524479\n",
      "59 0.0011042141122743487\n",
      "60 0.001069563557393849\n",
      "61 0.0010095617035403848\n",
      "62 0.0009990722173824906\n",
      "63 0.0010345462942495942\n",
      "64 0.0010513880988582969\n",
      "65 0.0010218595853075385\n",
      "66 0.0009871957590803504\n",
      "67 0.000989260501228273\n",
      "68 0.0010120130609720945\n",
      "69 0.0010133597534149885\n",
      "70 0.0009888701606541872\n",
      "71 0.0009720147354528308\n",
      "72 0.0009798912797123194\n",
      "73 0.0009909167420119047\n",
      "74 0.0009826096938923001\n",
      "75 0.0009650840074755251\n",
      "76 0.0009608225664123893\n",
      "77 0.0009687496931292117\n",
      "78 0.0009694767650216818\n",
      "79 0.000957978016231209\n",
      "80 0.0009491743985563517\n",
      "81 0.0009514008415862918\n",
      "82 0.0009542387560941279\n",
      "83 0.0009481392917223275\n",
      "84 0.0009397573885507882\n",
      "85 0.000938582350499928\n",
      "86 0.0009407968609593809\n",
      "87 0.0009376313537359238\n",
      "88 0.0009308888693340123\n",
      "89 0.0009282433893531561\n",
      "90 0.0009291568421758711\n",
      "91 0.0009271128801628947\n",
      "92 0.0009219664498232305\n",
      "93 0.0009192148572765291\n",
      "94 0.0009192856959998608\n",
      "95 0.0009175073355436325\n",
      "96 0.0009133959538303316\n",
      "97 0.0009108930826187134\n",
      "98 0.0009103595511987805\n",
      "99 0.0009084789780899882\n",
      "100 0.000905116496142\n",
      "101 0.0009030777146108449\n",
      "102 0.0009022293379530311\n",
      "103 0.00090019655181095\n",
      "104 0.0008973677759058774\n",
      "105 0.000895691046025604\n",
      "106 0.0008945433655753732\n",
      "107 0.0008924060966819525\n",
      "108 0.0008900982793420553\n",
      "109 0.0008887232979759574\n",
      "110 0.0008873292244970798\n",
      "111 0.0008852118626236916\n",
      "112 0.00088335300097242\n",
      "113 0.0008820710936561227\n",
      "114 0.0008804522221907973\n",
      "115 0.000878523918800056\n",
      "116 0.0008770427666604519\n",
      "117 0.0008757105679251254\n",
      "118 0.00087399547919631\n",
      "119 0.0008723513456061482\n",
      "120 0.0008710394613444805\n",
      "121 0.0008695730357430875\n",
      "122 0.0008679509628564119\n",
      "123 0.0008665815694257617\n",
      "124 0.0008652645628899336\n",
      "125 0.0008637529681436718\n",
      "126 0.0008623400353826582\n",
      "127 0.0008610756485722959\n",
      "128 0.0008596920524723828\n",
      "129 0.0008582996670156717\n",
      "130 0.0008570572244934738\n",
      "131 0.0008557718247175217\n",
      "132 0.0008544283336959779\n",
      "133 0.0008531969506293535\n",
      "134 0.0008519789553247392\n",
      "135 0.0008507009479217231\n",
      "136 0.0008494954090565443\n",
      "137 0.0008483276469632983\n",
      "138 0.0008471101755276322\n",
      "139 0.0008459349628537893\n",
      "140 0.0008448054431937635\n",
      "141 0.0008436422212980688\n",
      "142 0.0008425062987953424\n",
      "143 0.0008414127514697611\n",
      "144 0.0008402967359870672\n",
      "145 0.0008391985320486128\n",
      "146 0.0008381369989365339\n",
      "147 0.0008370628347620368\n",
      "148 0.000836004561278969\n",
      "149 0.0008349762065336108\n",
      "150 0.000833939528092742\n",
      "151 0.0008329178672283888\n",
      "152 0.0008319203625433147\n",
      "153 0.0008309184340760112\n",
      "154 0.0008299325709231198\n",
      "155 0.0008289653924293816\n",
      "156 0.0008279962348751724\n",
      "157 0.0008270431426353753\n",
      "158 0.0008261040784418583\n",
      "159 0.0008251661201938987\n",
      "160 0.0008242446929216385\n",
      "161 0.0008233330445364118\n",
      "162 0.000822424772195518\n",
      "163 0.0008215319248847663\n",
      "164 0.0008206455386243761\n",
      "165 0.0008197660790756345\n",
      "166 0.0008189002983272076\n",
      "167 0.0008180387085303664\n",
      "168 0.000817186024505645\n",
      "169 0.0008163439924828708\n",
      "170 0.0008155073155649006\n",
      "171 0.0008146805339492857\n",
      "172 0.0008138612611219287\n",
      "173 0.0008130478672683239\n",
      "174 0.0008122447761707008\n",
      "175 0.0008114466327242553\n",
      "176 0.0008106569875963032\n",
      "177 0.0008098746766336262\n",
      "178 0.0008090982446447015\n",
      "179 0.0008083295542746782\n",
      "180 0.0008075673249550164\n",
      "181 0.0008068112656474113\n",
      "182 0.0008060626569204032\n",
      "183 0.0008053191704675555\n",
      "184 0.0008045829017646611\n",
      "185 0.0008038525702431798\n",
      "186 0.0008031280594877899\n",
      "187 0.0008024097187444568\n",
      "188 0.0008016967331059277\n",
      "189 0.0008009892771951854\n",
      "190 0.0008002882823348045\n",
      "191 0.0007995920022949576\n",
      "192 0.0007989014848135412\n",
      "193 0.000798216147813946\n",
      "194 0.0007975359330885112\n",
      "195 0.0007968606660142541\n",
      "196 0.0007961905212141573\n",
      "197 0.0007955251494422555\n",
      "198 0.0007948648417368531\n",
      "199 0.000794209074229002\n",
      "200 0.000793557905126363\n",
      "201 0.0007929111015982926\n",
      "202 0.0007922686636447906\n",
      "203 0.0007916308240965009\n",
      "204 0.0007909974665381014\n",
      "205 0.0007903679506853223\n",
      "206 0.0007897430914454162\n",
      "207 0.0007891216082498431\n",
      "208 0.0007885040831752121\n",
      "209 0.0007878908072598279\n",
      "210 0.0007872813148424029\n",
      "211 0.0007866756641305983\n",
      "212 0.0007860734476707876\n",
      "213 0.0007854747236706316\n",
      "214 0.0007848797831684351\n",
      "215 0.0007842881605029106\n",
      "216 0.0007837002049200237\n",
      "217 0.0007831151597201824\n",
      "218 0.000782533548772335\n",
      "219 0.0007819552556611598\n",
      "220 0.0007813800475560129\n",
      "221 0.0007808078662492335\n",
      "222 0.0007802388863638043\n",
      "223 0.0007796730496920645\n",
      "224 0.0007791096577420831\n",
      "225 0.0007785492343828082\n",
      "226 0.0007779918378219008\n",
      "227 0.000777437468059361\n",
      "228 0.000776885193772614\n",
      "229 0.000776335597038269\n",
      "230 0.0007757889688946307\n",
      "231 0.0007752449600957334\n",
      "232 0.0007747028721496463\n",
      "233 0.0007741639856249094\n",
      "234 0.0007736270199529827\n",
      "235 0.0007730927900411189\n",
      "236 0.0007725606556050479\n",
      "237 0.0007720308494754136\n",
      "238 0.0007715032552368939\n",
      "239 0.0007709782803431153\n",
      "240 0.0007704552263021469\n",
      "241 0.0007699340931139886\n",
      "242 0.0007694154046475887\n",
      "243 0.000768898578826338\n",
      "244 0.0007683839066885412\n",
      "245 0.0007678711554035544\n",
      "246 0.0007673601503483951\n",
      "247 0.0007668514735996723\n",
      "248 0.000766344484873116\n",
      "249 0.0007658396498300135\n",
      "250 0.0007653365610167384\n",
      "251 0.0007648353348486125\n",
      "252 0.000764335272833705\n",
      "253 0.0007638379465788603\n",
      "254 0.000763341726269573\n",
      "255 0.0007628475432284176\n",
      "256 0.000762354873586446\n",
      "257 0.0007618637173436582\n",
      "258 0.0007613742491230369\n",
      "259 0.0007608867017552257\n",
      "260 0.0007604006095789373\n",
      "261 0.0007599161472171545\n",
      "262 0.0007594329072162509\n",
      "263 0.0007589515880681574\n",
      "264 0.0007584714912809432\n",
      "265 0.0007579930243082345\n",
      "266 0.0007575161871500313\n",
      "267 0.0007570404559373856\n",
      "268 0.0007565662963315845\n",
      "269 0.0007560935337096453\n",
      "270 0.0007556223426945508\n",
      "271 0.0007551527232863009\n",
      "272 0.0007546839769929647\n",
      "273 0.0007542168023064733\n",
      "274 0.0007537510828115046\n",
      "275 0.0007532864110544324\n",
      "276 0.0007528233691118658\n",
      "277 0.0007523613166995347\n",
      "278 0.0007519008358940482\n",
      "279 0.0007514413446187973\n",
      "280 0.000750983483158052\n",
      "281 0.0007505267858505249\n",
      "282 0.0007500715437345207\n",
      "283 0.0007496172329410911\n",
      "284 0.0007491638534702361\n",
      "285 0.0007487121038138866\n",
      "286 0.0007482618093490601\n",
      "287 0.0007478123879991472\n",
      "288 0.000747363839764148\n",
      "289 0.000746917212381959\n",
      "290 0.0007464712834917009\n",
      "291 0.0007460264023393393\n",
      "292 0.0007455828017555177\n",
      "293 0.0007451405399478972\n",
      "294 0.0007446992676705122\n",
      "295 0.0007442593923769891\n",
      "296 0.0007438204484060407\n",
      "297 0.0007433826103806496\n",
      "298 0.0007429459365084767\n",
      "299 0.0007425102521665394\n",
      "300 0.0007420758483931422\n",
      "301 0.0007416423177346587\n",
      "302 0.0007412101258523762\n",
      "303 0.0007407790399156511\n",
      "304 0.0007403491181321442\n",
      "305 0.0007399201858788729\n",
      "306 0.0007394920685328543\n",
      "307 0.0007390650571323931\n",
      "308 0.0007386393845081329\n",
      "309 0.0007382147596217692\n",
      "310 0.0007377911824733019\n",
      "311 0.0007373688858933747\n",
      "312 0.0007369474042207003\n",
      "313 0.0007365269702859223\n",
      "314 0.0007361075258813798\n",
      "315 0.0007356888963840902\n",
      "316 0.0007352718967013061\n",
      "317 0.0007348557119257748\n",
      "318 0.000734440516680479\n",
      "319 0.0007340264855884016\n",
      "320 0.0007336135022342205\n",
      "321 0.000733201508410275\n",
      "322 0.0007327906787395477\n",
      "323 0.0007323806057684124\n",
      "324 0.0007319720461964607\n",
      "325 0.0007315641851164401\n",
      "326 0.000731157255358994\n",
      "327 0.000730751664377749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328 0.0007303470047190785\n",
      "329 0.0007299432763829827\n",
      "330 0.0007295407122001052\n",
      "331 0.0007291390211321414\n",
      "332 0.0007287386106327176\n",
      "333 0.0007283391314558685\n",
      "334 0.0007279410492628813\n",
      "335 0.0007275433163158596\n",
      "336 0.0007271469221450388\n",
      "337 0.0007267517503350973\n",
      "338 0.0007263575098477304\n",
      "339 0.000725964258890599\n",
      "340 0.0007255717646330595\n",
      "341 0.0007251808419823647\n",
      "342 0.0007247906178236008\n",
      "343 0.0007244014996103942\n",
      "344 0.0007240134291350842\n",
      "345 0.0007236266392283142\n",
      "346 0.0007232404896058142\n",
      "347 0.0007228556205518544\n",
      "348 0.0007224719738587737\n",
      "349 0.0007220892002806067\n",
      "350 0.000721707649063319\n",
      "351 0.0007213270873762667\n",
      "352 0.0007209470495581627\n",
      "353 0.0007205686997622252\n",
      "354 0.0007201912230812013\n",
      "355 0.0007198149687610567\n",
      "356 0.0007194398203864694\n",
      "357 0.0007190654287114739\n",
      "358 0.0007186923176050186\n",
      "359 0.0007183203124441206\n",
      "360 0.0007179492968134582\n",
      "361 0.0007175792707130313\n",
      "362 0.0007172104669734836\n",
      "363 0.0007168425945565104\n",
      "364 0.0007164765265770257\n",
      "365 0.00071611104067415\n",
      "366 0.0007157464860938489\n",
      "367 0.0007153829792514443\n",
      "368 0.0007150208111852407\n",
      "369 0.0007146599236875772\n",
      "370 0.0007142998510971665\n",
      "371 0.0007139412919059396\n",
      "372 0.0007135833729989827\n",
      "373 0.0007132271421141922\n",
      "374 0.0007128715515136719\n",
      "375 0.0007125173578970134\n",
      "376 0.0007121644448488951\n",
      "377 0.0007118121720850468\n",
      "378 0.000711461529135704\n",
      "379 0.0007111119921319187\n",
      "380 0.0007107637939043343\n",
      "381 0.0007104164105840027\n",
      "382 0.0007100700750015676\n",
      "383 0.0007097251946106553\n",
      "384 0.000709381652995944\n",
      "385 0.0007090392173267901\n",
      "386 0.0007086980040185153\n",
      "387 0.0007083581876941025\n",
      "388 0.0007080193026922643\n",
      "389 0.0007076815818436444\n",
      "390 0.00070734549080953\n",
      "391 0.0007070103310979903\n",
      "392 0.0007066761027090251\n",
      "393 0.0007063436205498874\n",
      "394 0.0007060121861286461\n",
      "395 0.0007056824397295713\n",
      "396 0.0007053536828607321\n",
      "397 0.0007050259737297893\n",
      "398 0.0007046997779980302\n",
      "399 0.0007043747464194894\n",
      "400 0.0007040515192784369\n",
      "401 0.0007037293980829418\n",
      "402 0.0007034080917946994\n",
      "403 0.0007030884153209627\n",
      "404 0.0007027704268693924\n",
      "405 0.0007024532533250749\n",
      "406 0.0007021378260105848\n",
      "407 0.0007018235046416521\n",
      "408 0.0007015108712948859\n",
      "409 0.0007011991692706943\n",
      "410 0.0007008890970610082\n",
      "411 0.0007005801307968795\n",
      "412 0.0007002727943472564\n",
      "413 0.0006999668548814952\n",
      "414 0.0006996625452302396\n",
      "415 0.0006993593415245414\n",
      "416 0.0006990574183873832\n",
      "417 0.0006987572414800525\n",
      "418 0.0006984584033489227\n",
      "419 0.0006981611950322986\n",
      "420 0.0006978650344535708\n",
      "421 0.0006975706783123314\n",
      "422 0.0006972773117013276\n",
      "423 0.000696985749527812\n",
      "424 0.0006966956425458193\n",
      "425 0.0006964069907553494\n",
      "426 0.0006961198523640633\n",
      "427 0.0006958342855796218\n",
      "428 0.0006955500575713813\n",
      "429 0.0006952672265470028\n",
      "430 0.0006949857925064862\n",
      "431 0.0006947062211111188\n",
      "432 0.0006944280466996133\n",
      "433 0.0006941512692719698\n",
      "434 0.00069387600524351\n",
      "435 0.000693602254614234\n",
      "436 0.0006933298427611589\n",
      "437 0.000693059409968555\n",
      "438 0.0006927901995368302\n",
      "439 0.0006925225025042892\n",
      "440 0.0006922566099092364\n",
      "441 0.0006919917068444192\n",
      "442 0.000691728841047734\n",
      "443 0.0006914670811966062\n",
      "444 0.0006912073586136103\n",
      "445 0.0006909488001838326\n",
      "446 0.0006906916969455779\n",
      "447 0.0006904361071065068\n",
      "448 0.0006901820306666195\n",
      "449 0.0006899298168718815\n",
      "450 0.0006896788836456835\n",
      "451 0.0006894293474033475\n",
      "452 0.000689181440975517\n",
      "453 0.0006889351061545312\n",
      "454 0.0006886901101097465\n",
      "455 0.0006884469767101109\n",
      "456 0.0006882051238790154\n",
      "457 0.0006879643187858164\n",
      "458 0.0006877254927530885\n",
      "459 0.0006874879472889006\n",
      "460 0.0006872517988085747\n",
      "461 0.0006870171637274325\n",
      "462 0.0006867843330837786\n",
      "463 0.000686552666593343\n",
      "464 0.0006863227463327348\n",
      "465 0.0006860935827717185\n",
      "466 0.0006858663400635123\n",
      "467 0.0006856404943391681\n",
      "468 0.0006854161038063467\n",
      "469 0.0006851929356344044\n",
      "470 0.0006849713972769678\n",
      "471 0.0006847509648650885\n",
      "472 0.0006845322786830366\n",
      "473 0.0006843148148618639\n",
      "474 0.0006840983987785876\n",
      "475 0.0006838838453404605\n",
      "476 0.0006836700486019254\n",
      "477 0.0006834581727162004\n",
      "478 0.0006832477520219982\n",
      "479 0.0006830381462350488\n",
      "480 0.000682829471770674\n",
      "481 0.0006826225435361266\n",
      "482 0.0006824170122854412\n",
      "483 0.0006822129362262785\n",
      "484 0.0006820093840360641\n",
      "485 0.0006818077526986599\n",
      "486 0.0006816069944761693\n",
      "487 0.0006814078078605235\n",
      "488 0.0006812094361521304\n",
      "489 0.0006810126360505819\n",
      "490 0.0006808165344409645\n",
      "491 0.0006806218880228698\n",
      "492 0.0006804283475503325\n",
      "493 0.0006802360876463354\n",
      "494 0.0006800447008572519\n",
      "495 0.0006798548856750131\n",
      "496 0.0006796655361540616\n",
      "497 0.0006794774672016501\n",
      "498 0.0006792909116484225\n",
      "499 0.000679104938171804\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for i in range(iterations):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(trainX_tensor)\n",
    "    loss = criterion(outputs, trainY_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(testY)\n",
    "plt.plot(net(testX_tensor).data.numpy())\n",
    "plt.legend(['original', 'prediction'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence to Sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main reference\n",
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = [\"I feel hungry.\t나는 배가 고프다.\",\n",
    "       \"Pytorch is very easy.\t파이토치는 매우 쉽다.\",\n",
    "       \"Pytorch is a framework for deep learning.\t파이토치는 딥러닝을 위한 프레임워크이다.\",\n",
    "       \"Pytorch is very clear to use.\t파이토치는 사용하기 매우 직관적이다.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix token for 'start of sentence' and 'end of sentence'\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for vocabulary related information of data\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.vocab2index = {'<SOS>' : SOS_token, '<EOS>':EOS_token}\n",
    "        self.index2vocab = {SOS_token:'<SOS>', EOS_token:'<EOS>'}\n",
    "        self.vocab_count = {}\n",
    "        self.n_vocab = len(self.vocab2index)\n",
    "        \n",
    "    def add_vocab(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            if word not in self.vocab2index:\n",
    "                self.vocab2index[word] = self.n_vocab\n",
    "                self.vocab_count[word] = 1\n",
    "                self.index2vocab[self.n_vocab] = word\n",
    "                self.n_vocab += 1\n",
    "            else:\n",
    "                self.vocab_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the long sentence from source and target data\n",
    "def filter_pair(pair, source_max_length, target_max_length):\n",
    "    return len(pair[0].split(\" \")) < source_max_length and len(pair[1].split(\" \")) < target_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and preprocess the corpus data\n",
    "def preprocess(corpus, source_max_length, target_max_length):\n",
    "    print('reading corpus...')\n",
    "    pairs = []\n",
    "    for line in corpus:\n",
    "        pairs.append([s for s in line.strip().lower().split('\\t')])\n",
    "    print('Read {} sentence pairs'.format(len(pairs)))\n",
    "    \n",
    "    pairs = [pair for pair in pairs if filter_pair(pair, source_max_length, target_max_length)]\n",
    "    print(\"Trimmed to {} sentence pairs\".format(len(pairs)))\n",
    "    \n",
    "    source_vocab = Vocab()\n",
    "    target_vocab = Vocab()\n",
    "    \n",
    "    print('Counting words...')\n",
    "    for pair in pairs:\n",
    "        source_vocab.add_vocab(pair[0])\n",
    "        target_vocab.add_vocab(pair[1])\n",
    "    print('source vocab size =', source_vocab.n_vocab)\n",
    "    print('target vocab size =', target_vocab.n_vocab)\n",
    "    \n",
    "    return pairs, source_vocab, target_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare simple encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x).view(1, 1, -1)\n",
    "        x, hidden = self.gru(x, hidden)\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare simple decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x).view(1, 1, -1)\n",
    "        x, hidden = self.gru(x, hidden)\n",
    "        x = self.softmax(self.out(x[0]))\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sentence to the index tensor\n",
    "def tensorize(vocab, sentence):\n",
    "    indexes = [vocab.vocab2index[word] for word in sentence.split(' ')]\n",
    "    indexes.append(vocab.vocab2index['<EOS>'])\n",
    "    return torch.Tensor(indexes).long().to(device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training seq2seq\n",
    "def train(pairs, source_vocab, target_vocab, encoder, decoder, n_iter, \n",
    "         print_every=1000, learning_rate=0.01):\n",
    "    loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    training_batch = [random.choice(pairs) for _ in range(n_iter)]\n",
    "    training_source = [tensorize(source_vocab, pair[0]) for pair in training_batch]\n",
    "    training_target = [tensorize(target_vocab, pair[1]) for pair in training_batch]\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for i in range(1, n_iter + 1):\n",
    "        source_tensor = training_source[i - 1]\n",
    "        target_tensor = training_target[i - 1]\n",
    "        \n",
    "        encoder_hidden = torch.zeros([1, 1, encoder.hidden_size]).to(device)\n",
    "        \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        source_length = source_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        for enc_input in range(source_length):\n",
    "            _, encoder_hidden  = encoder(source_tensor[enc_input], encoder_hidden)\n",
    "        \n",
    "        decoder_input = torch.Tensor([[SOS_token]]).long().to(device)\n",
    "        decoder_hidden = encoder_hidden # connect encoder output to decoder input\n",
    "        \n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di] # teacher forcing\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        loss_iter = loss.item() / target_length\n",
    "        loss_total += loss_iter\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            loss_avg = loss_total / print_every\n",
    "            loss_total = 0\n",
    "            print('[{} - {}%] loss = {:05.4f}'.format(\n",
    "            i, i/n_iter*100, loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert given sentence to check the training\n",
    "def evaluate(pairs, source_vocab, target_vocab, encoder, decoder, target_max_length):\n",
    "    for pair in pairs:\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        source_tensor = tensorize(source_vocab, pair[0])\n",
    "        source_length = source_tensor.size()[0]\n",
    "        encoder_hidden = torch.zeros([1, 1, encoder.hidden_size]).to(device)\n",
    "        \n",
    "        for ei in range(source_length):\n",
    "            _, encoder_hidden = encoder(source_tensor[ei], encoder_hidden)\n",
    "        \n",
    "        decoder_input = torch.Tensor([[SOS_token]], device=device).long()\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "        \n",
    "        for di in range(target_max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            _, top_index = decoder_output.data.topk(1)\n",
    "            if top_index.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(target_vocab.index2vocab[top_index.item()])\n",
    "                \n",
    "            decoder_input = top_index.squeeze().detach()\n",
    "        \n",
    "        predict_words = decoded_words\n",
    "        predict_sentence = ' '.join(predict_words)\n",
    "        print('<', predict_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare max length for sentence\n",
    "SOURCE_MAX_LENGTH = 10\n",
    "TARGET_MAX_LENGTH = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading corpus...\n",
      "Read 4 sentence pairs\n",
      "Trimmed to 4 sentence pairs\n",
      "Counting words...\n",
      "source vocab size = 17\n",
      "target vocab size = 13\n"
     ]
    }
   ],
   "source": [
    "# preprocess the corpus\n",
    "load_pairs, load_source_vocab, load_target_vocab = preprocess(\n",
    "    raw, SOURCE_MAX_LENGTH, TARGET_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the encoder and the decoder\n",
    "enc_hidden_size = 16\n",
    "dec_hidden_size = enc_hidden_size\n",
    "enc = Encoder(load_source_vocab.n_vocab, enc_hidden_size).to(device)\n",
    "dec = Decoder(dec_hidden_size, load_target_vocab.n_vocab).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000 - 20.0%] loss = 0.7338\n",
      "[2000 - 40.0%] loss = 0.1047\n",
      "[3000 - 60.0%] loss = 0.0339\n",
      "[4000 - 80.0%] loss = 0.0183\n",
      "[5000 - 100.0%] loss = 0.0125\n"
     ]
    }
   ],
   "source": [
    "# train seq2seq model\n",
    "train(load_pairs, load_source_vocab, load_target_vocab, enc, dec, 5000, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> i feel hungry.\n",
      "= 나는 배가 고프다.\n",
      "< 나는 배가 고프다. <EOS>\n",
      "\n",
      "> pytorch is very easy.\n",
      "= 파이토치는 매우 쉽다.\n",
      "< 파이토치는 매우 쉽다. <EOS>\n",
      "\n",
      "> pytorch is a framework for deep learning.\n",
      "= 파이토치는 딥러닝을 위한 프레임워크이다.\n",
      "< 파이토치는 딥러닝을 위한 프레임워크이다. <EOS>\n",
      "\n",
      "> pytorch is very clear to use.\n",
      "= 파이토치는 사용하기 매우 직관적이다.\n",
      "< 파이토치는 사용하기 매우 직관적이다. <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the model with given data\n",
    "evaluate(load_pairs, load_source_vocab, load_target_vocab, enc, dec, TARGET_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random word from random word generator\n",
    "# batch size = 5, max length sequence = 13\n",
    "data = ['hello world',\n",
    "        'midnight',\n",
    "        'calculation',\n",
    "        'path',\n",
    "        'short circuit']\n",
    "\n",
    "# Make dictionary\n",
    "# Get all characters and include pad token\n",
    "char_set = ['<pad>'] + list(set(char for seq in data for char in seq))\n",
    "\n",
    "# Construct character to index dictionary\n",
    "char2idx = {char:idx for idx, char in enumerate(char_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4, 13, 18, 18,  2, 15, 10,  2, 14, 18,  1])\n",
      "tensor([12, 16,  1,  8, 16,  7,  4,  5])\n",
      "tensor([ 3,  9, 18,  3,  6, 18,  9,  5, 16,  2,  8])\n",
      "tensor([11,  9,  5,  4])\n",
      "tensor([17,  4,  2, 14,  5, 15,  3, 16, 14,  3,  6, 16,  5])\n"
     ]
    }
   ],
   "source": [
    "# Convert character to index and make list to tensors.\n",
    "X = [torch.LongTensor([char2idx[char] for char in seq]) for seq in data]\n",
    "\n",
    "# Check converted result\n",
    "for sequence in X:\n",
    "    print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengths: [11, 8, 11, 4, 13]\n"
     ]
    }
   ],
   "source": [
    "# Make length tensor (will be used later in 'pack_padded_sequence' function)\n",
    "lengths = [len(seq) for seq in X]\n",
    "print('lengths:',lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence 데이터를 batch로 묶을때  \n",
    " text나 audio처럼 sequence 한 데이터의 경우 길이가 각각 다르기 때문에  \n",
    "하나의 batch로 만들어주기 위해서 일반적으로 제일 긴 sequence 길이에 맞춰  \n",
    "뒷 부분에 padding을 추가함. 이는 일반적으로 많이 쓰임.  \n",
    " 하지만 PackedSequence 를 쓰면 padding 없이도 정확히 필요한 부분까지만  \n",
    "병렬 계산 가능함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PaddedSequence는 sequence중에서 가장 긴 sequence와 길이를 맞추기 위해 padding을  \n",
    "추가한 일반적인 Tensor임.  \n",
    "input이 Tensor들의 list로 주어져야 함. list 안에 있는 각각의 Tensor의 shape이  \n",
    "(?, a, b, ...)라고 할 때, (?는 각각 다른 sequence length) pad_sequence 함수는  \n",
    "(T, batch_size, a, b, ...) shape를 가지는 Tensor가 리턴됨. (여기서 T는 batch 안에서  \n",
    "가장 큰 sequence length 임)  \n",
    "만약, pad_sequence에 명시적으로 batch_first=True 라는 파라미터를 지정해주면,  \n",
    "(batch_size, T, a, b, ...) shape을 가지는 Tensor가 리턴됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4, 13, 18, 18,  2, 15, 10,  2, 14, 18,  1,  0,  0],\n",
      "        [12, 16,  1,  8, 16,  7,  4,  5,  0,  0,  0,  0,  0],\n",
      "        [ 3,  9, 18,  3,  6, 18,  9,  5, 16,  2,  8,  0,  0],\n",
      "        [11,  9,  5,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [17,  4,  2, 14,  5, 15,  3, 16, 14,  3,  6, 16,  5]])\n",
      "torch.Size([5, 13])\n"
     ]
    }
   ],
   "source": [
    "# Make a Tensor of shape (Batch x Maximum_Sequence_Length)\n",
    "padded_sequence = pad_sequence(X, batch_first=True) # X is now padded sequence\n",
    "print(padded_sequence)\n",
    "print(padded_sequence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pack_sequence 함수를 이용하여 PackedSequence 만들기\n",
    "- PackedSequence는 위와 같이 padding token을 추가하여 sequence의 최대 길이에 맞는 Tensor를 만드는게 아닌, padding을 추가하지 않고 정확히 주어진 sequence 길이까지만 모델이 연산을 하게끔 만드는 pytorch의 자료구조임.  \n",
    "- 단, 주어지는 input (list of Tensor)은 길이에 따른 내림차순으로 정렬 되어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17,  4,  2, 14,  5, 15,  3, 16, 14,  3,  6, 16,  5])\n",
      "tensor([ 4, 13, 18, 18,  2, 15, 10,  2, 14, 18,  1])\n",
      "tensor([ 3,  9, 18,  3,  6, 18,  9,  5, 16,  2,  8])\n",
      "tensor([12, 16,  1,  8, 16,  7,  4,  5])\n",
      "tensor([11,  9,  5,  4])\n"
     ]
    }
   ],
   "source": [
    "# Sort by descending lengths\n",
    "sorted_idx = sorted(range(len(lengths)), key=lengths.__getitem__, reverse=True)\n",
    "sorted_X = [X[idx] for idx in sorted_idx]\n",
    "\n",
    "# check converted result\n",
    "for sequence in sorted_X:\n",
    "    print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([17,  4,  3, 12, 11,  4, 13,  9, 16,  9,  2, 18, 18,  1,  5, 14, 18,  3,\n",
      "         8,  4,  5,  2,  6, 16, 15, 15, 18,  7,  3, 10,  9,  4, 16,  2,  5,  5,\n",
      "        14, 14, 16,  3, 18,  2,  6,  1,  8, 16,  5]), batch_sizes=tensor([5, 5, 5, 5, 4, 4, 4, 4, 3, 3, 3, 1, 1]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "# pack_sequence를 이용하여 PackedSequence 만들기\n",
    "packed_sequence = pack_sequence(sorted_X)\n",
    "print(packed_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding 적용하기   \n",
    "- RNN에 input으로 넣어서 테스트\n",
    "- 위 예제들은 input이 character의 index들을 가지고 있었지만, 보통은 주로 이를 embedding 한 값을 RNN의 input으로 넣어줌\n",
    "- one-hot character embedding을 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 13, 19])\n"
     ]
    }
   ],
   "source": [
    "# one-hot embedding using PaddedSequence\n",
    "eye = torch.eye(len(char_set)) # Identity matrix of shape (len(char_set), len(char_Set))\n",
    "embedded_tensor = eye[padded_sequence]\n",
    "\n",
    "# shape : (Batch_size, max_sequence_length, number_of_input_tokens)\n",
    "print(embedded_tensor.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47, 19])\n"
     ]
    }
   ],
   "source": [
    "# one-hot embedding using PackedSequence\n",
    "embedded_packed_seq = pack_sequence([eye[X[idx]] for idx in sorted_idx])\n",
    "print(embedded_packed_seq.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare RNN\n",
    "rnn = torch.nn.RNN(input_size=len(char_set), hidden_size=30, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 13, 30])\n",
      "torch.Size([1, 5, 30])\n"
     ]
    }
   ],
   "source": [
    "# PaddedSequence 이용\n",
    "rnn_output, hidden = rnn(embedded_tensor)\n",
    "print(rnn_output.shape) # shape : (batch_size, max_seq_length, hidden_size)\n",
    "print(hidden.shape) # shape : (num_layers * num_directions, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47, 30])\n",
      "torch.Size([1, 5, 30])\n"
     ]
    }
   ],
   "source": [
    "# PackedSequence 이용\n",
    "rnn_output, hidden = rnn(embedded_packed_seq)\n",
    "print(rnn_output.data.shape)\n",
    "print(hidden.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad_packed_sequence  \n",
    "PackedSequence -> PaddedSequence(Tensor)로 바꿔주는 함수  \n",
    "return : (Tensor, list_of_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 13, 19])\n",
      "tensor([13, 11, 11,  8,  4])\n"
     ]
    }
   ],
   "source": [
    "unpacked_sequence, seq_lengths = pad_packed_sequence(\n",
    "    embedded_packed_seq, batch_first=True)\n",
    "print(unpacked_sequence.shape)\n",
    "print(seq_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pack_padded_sequence  \n",
    "PaddedSequence -> PackedSequence 로 바꿔주는 함수  \n",
    "pack_padded_sequence 함수는 실제 sequence 길이에 대한 정보를 모르기 때문에,  \n",
    "파라미터로 꼭 제공해줘야 함  \n",
    "input인 PaddedSequence 가 아까 언급드린 길이에 따른 내림차순으로 정렬되어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 13, 19])\n"
     ]
    }
   ],
   "source": [
    "embedded_padded_sequence = eye[pad_sequence(sorted_X, batch_first=True)]\n",
    "print(embedded_padded_sequence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47, 19])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.]])\n"
     ]
    }
   ],
   "source": [
    "# padding이 된 Tensor를 PackedSequence 로 변환\n",
    "sorted_lengths = sorted(lengths, reverse=True)\n",
    "new_packed_sequence = pack_padded_sequence(\n",
    "    embedded_padded_sequence, sorted_lengths, batch_first=True)\n",
    "print(new_packed_sequence.data.shape)\n",
    "print(new_packed_sequence.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
