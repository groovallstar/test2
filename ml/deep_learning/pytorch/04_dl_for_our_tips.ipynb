{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Likelihood Estimation (MLE)\n",
    "- 최대 가능도(우도) 추정\n",
    "  - 압정을 던졌을 때 두가지 Class가 존재함 (뒤집어지거나 옆으로 뉘거나)\n",
    "  - 이를 베르누이 분포(이항 분포)라고 함 (class 0, class 1)\n",
    "  - 100번을 던졌을때, \n",
    "    - n = 100\n",
    "    - k = 27 (뒤집어진 횟수) \n",
    "    - $ K \\sim B(n, \\theta) $ (observation)\n",
    "  - $ P(K = k) = \\binom{n}{k} \\theta^k (1 - \\theta)^{n-k} $\n",
    "  - $=\\frac{n!}{k!(n - k)!} \\cdot\\theta^{k}(1 - \\theta)^{n-k} $\n",
    "  - $=F(\\theta)$ 함수를 만들 수 있음. n=100, k=27 일때의 값을 계산하면 theta를 구할 수 있음\n",
    "  - 우리가 관찰 한(observation) 것을 가장 잘 설명하는 theta를 찾아내는 과정\n",
    "  - 어떻게 찾을까?\n",
    "    - Optimization via Gradient Descent\n",
    "    - 기울기를 통해 최대 값을 찾아냄 (Gradient Ascent/Descent:Local Maximum/Minimum)\n",
    "    - 기존 theta - (learning rate) * theta에 대해서 미분(손실 함수의 값(데이터X_i theta))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting\n",
    "  - 주어진 데이터에 대해 과도하게 fitting 되어 버린 상황\n",
    "    - 이 데이터에 대해 최적화 된 확률 분포를 설명하기 때문\n",
    "    \n",
    "Overfitting 줄이는 방법\n",
    "  - more data\n",
    "  - less features\n",
    "  - regularization\n",
    "    - early stopping : validation loss 제한\n",
    "    - reducing network size : nn 사이즈 제한\n",
    "    - weight decay : nn weight parameter 제한\n",
    "    - dropout\n",
    "    - batch normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Approach to Train DNN\n",
    "1. Make a neural network architecture.\n",
    "2. Train and check that model is over-fitted.\n",
    "  - If it is not, increase the model size (deeper and wider)\n",
    "  - If it is, add regularization, such as drop-out, batch-normalization.\n",
    "3. Repeat from step-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbb88082d50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test Dataset\n",
    "# |x_train| = (m, 3)\n",
    "x_train = torch.FloatTensor([[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5],\n",
    "                             [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]])\n",
    "# |y_train| = (m, )\n",
    "y_train = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])\n",
    "\n",
    "# |x_test| = (m`, 3)\n",
    "x_test = torch.FloatTensor([[2, 1, 1], [3, 1, 2], [3, 3, 4]])\n",
    "# |y_test| = (m`,)\n",
    "y_test = torch.LongTensor([2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 3) # 3 -> 3\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x) # |X| = (m, 3) => (m, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(model, optimizer, x_train, y_train):\n",
    "    nb_epochs = 20\n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        # cost 계산\n",
    "        cost = F.cross_entropy(prediction, y_train)\n",
    "        \n",
    "        # cost로 H(x) 개선\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test (Validation)\n",
    "def test(model, optimizer, x_test, y_test):\n",
    "    prediction = model(x_test)               # |x_test| = (m`, 3)\n",
    "    predicted_classes = prediction.max(1)[1]\n",
    "    correct_count = (predicted_classes == y_test).sum().item()\n",
    "    cost = F.cross_entropy(prediction, y_test)\n",
    "    \n",
    "    print('Accuracy: {}% Cost: {:.6f}'.format(\n",
    "        correct_count / len(y_test) * 100, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 2.203667\n",
      "Epoch    1/20 Cost: 1.199645\n",
      "Epoch    2/20 Cost: 1.142985\n",
      "Epoch    3/20 Cost: 1.117769\n",
      "Epoch    4/20 Cost: 1.100901\n",
      "Epoch    5/20 Cost: 1.089523\n",
      "Epoch    6/20 Cost: 1.079872\n",
      "Epoch    7/20 Cost: 1.071320\n",
      "Epoch    8/20 Cost: 1.063325\n",
      "Epoch    9/20 Cost: 1.055720\n",
      "Epoch   10/20 Cost: 1.048378\n",
      "Epoch   11/20 Cost: 1.041245\n",
      "Epoch   12/20 Cost: 1.034285\n",
      "Epoch   13/20 Cost: 1.027478\n",
      "Epoch   14/20 Cost: 1.020813\n",
      "Epoch   15/20 Cost: 1.014279\n",
      "Epoch   16/20 Cost: 1.007872\n",
      "Epoch   17/20 Cost: 1.001586\n",
      "Epoch   18/20 Cost: 0.995419\n",
      "Epoch   19/20 Cost: 0.989365\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0% Cost: 1.425844\n"
     ]
    }
   ],
   "source": [
    "test(model, optimizer, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate\n",
    "- learning rate가 너무 크면 diverge(발산) 하면서 cost가 점점 늘어난다. (overshooting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 1.280268\n",
      "Epoch    1/20 Cost: 976950.625000\n",
      "Epoch    2/20 Cost: 1279135.000000\n",
      "Epoch    3/20 Cost: 1198379.250000\n",
      "Epoch    4/20 Cost: 1098825.625000\n",
      "Epoch    5/20 Cost: 1968197.625000\n",
      "Epoch    6/20 Cost: 284763.125000\n",
      "Epoch    7/20 Cost: 1532260.000000\n",
      "Epoch    8/20 Cost: 1651504.250000\n",
      "Epoch    9/20 Cost: 521878.437500\n",
      "Epoch   10/20 Cost: 1397263.125000\n",
      "Epoch   11/20 Cost: 750986.250000\n",
      "Epoch   12/20 Cost: 918691.750000\n",
      "Epoch   13/20 Cost: 1487888.125000\n",
      "Epoch   14/20 Cost: 1582260.125000\n",
      "Epoch   15/20 Cost: 685818.000000\n",
      "Epoch   16/20 Cost: 1140048.750000\n",
      "Epoch   17/20 Cost: 940566.750000\n",
      "Epoch   18/20 Cost: 931638.125000\n",
      "Epoch   19/20 Cost: 1971322.625000\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "적절한 숫자로 시작해 발산하면 작게, cost가 줄어들지 않으면 크게 조정하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 3.187324\n",
      "Epoch    1/20 Cost: 1.334308\n",
      "Epoch    2/20 Cost: 1.047911\n",
      "Epoch    3/20 Cost: 0.996043\n",
      "Epoch    4/20 Cost: 0.985740\n",
      "Epoch    5/20 Cost: 0.977224\n",
      "Epoch    6/20 Cost: 0.970065\n",
      "Epoch    7/20 Cost: 0.963589\n",
      "Epoch    8/20 Cost: 0.957562\n",
      "Epoch    9/20 Cost: 0.951825\n",
      "Epoch   10/20 Cost: 0.946302\n",
      "Epoch   11/20 Cost: 0.940942\n",
      "Epoch   12/20 Cost: 0.935719\n",
      "Epoch   13/20 Cost: 0.930613\n",
      "Epoch   14/20 Cost: 0.925613\n",
      "Epoch   15/20 Cost: 0.920711\n",
      "Epoch   16/20 Cost: 0.915902\n",
      "Epoch   17/20 Cost: 0.911182\n",
      "Epoch   18/20 Cost: 0.906547\n",
      "Epoch   19/20 Cost: 0.901994\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "\n",
    "$$ X_j^i = \\frac{X_j - \\mu_j}{\\sigma_j} $$\n",
    "- $\\sigma$는 standard deviation, $\\mu$는 평균값이다.\n",
    "- Standardization : 정규분포화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75], [93, 88, 93], [89, 91, 90],\n",
    "                             [96, 98, 100],[73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([84.8000, 84.6000, 85.6000])\n"
     ]
    }
   ],
   "source": [
    "mu = x_train.mean(dim=0)\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11.0544, 12.2393, 12.6214])\n"
     ]
    }
   ],
   "source": [
    "sigma = x_train.std(dim=0)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0674, -0.3758, -0.8398],\n",
      "        [ 0.7418,  0.2778,  0.5863],\n",
      "        [ 0.3799,  0.5229,  0.3486],\n",
      "        [ 1.0132,  1.0948,  1.1409],\n",
      "        [-1.0674, -1.5197, -1.2360]])\n"
     ]
    }
   ],
   "source": [
    "# ~N(0, 1) : 어떤 정규 분포\n",
    "norm_x_train = (x_train - mu) / sigma\n",
    "print(norm_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegressionModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with Preprocessed Data\n",
    "def train(model, optimizer, x_train, y_train):\n",
    "    nb_epochs = 20\n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        # H(X) 계산\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        # cost 로 H(x) 개선\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 29785.089844\n",
      "Epoch    1/20 Cost: 18906.166016\n",
      "Epoch    2/20 Cost: 12054.674805\n",
      "Epoch    3/20 Cost: 7702.029785\n",
      "Epoch    4/20 Cost: 4925.733398\n",
      "Epoch    5/20 Cost: 3151.632812\n",
      "Epoch    6/20 Cost: 2016.996094\n",
      "Epoch    7/20 Cost: 1291.051270\n",
      "Epoch    8/20 Cost: 826.505249\n",
      "Epoch    9/20 Cost: 529.207397\n",
      "Epoch   10/20 Cost: 338.934204\n",
      "Epoch   11/20 Cost: 217.153564\n",
      "Epoch   12/20 Cost: 139.206757\n",
      "Epoch   13/20 Cost: 89.313835\n",
      "Epoch   14/20 Cost: 57.375465\n",
      "Epoch   15/20 Cost: 36.928429\n",
      "Epoch   16/20 Cost: 23.835773\n",
      "Epoch   17/20 Cost: 15.450429\n",
      "Epoch   18/20 Cost: 10.077809\n",
      "Epoch   19/20 Cost: 6.633700\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, norm_x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization: Let's not have too big numbers in the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_regularization(model, optimizer, x_train, y_train):\n",
    "    nb_epochs = 20\n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        # l2 norm 계산\n",
    "        l2_reg = 0\n",
    "        for param in model.parameters():\n",
    "            l2_reg += torch.norm(param)\n",
    "            \n",
    "        cost += l2_reg\n",
    "        \n",
    "        # cost로 H(x) 개선\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch+1, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegressionModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/20 Cost: 29602.789062\n",
      "Epoch    2/20 Cost: 18866.142578\n",
      "Epoch    3/20 Cost: 12100.623047\n",
      "Epoch    4/20 Cost: 7799.223633\n",
      "Epoch    5/20 Cost: 5054.626465\n",
      "Epoch    6/20 Cost: 3300.495117\n",
      "Epoch    7/20 Cost: 2178.548340\n",
      "Epoch    8/20 Cost: 1460.702148\n",
      "Epoch    9/20 Cost: 1001.335327\n",
      "Epoch   10/20 Cost: 707.352783\n",
      "Epoch   11/20 Cost: 519.203857\n",
      "Epoch   12/20 Cost: 398.785706\n",
      "Epoch   13/20 Cost: 321.714294\n",
      "Epoch   14/20 Cost: 272.384827\n",
      "Epoch   15/20 Cost: 240.810089\n",
      "Epoch   16/20 Cost: 220.598663\n",
      "Epoch   17/20 Cost: 207.659821\n",
      "Epoch   18/20 Cost: 199.375748\n",
      "Epoch   19/20 Cost: 194.070847\n",
      "Epoch   20/20 Cost: 190.672821\n"
     ]
    }
   ],
   "source": [
    "train_with_regularization(model, optimizer, norm_x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramters\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df041c1dbd741349ad260dc0b06553f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378adf3a8a03485bbd5f37871e64b99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db7ae7d887c488b8b3ddce4310c0505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/', train=True,\n",
    "                         transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/', train=False,\n",
    "                        transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size,\n",
    "                                         shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST data image of shape 28 * 28 = 784\n",
    "linear = torch.nn.Linear(784, 10, bias=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0001, cost:0.535468519\n",
      "Epoch:0002, cost:0.359274179\n",
      "Epoch:0003, cost:0.331187516\n",
      "Epoch:0004, cost:0.316578031\n",
      "Epoch:0005, cost:0.307158172\n",
      "Epoch:0006, cost:0.300180703\n",
      "Epoch:0007, cost:0.295130193\n",
      "Epoch:0008, cost:0.290851504\n",
      "Epoch:0009, cost:0.287417084\n",
      "Epoch:0010, cost:0.284379542\n",
      "Epoch:0011, cost:0.281825215\n",
      "Epoch:0012, cost:0.279800713\n",
      "Epoch:0013, cost:0.277808994\n",
      "Epoch:0014, cost:0.276154310\n",
      "Epoch:0015, cost:0.274440885\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # label is not one-hot encoded\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / total_batch\n",
    "    \n",
    "    print('Epoch:{:04d}, cost:{:.9f}'.format(epoch + 1, avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8862999677658081\n",
      "Label:  5\n",
      "Prediction:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the model using test sets\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28*28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = linear(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "    \n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28*28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = linear(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
    "    \n",
    "    plt.imshow(mnist_test.test_data[r:r+1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
