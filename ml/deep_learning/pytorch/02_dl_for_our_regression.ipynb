{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 W:0.0 b:0.0\n",
      "epoch:2 W:0.18666668236255646 b:0.07999999821186066\n",
      "epoch:3 W:0.35271111130714417 b:0.15093332529067993\n",
      "epoch:4 W:0.5004207491874695 b:0.21380621194839478\n",
      "epoch:5 W:0.6318292617797852 b:0.26951324939727783\n",
      "epoch:6 W:0.7487446665763855 b:0.3188498020172119\n",
      "epoch:7 W:0.8527745008468628 b:0.36252301931381226\n",
      "epoch:8 W:0.9453479647636414 b:0.4011615812778473\n",
      "epoch:9 W:1.027735710144043 b:0.43532443046569824\n",
      "epoch:10 W:1.1010674238204956 b:0.46550852060317993\n",
      "epoch:11 W:1.1663475036621094 b:0.49215564131736755\n",
      "epoch:12 W:1.2244688272476196 b:0.5156586170196533\n",
      "epoch:13 W:1.2762254476547241 b:0.5363667011260986\n",
      "epoch:14 W:1.3223230838775635 b:0.5545903444290161\n",
      "epoch:15 W:1.3633893728256226 b:0.5706056356430054\n",
      "epoch:16 W:1.3999820947647095 b:0.5846579670906067\n",
      "epoch:17 W:1.4325973987579346 b:0.5969655513763428\n",
      "epoch:18 W:1.4616763591766357 b:0.6077223420143127\n",
      "epoch:19 W:1.4876110553741455 b:0.6171008348464966\n",
      "epoch:20 W:1.5107499361038208 b:0.6252543926239014\n",
      "epoch:21 W:1.5314030647277832 b:0.6323193311691284\n",
      "epoch:22 W:1.5498460531234741 b:0.6384168267250061\n",
      "epoch:23 W:1.5663237571716309 b:0.6436546444892883\n",
      "epoch:24 W:1.5810539722442627 b:0.6481286287307739\n",
      "epoch:25 W:1.5942304134368896 b:0.6519238948822021\n",
      "epoch:26 W:1.6060253381729126 b:0.6551162004470825\n",
      "epoch:27 W:1.6165916919708252 b:0.6577728390693665\n",
      "epoch:28 W:1.6260654926300049 b:0.6599537134170532\n",
      "epoch:29 W:1.6345678567886353 b:0.6617119908332825\n",
      "epoch:30 W:1.6422064304351807 b:0.6630950570106506\n",
      "epoch:31 W:1.6490767002105713 b:0.6641448736190796\n",
      "epoch:32 W:1.6552637815475464 b:0.6648989319801331\n",
      "epoch:33 W:1.6608432531356812 b:0.6653904318809509\n",
      "epoch:34 W:1.6658822298049927 b:0.665648877620697\n",
      "epoch:35 W:1.6704405546188354 b:0.6657006144523621\n",
      "epoch:36 W:1.6745713949203491 b:0.665569007396698\n",
      "epoch:37 W:1.6783219575881958 b:0.6652747988700867\n",
      "epoch:38 W:1.6817342042922974 b:0.6648364067077637\n",
      "epoch:39 W:1.6848455667495728 b:0.664270281791687\n",
      "epoch:40 W:1.687689185142517 b:0.6635910272598267\n",
      "epoch:41 W:1.6902945041656494 b:0.6628116369247437\n",
      "epoch:42 W:1.6926878690719604 b:0.6619436144828796\n",
      "epoch:43 W:1.6948926448822021 b:0.660997211933136\n",
      "epoch:44 W:1.6969294548034668 b:0.6599815487861633\n",
      "epoch:45 W:1.6988167762756348 b:0.6589047312736511\n",
      "epoch:46 W:1.700571060180664 b:0.6577739715576172\n",
      "epoch:47 W:1.7022068500518799 b:0.6565956473350525\n",
      "epoch:48 W:1.7037370204925537 b:0.6553754806518555\n",
      "epoch:49 W:1.7051732540130615 b:0.6541184782981873\n",
      "epoch:50 W:1.7065256834030151 b:0.6528291702270508\n",
      "epoch:51 W:1.70780348777771 b:0.651511549949646\n",
      "epoch:52 W:1.709014654159546 b:0.6501691937446594\n",
      "epoch:53 W:1.710166573524475 b:0.6488052010536194\n",
      "epoch:54 W:1.7112654447555542 b:0.6474224328994751\n",
      "epoch:55 W:1.7123171091079712 b:0.6460233926773071\n",
      "epoch:56 W:1.7133265733718872 b:0.6446102261543274\n",
      "epoch:57 W:1.7142983675003052 b:0.6431849598884583\n",
      "epoch:58 W:1.7152364253997803 b:0.6417493224143982\n",
      "epoch:59 W:1.7161444425582886 b:0.6403048634529114\n",
      "epoch:60 W:1.7170253992080688 b:0.6388530135154724\n",
      "epoch:61 W:1.7178822755813599 b:0.6373949646949768\n",
      "epoch:62 W:1.7187174558639526 b:0.6359317898750305\n",
      "epoch:63 W:1.7195332050323486 b:0.63446444272995\n",
      "epoch:64 W:1.7203315496444702 b:0.6329938173294067\n",
      "epoch:65 W:1.721114158630371 b:0.631520688533783\n",
      "epoch:66 W:1.721882700920105 b:0.6300457119941711\n",
      "epoch:67 W:1.722638487815857 b:0.628569483757019\n",
      "epoch:68 W:1.723382830619812 b:0.6270925402641296\n",
      "epoch:69 W:1.7241166830062866 b:0.6256153583526611\n",
      "epoch:70 W:1.7248412370681763 b:0.6241384148597717\n",
      "epoch:71 W:1.7255572080612183 b:0.6226620078086853\n",
      "epoch:72 W:1.7262654304504395 b:0.6211864948272705\n",
      "epoch:73 W:1.7269665002822876 b:0.6197121739387512\n",
      "epoch:74 W:1.7276611328125 b:0.6182392835617065\n",
      "epoch:75 W:1.7283498048782349 b:0.6167680621147156\n",
      "epoch:76 W:1.72903311252594 b:0.6152986884117126\n",
      "epoch:77 W:1.7297114133834839 b:0.6138314008712769\n",
      "epoch:78 W:1.7303850650787354 b:0.6123663187026978\n",
      "epoch:79 W:1.731054425239563 b:0.6109035611152649\n",
      "epoch:80 W:1.7317198514938354 b:0.6094433069229126\n",
      "epoch:81 W:1.7323815822601318 b:0.6079856753349304\n",
      "epoch:82 W:1.7330398559570312 b:0.6065307259559631\n",
      "epoch:83 W:1.7336949110031128 b:0.6050785183906555\n",
      "epoch:84 W:1.734346866607666 b:0.6036291718482971\n",
      "epoch:85 W:1.73499596118927 b:0.6021826863288879\n",
      "epoch:86 W:1.7356423139572144 b:0.6007391810417175\n",
      "epoch:87 W:1.7362861633300781 b:0.5992987155914307\n",
      "epoch:88 W:1.7369275093078613 b:0.5978612899780273\n",
      "epoch:89 W:1.7375664710998535 b:0.5964269638061523\n",
      "epoch:90 W:1.7382031679153442 b:0.5949957370758057\n",
      "epoch:91 W:1.738837718963623 b:0.5935676693916321\n",
      "epoch:92 W:1.73947012424469 b:0.5921428203582764\n",
      "epoch:93 W:1.7401005029678345 b:0.5907211303710938\n",
      "epoch:94 W:1.7407289743423462 b:0.5893027186393738\n",
      "epoch:95 W:1.741355538368225 b:0.5878875255584717\n",
      "epoch:96 W:1.7419801950454712 b:0.5864755511283875\n",
      "epoch:97 W:1.742603063583374 b:0.5850668549537659\n",
      "epoch:98 W:1.7432241439819336 b:0.5836613774299622\n",
      "epoch:99 W:1.74384343624115 b:0.5822591781616211\n",
      "epoch:100 W:1.7444610595703125 b:0.5808602571487427\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    hypothesis = x_train * W + b\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    print('epoch:{0} W:{1} b:{2}'.format(epoch, W.item(), b.item()))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/10 W: 0.000, Cost: 4.666667\n",
      "Epoch    1/10 W: 1.400, Cost: 0.746667\n",
      "Epoch    2/10 W: 0.840, Cost: 0.119467\n",
      "Epoch    3/10 W: 1.064, Cost: 0.019115\n",
      "Epoch    4/10 W: 0.974, Cost: 0.003058\n",
      "Epoch    5/10 W: 1.010, Cost: 0.000489\n",
      "Epoch    6/10 W: 0.996, Cost: 0.000078\n",
      "Epoch    7/10 W: 1.002, Cost: 0.000013\n",
      "Epoch    8/10 W: 0.999, Cost: 0.000002\n",
      "Epoch    9/10 W: 1.000, Cost: 0.000000\n",
      "Epoch   10/10 W: 1.000, Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 데이터 \n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W], lr=0.15)\n",
    "\n",
    "nb_epochs = 10\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W\n",
    "    \n",
    "    # cost gradient 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    print('Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(\n",
    "    epoch, nb_epochs, W.item(), cost.item()))\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariable Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) cost: 9298.520508\n",
      "Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) cost: 2915.712402\n",
      "Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) cost: 915.040649\n",
      "Epoch    4/20 hypothesis: tensor([137.7968, 165.6247, 163.1912, 177.7112, 126.3307]) cost: 287.935822\n",
      "Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2331, 132.3891]) cost: 91.371170\n",
      "Epoch    6/20 hypothesis: tensor([148.1035, 178.0143, 175.3980, 191.0042, 135.7812]) cost: 29.758301\n",
      "Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]) cost: 10.445318\n",
      "Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) cost: 4.391228\n",
      "Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) cost: 2.493135\n",
      "Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) cost: 1.897688\n",
      "Epoch   11/20 hypothesis: tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602]) cost: 1.710541\n",
      "Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) cost: 1.651413\n",
      "Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) cost: 1.632375\n",
      "Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) cost: 1.625923\n",
      "Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) cost: 1.623412\n",
      "Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) cost: 1.622141\n",
      "Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) cost: 1.621253\n",
      "Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963]) cost: 1.620500\n",
      "Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985]) cost: 1.619770\n",
      "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 1.619063\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis: {} cost: {:6f}'.format(\n",
    "    epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch    1/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch    2/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch    3/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch    4/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch    5/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch    6/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch    7/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch    8/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch    9/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch   10/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch   11/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch   12/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch   13/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch   14/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch   15/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch   16/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch   17/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch   18/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch   19/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n",
      "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 11971.615234\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 모델 초기화\n",
    "model = MultivariateLinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis: {} cost: {:6f}'.format(\n",
    "    epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = [[73, 80, 75], [93, 88, 93],[89, 91, 90],\n",
    "                       [96, 98, 100],[73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "dataset = CustomDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 12150.919922\n",
      "Epoch    0/20 Batch 2/3 Cost: 11308.335938\n",
      "Epoch    0/20 Batch 3/3 Cost: 12939.562500\n",
      "Epoch    1/20 Batch 1/3 Cost: 14090.558594\n",
      "Epoch    1/20 Batch 2/3 Cost: 11583.566406\n",
      "Epoch    1/20 Batch 3/3 Cost: 8509.826172\n",
      "Epoch    2/20 Batch 1/3 Cost: 11583.566406\n",
      "Epoch    2/20 Batch 2/3 Cost: 10724.694336\n",
      "Epoch    2/20 Batch 3/3 Cost: 15241.553711\n",
      "Epoch    3/20 Batch 1/3 Cost: 11583.566406\n",
      "Epoch    3/20 Batch 2/3 Cost: 11875.689453\n",
      "Epoch    3/20 Batch 3/3 Cost: 12939.562500\n",
      "Epoch    4/20 Batch 1/3 Cost: 14674.200195\n",
      "Epoch    4/20 Batch 2/3 Cost: 10999.923828\n",
      "Epoch    4/20 Batch 3/3 Cost: 8509.826172\n",
      "Epoch    5/20 Batch 1/3 Cost: 8785.055664\n",
      "Epoch    5/20 Batch 2/3 Cost: 14090.558594\n",
      "Epoch    5/20 Batch 3/3 Cost: 14106.846680\n",
      "Epoch    6/20 Batch 1/3 Cost: 10724.694336\n",
      "Epoch    6/20 Batch 2/3 Cost: 12150.919922\n",
      "Epoch    6/20 Batch 3/3 Cost: 14106.846680\n",
      "Epoch    7/20 Batch 1/3 Cost: 14090.558594\n",
      "Epoch    7/20 Batch 2/3 Cost: 11308.335938\n",
      "Epoch    7/20 Batch 3/3 Cost: 9060.285156\n",
      "Epoch    8/20 Batch 1/3 Cost: 14090.558594\n",
      "Epoch    8/20 Batch 2/3 Cost: 8785.055664\n",
      "Epoch    8/20 Batch 3/3 Cost: 14106.846680\n",
      "Epoch    9/20 Batch 1/3 Cost: 11875.689453\n",
      "Epoch    9/20 Batch 2/3 Cost: 13523.205078\n",
      "Epoch    9/20 Batch 3/3 Cost: 9060.285156\n",
      "Epoch   10/20 Batch 1/3 Cost: 11583.566406\n",
      "Epoch   10/20 Batch 2/3 Cost: 10724.694336\n",
      "Epoch   10/20 Batch 3/3 Cost: 15241.553711\n",
      "Epoch   11/20 Batch 1/3 Cost: 11583.566406\n",
      "Epoch   11/20 Batch 2/3 Cost: 14090.558594\n",
      "Epoch   11/20 Batch 3/3 Cost: 8509.826172\n",
      "Epoch   12/20 Batch 1/3 Cost: 13523.205078\n",
      "Epoch   12/20 Batch 2/3 Cost: 11875.689453\n",
      "Epoch   12/20 Batch 3/3 Cost: 9060.285156\n",
      "Epoch   13/20 Batch 1/3 Cost: 8785.055664\n",
      "Epoch   13/20 Batch 2/3 Cost: 14090.558594\n",
      "Epoch   13/20 Batch 3/3 Cost: 14106.846680\n",
      "Epoch   14/20 Batch 1/3 Cost: 11583.566406\n",
      "Epoch   14/20 Batch 2/3 Cost: 14090.558594\n",
      "Epoch   14/20 Batch 3/3 Cost: 8509.826172\n",
      "Epoch   15/20 Batch 1/3 Cost: 10724.694336\n",
      "Epoch   15/20 Batch 2/3 Cost: 11583.566406\n",
      "Epoch   15/20 Batch 3/3 Cost: 15241.553711\n",
      "Epoch   16/20 Batch 1/3 Cost: 14090.558594\n",
      "Epoch   16/20 Batch 2/3 Cost: 8785.055664\n",
      "Epoch   16/20 Batch 3/3 Cost: 14106.846680\n",
      "Epoch   17/20 Batch 1/3 Cost: 14090.558594\n",
      "Epoch   17/20 Batch 2/3 Cost: 11583.566406\n",
      "Epoch   17/20 Batch 3/3 Cost: 8509.826172\n",
      "Epoch   18/20 Batch 1/3 Cost: 14674.200195\n",
      "Epoch   18/20 Batch 2/3 Cost: 8785.055664\n",
      "Epoch   18/20 Batch 3/3 Cost: 12939.562500\n",
      "Epoch   19/20 Batch 1/3 Cost: 13523.205078\n",
      "Epoch   19/20 Batch 2/3 Cost: 11875.689453\n",
      "Epoch   19/20 Batch 3/3 Cost: 9060.285156\n",
      "Epoch   20/20 Batch 1/3 Cost: 10724.694336\n",
      "Epoch   20/20 Batch 2/3 Cost: 14674.200195\n",
      "Epoch   20/20 Batch 3/3 Cost: 9060.285156\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        \n",
    "        x_train, y_train = samples\n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        # cost로 H(x) 개선\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader), cost.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9ab4060d30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following classification problem: given the number of hours each student spent watching the lecture and working in the code lab, predict whether the student passed or failed a course. For example, the first (index 0) student watched the lecture for 1 hour and spend 2 hours in the lab session ([1, 2]), and ended up failing the course ([0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we need these data to be in torch.Tensor format, so we convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has a torch.exp() function that resembles the exponential function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e^1 equals :  tensor([2.7183])\n"
     ]
    }
   ],
   "source": [
    "print('e^1 equals : ', torch.exp(torch.FloatTensor([1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it to compute the hypothesis function conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W)+b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<MulBackward0>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(hypothesis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we could use torch.sigmoid() function! This resembles the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/(1+e^{-1}) equals:  tensor([0.7311])\n"
     ]
    }
   ],
   "source": [
    "print('1/(1+e^{-1}) equals: ', torch.sigmoid(torch.FloatTensor([1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the code for hypothesis function is cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(hypothesis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to measure the difference between hypothesis and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the Cost Function\n",
    "\n",
    "For one element, the loss can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6931], grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "losses = -(y_train[0] * torch.log(hypothesis[0]) + \n",
    "           (1 - y_train[0]) * torch.log(1 - hypothesis[0]))\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we just .mean() to take the mean of these individual losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = losses.mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(hypothesis, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.693147\n",
      "Epoch  100/1000 Cost: 0.134722\n",
      "Epoch  200/1000 Cost: 0.080643\n",
      "Epoch  300/1000 Cost: 0.057900\n",
      "Epoch  400/1000 Cost: 0.045300\n",
      "Epoch  500/1000 Cost: 0.037261\n",
      "Epoch  600/1000 Cost: 0.031672\n",
      "Epoch  700/1000 Cost: 0.027556\n",
      "Epoch  800/1000 Cost: 0.024394\n",
      "Epoch  900/1000 Cost: 0.021888\n",
      "Epoch 1000/1000 Cost: 0.019852\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 실행\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "    \n",
    "    # cost 로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7648e-04],\n",
      "        [3.1608e-02],\n",
      "        [3.8977e-02],\n",
      "        [9.5622e-01],\n",
      "        [9.9823e-01]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "# After we finish training the model, \n",
    "# we want to check how well our model fits the training set.\n",
    "\n",
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b) # 원래는 x_test 셋으로 테스트.\n",
    "print(hypothesis[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True]])\n"
     ]
    }
   ],
   "source": [
    "# We can change hypothesis (real number from 0 to 1) \n",
    "# to binary predictions (either 0 or 1) by comparing them to 0.5.\n",
    "\n",
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "print(prediction[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# Then, we compare it with the correct labels y_train.\n",
    "print(prediction[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True]])\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = prediction.float() == y_train\n",
    "print(correct_prediction[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher Implementation with Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1) # self.linear = (W, b)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.539713 Accuracy 83.33%\n",
      "Epoch   10/100 Cost: 0.614853 Accuracy 66.67%\n",
      "Epoch   20/100 Cost: 0.441875 Accuracy 66.67%\n",
      "Epoch   30/100 Cost: 0.373145 Accuracy 83.33%\n",
      "Epoch   40/100 Cost: 0.316358 Accuracy 83.33%\n",
      "Epoch   50/100 Cost: 0.266094 Accuracy 83.33%\n",
      "Epoch   60/100 Cost: 0.220498 Accuracy 100.00%\n",
      "Epoch   70/100 Cost: 0.182095 Accuracy 100.00%\n",
      "Epoch   80/100 Cost: 0.157299 Accuracy 100.00%\n",
      "Epoch   90/100 Cost: 0.144091 Accuracy 100.00%\n",
      "Epoch  100/100 Cost: 0.134272 Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "    \n",
    "    # cost 로 H(x) 계산\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "        print('Epoch {:4d}/{} Cost: {:6f} Accuracy {:2.2f}%'.format(\n",
    "        epoch, nb_epochs, cost.item(), accuracy * 100,))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
