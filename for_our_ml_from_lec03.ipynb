{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "for_our_ml.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groovallstar/test2/blob/feature%2Ffrom_colab/for_our_ml_from_lec03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9b49rPihu4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwqXrGbRXV7F",
        "colab_type": "text"
      },
      "source": [
        "<h1> hypothesis and cost function </h1>\n",
        "- 1차 방정식\n",
        "$$H(x) = Wx + b$$\n",
        "- 임의로 표현한 1차방정식과 실제 값과의 차이를 구하는 식 (제곱 하면 음수도 양수 표현 가능)\n",
        "$$(H(x) - y)^2$$<br>\n",
        "- formal하게 바꾼 수식\n",
        "$$ cost(W,b) = {1 \\over m}\\sum_{i=1}^m(H(x^{(i)}) - y^{(i)})^2 $$\n",
        "    * m = 데이터 개수<br>\n",
        "- 결론 minimize cost(W,b) -> 학습을 통해 W, b의 최소화 값(가장 근접한 값) 구하는 것\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiHhLXjBLidM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "da8353d6-30b3-499c-94ce-412eba0510f5"
      },
      "source": [
        "node1 = tf.constant(3.0, tf.float32)\n",
        "node2 = tf.constant(4.0)\n",
        "node3 = tf.add(node1, node2)\n",
        "\n",
        "print('node1:', node1, \"node2:\", node2)\n",
        "print(\"node3:\", node3)\n",
        "\n",
        "print('sess.run(node1, node2): ', sess.run([node1, node2]))\n",
        "print('sess.run(node3): ', sess.run(node3))\n",
        "\n",
        "a = tf.placeholder(tf.float32)\n",
        "b = tf.placeholder(tf.float32)\n",
        "adder_node = a + b\n",
        "\n",
        "print(sess.run(adder_node, feed_dict={a:3, b:4.5}))\n",
        "print(sess.run(adder_node, feed_dict={a:[1,3], b:[2,4]}))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "node1: Tensor(\"Const_13:0\", shape=(), dtype=float32) node2: Tensor(\"Const_14:0\", shape=(), dtype=float32)\n",
            "node3: Tensor(\"Add_15:0\", shape=(), dtype=float32)\n",
            "sess.run(node1, node2):  [3.0, 4.0]\n",
            "sess.run(node3):  7.0\n",
            "7.5\n",
            "[3. 7.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWbaHBishxi7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "38387534-addf-4f52-8979-4dba012a3522"
      },
      "source": [
        "x_train = [1, 2, 3]\n",
        "y_train = [1, 2, 3]\n",
        "\n",
        "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "# H(x) = Wx + b\n",
        "hypothesis = x_train * W + b\n",
        "\n",
        "# cost/loss function\n",
        "# t = [1., 2., 3., 4.]\n",
        "# tf.reduce_mean(t) ==> 2.5\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
        "\n",
        "# minimize\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "# initializes global variables in the graph.\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "'''\n",
        "adder_node = a + b\n",
        "print(sess.run(adder_node, feed_dict={a:3, b:4.5}))\n",
        "print(sess.run(adder_node, feed_dict={a:[1,3], b:[2,4]}))\n",
        "'''\n",
        "\n",
        "X = tf.placeholder(tf.float32)\n",
        "Y = tf.placeholder(tf.float32)\n",
        "\n",
        "for step in range(2001):\n",
        "  cost_val, W_val, b_val, _ = sess.run([cost, W, b, train], feed_dict={X:[1,2,3], Y:[1,2,3]})\n",
        "  if step % 20 == 0:\n",
        "    print(step, cost_val, W_val, b_val)\n",
        "\n",
        "'''\n",
        "# Fit the line\n",
        "for step in range(2001):\n",
        "  sess.run(train)\n",
        "  if step % 20 == 0:\n",
        "    print(step, sess.run(cost), sess.run(W), sess.run(b))\n",
        "'''"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 4.9856014 [0.01783126] [0.14645904]\n",
            "20 0.07565526 [0.7235749] [0.42972872]\n",
            "40 0.028343134 [0.7993711] [0.4371612]\n",
            "60 0.025375687 [0.81478024] [0.419247]\n",
            "80 0.023043295 [0.82405436] [0.39979443]\n",
            "100 0.020928249 [0.8323773] [0.38102943]\n",
            "120 0.01900737 [0.84026015] [0.36312476]\n",
            "140 0.017262803 [0.8477678] [0.3460594]\n",
            "160 0.01567834 [0.8549222] [0.3297959]\n",
            "180 0.014239346 [0.86174035] [0.3142967]\n",
            "200 0.01293238 [0.86823803] [0.29952592]\n",
            "220 0.011745408 [0.87443036] [0.2854493]\n",
            "240 0.010667349 [0.8803317] [0.2720342]\n",
            "260 0.009688272 [0.8859557] [0.2592496]\n",
            "280 0.00879903 [0.8913154] [0.24706578]\n",
            "300 0.007991419 [0.89642316] [0.23545457]\n",
            "320 0.007257938 [0.9012909] [0.22438909]\n",
            "340 0.0065917806 [0.9059298] [0.21384363]\n",
            "360 0.0059867557 [0.9103508] [0.20379378]\n",
            "380 0.0054372703 [0.91456395] [0.19421622]\n",
            "400 0.004938217 [0.9185793] [0.18508875]\n",
            "420 0.0044849645 [0.9224057] [0.17639019]\n",
            "440 0.004073314 [0.92605233] [0.1681005]\n",
            "460 0.0036994468 [0.92952764] [0.16020037]\n",
            "480 0.0033598973 [0.9328395] [0.15267152]\n",
            "500 0.003051512 [0.9359959] [0.14549652]\n",
            "520 0.0027714332 [0.9390038] [0.13865872]\n",
            "540 0.0025170615 [0.9418704] [0.13214228]\n",
            "560 0.002286037 [0.94460225] [0.1259321]\n",
            "580 0.0020762135 [0.9472057] [0.12001377]\n",
            "600 0.001885655 [0.949687] [0.11437355]\n",
            "620 0.001712574 [0.9520514] [0.10899838]\n",
            "640 0.0015553888 [0.95430475] [0.10387588]\n",
            "660 0.0014126339 [0.9564523] [0.09899414]\n",
            "680 0.0012829704 [0.9584989] [0.09434177]\n",
            "700 0.0011652192 [0.96044934] [0.08990802]\n",
            "720 0.0010582684 [0.9623081] [0.08568266]\n",
            "740 0.00096113543 [0.96407944] [0.08165589]\n",
            "760 0.0008729229 [0.96576756] [0.07781836]\n",
            "780 0.0007928 [0.96737635] [0.07416118]\n",
            "800 0.0007200328 [0.9689095] [0.0706759]\n",
            "820 0.000653943 [0.9703707] [0.0673544]\n",
            "840 0.0005939253 [0.9717632] [0.06418898]\n",
            "860 0.0005394094 [0.9730902] [0.06117233]\n",
            "880 0.0004899009 [0.9743548] [0.05829747]\n",
            "900 0.00044493438 [0.97556007] [0.05555772]\n",
            "920 0.0004040997 [0.97670865] [0.05294671]\n",
            "940 0.00036701027 [0.97780323] [0.05045844]\n",
            "960 0.0003333233 [0.9788463] [0.04808708]\n",
            "980 0.00030273176 [0.9798405] [0.04582722]\n",
            "1000 0.000274947 [0.980788] [0.04367353]\n",
            "1020 0.0002497088 [0.9816909] [0.041621]\n",
            "1040 0.00022678993 [0.98255134] [0.03966495]\n",
            "1060 0.00020597332 [0.9833714] [0.03780082]\n",
            "1080 0.00018706878 [0.98415285] [0.03602431]\n",
            "1100 0.00016989867 [0.9848976] [0.03433131]\n",
            "1120 0.00015430608 [0.9856074] [0.03271785]\n",
            "1140 0.00014014351 [0.98628384] [0.03118022]\n",
            "1160 0.00012727921 [0.98692834] [0.02971487]\n",
            "1180 0.00011559722 [0.9875427] [0.02831841]\n",
            "1200 0.00010498746 [0.9881281] [0.02698755]\n",
            "1220 9.5352334e-05 [0.988686] [0.02571925]\n",
            "1240 8.659961e-05 [0.9892177] [0.02451058]\n",
            "1260 7.865186e-05 [0.98972446] [0.02335867]\n",
            "1280 7.1431525e-05 [0.99020755] [0.02226087]\n",
            "1300 6.4875065e-05 [0.9906677] [0.02121462]\n",
            "1320 5.8920745e-05 [0.9911063] [0.02021759]\n",
            "1340 5.351237e-05 [0.9915242] [0.01926741]\n",
            "1360 4.8601098e-05 [0.99192256] [0.01836191]\n",
            "1380 4.4140474e-05 [0.9923021] [0.01749898]\n",
            "1400 4.0089308e-05 [0.9926639] [0.0166766]\n",
            "1420 3.6410183e-05 [0.9930087] [0.01589288]\n",
            "1440 3.3067565e-05 [0.9933373] [0.01514597]\n",
            "1460 3.003322e-05 [0.9936504] [0.01443417]\n",
            "1480 2.727586e-05 [0.9939488] [0.01375582]\n",
            "1500 2.477278e-05 [0.99423313] [0.01310935]\n",
            "1520 2.2498934e-05 [0.99450415] [0.01249328]\n",
            "1540 2.0434445e-05 [0.9947624] [0.01190616]\n",
            "1560 1.855863e-05 [0.9950086] [0.01134664]\n",
            "1580 1.685491e-05 [0.9952432] [0.01081339]\n",
            "1600 1.5307976e-05 [0.99546677] [0.01030519]\n",
            "1620 1.3903064e-05 [0.9956798] [0.00982088]\n",
            "1640 1.2627331e-05 [0.9958828] [0.00935933]\n",
            "1660 1.14681925e-05 [0.9960763] [0.0089195]\n",
            "1680 1.0415165e-05 [0.9962607] [0.00850031]\n",
            "1700 9.459502e-06 [0.9964364] [0.00810082]\n",
            "1720 8.5912425e-06 [0.9966039] [0.00772012]\n",
            "1740 7.802494e-06 [0.9967635] [0.0073573]\n",
            "1760 7.086795e-06 [0.99691564] [0.00701154]\n",
            "1780 6.4360993e-06 [0.99706054] [0.00668204]\n",
            "1800 5.8452474e-06 [0.9971987] [0.00636801]\n",
            "1820 5.309277e-06 [0.9973303] [0.00606875]\n",
            "1840 4.8218835e-06 [0.9974558] [0.00578357]\n",
            "1860 4.3793166e-06 [0.9975753] [0.00551178]\n",
            "1880 3.977208e-06 [0.9976893] [0.00525276]\n",
            "1900 3.61227e-06 [0.9977979] [0.00500588]\n",
            "1920 3.2804837e-06 [0.9979014] [0.00477062]\n",
            "1940 2.979565e-06 [0.99799997] [0.00454644]\n",
            "1960 2.706014e-06 [0.99809396] [0.00433279]\n",
            "1980 2.4577082e-06 [0.99818355] [0.00412919]\n",
            "2000 2.232258e-06 [0.9982689] [0.00393515]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Fit the line\\nfor step in range(2001):\\n  sess.run(train)\\n  if step % 20 == 0:\\n    print(step, sess.run(cost), sess.run(W), sess.run(b))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvhrq_wxKkVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dd2eb614-9542-4ddc-86cb-43d230d8e8a1"
      },
      "source": [
        "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "X = tf.placeholder(tf.float32, shape=[None])\n",
        "Y = tf.placeholder(tf.float32, shape=[None])\n",
        "\n",
        "hypothesis = X * W + b\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(2001):\n",
        "  cost_val, W_val, b_val, _ = sess.run([cost, W, b, train],\n",
        "    feed_dict={X:[1,2,3,4,5], Y:[2.1, 3.1, 4.1, 5.1, 6.1]})\n",
        "  if step % 20 == 0:\n",
        "    print(step, cost_val, W_val, b_val)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 83.96294 [-0.44505155] [-1.2734768]\n",
            "20 0.5391868 [1.4655435] [-0.61504465]\n",
            "40 0.46939096 [1.443257] [-0.5004533]\n",
            "60 0.4099225 [1.4142648] [-0.39562804]\n",
            "80 0.3579885 [1.3871344] [-0.2976782]\n",
            "100 0.31263405 [1.3617806] [-0.20614313]\n",
            "120 0.2730257 [1.3380873] [-0.12060278]\n",
            "140 0.2384353 [1.3159457] [-0.04066447]\n",
            "160 0.20822737 [1.2952542] [0.03403853]\n",
            "180 0.18184637 [1.2759178] [0.10384915]\n",
            "200 0.15880787 [1.2578477] [0.16908783]\n",
            "220 0.13868809 [1.2409611] [0.23005402]\n",
            "240 0.12111728 [1.2251803] [0.2870275]\n",
            "260 0.105772614 [1.210433] [0.34026977]\n",
            "280 0.092372075 [1.1966517] [0.39002514]\n",
            "300 0.08066925 [1.1837728] [0.43652192]\n",
            "320 0.07044903 [1.1717373] [0.4799736]\n",
            "340 0.061523624 [1.1604902] [0.5205797]\n",
            "360 0.05372907 [1.1499795] [0.55852634]\n",
            "380 0.04692197 [1.1401572] [0.59398794]\n",
            "400 0.04097733 [1.1309781] [0.6271271]\n",
            "420 0.035785787 [1.1224003] [0.65809596]\n",
            "440 0.03125199 [1.1143843] [0.68703663]\n",
            "460 0.02729262 [1.1068932] [0.7140819]\n",
            "480 0.023834813 [1.0998925] [0.7393559]\n",
            "500 0.020815104 [1.0933505] [0.7629748]\n",
            "520 0.018178016 [1.0872369] [0.78504694]\n",
            "540 0.01587495 [1.0815235] [0.80567366]\n",
            "560 0.013863725 [1.0761846] [0.8249494]\n",
            "580 0.012107299 [1.0711951] [0.8429626]\n",
            "600 0.010573385 [1.0665326] [0.8597962]\n",
            "620 0.009233853 [1.0621754] [0.8755272]\n",
            "640 0.008063979 [1.0581034] [0.89022803]\n",
            "660 0.0070423246 [1.0542983] [0.9039662]\n",
            "680 0.0061501144 [1.050742] [0.9168046]\n",
            "700 0.005370938 [1.0474191] [0.9288022]\n",
            "720 0.0046904944 [1.0443135] [0.94001406]\n",
            "740 0.004096249 [1.0414114] [0.95049155]\n",
            "760 0.0035772803 [1.0386994] [0.960283]\n",
            "780 0.0031240652 [1.0361649] [0.9694331]\n",
            "800 0.0027282715 [1.0337964] [0.9779841]\n",
            "820 0.0023826144 [1.0315831] [0.98597497]\n",
            "840 0.0020807565 [1.0295147] [0.9934426]\n",
            "860 0.0018171361 [1.0275817] [1.000421]\n",
            "880 0.0015869222 [1.0257754] [1.0069424]\n",
            "900 0.0013858742 [1.0240873] [1.013037]\n",
            "920 0.0012103016 [1.0225099] [1.018732]\n",
            "940 0.0010569601 [1.0210356] [1.0240544]\n",
            "960 0.0009230537 [1.0196581] [1.029028]\n",
            "980 0.00080610515 [1.0183705] [1.033676]\n",
            "1000 0.000703982 [1.0171676] [1.0380198]\n",
            "1020 0.00061479135 [1.0160432] [1.0420789]\n",
            "1040 0.00053690025 [1.0149925] [1.0458722]\n",
            "1060 0.00046888104 [1.0140105] [1.049417]\n",
            "1080 0.00040947134 [1.013093] [1.0527298]\n",
            "1100 0.00035759795 [1.0122355] [1.0558257]\n",
            "1120 0.00031228623 [1.0114342] [1.0587188]\n",
            "1140 0.00027272193 [1.0106854] [1.0614225]\n",
            "1160 0.00023817396 [1.0099857] [1.0639488]\n",
            "1180 0.00020799872 [1.0093316] [1.0663098]\n",
            "1200 0.00018164542 [1.0087204] [1.0685163]\n",
            "1220 0.00015863145 [1.0081493] [1.0705782]\n",
            "1240 0.00013853433 [1.0076157] [1.072505]\n",
            "1260 0.000120984114 [1.0071169] [1.0743057]\n",
            "1280 0.00010565562 [1.0066508] [1.0759884]\n",
            "1300 9.2269984e-05 [1.0062152] [1.077561]\n",
            "1320 8.0579244e-05 [1.0058082] [1.0790305]\n",
            "1340 7.0369635e-05 [1.0054277] [1.080404]\n",
            "1360 6.145424e-05 [1.0050724] [1.0816873]\n",
            "1380 5.3666958e-05 [1.0047401] [1.0828867]\n",
            "1400 4.6868143e-05 [1.0044297] [1.0840074]\n",
            "1420 4.0932464e-05 [1.0041397] [1.0850545]\n",
            "1440 3.5747034e-05 [1.0038686] [1.0860332]\n",
            "1460 3.1217114e-05 [1.0036151] [1.0869479]\n",
            "1480 2.7262713e-05 [1.0033784] [1.0878026]\n",
            "1500 2.3809163e-05 [1.0031571] [1.0886016]\n",
            "1520 2.0791851e-05 [1.0029504] [1.089348]\n",
            "1540 1.8159026e-05 [1.0027572] [1.0900453]\n",
            "1560 1.5858324e-05 [1.0025767] [1.0906974]\n",
            "1580 1.3849203e-05 [1.0024079] [1.0913066]\n",
            "1600 1.2094586e-05 [1.0022502] [1.0918759]\n",
            "1620 1.0562317e-05 [1.002103] [1.0924078]\n",
            "1640 9.224166e-06 [1.0019652] [1.092905]\n",
            "1660 8.055321e-06 [1.0018365] [1.0933697]\n",
            "1680 7.03536e-06 [1.0017163] [1.0938039]\n",
            "1700 6.144339e-06 [1.0016037] [1.0942097]\n",
            "1720 5.3656536e-06 [1.0014988] [1.0945888]\n",
            "1740 4.687031e-06 [1.0014007] [1.0949428]\n",
            "1760 4.0925993e-06 [1.001309] [1.095274]\n",
            "1780 3.574085e-06 [1.0012233] [1.0955834]\n",
            "1800 3.1219122e-06 [1.0011432] [1.0958725]\n",
            "1820 2.7262645e-06 [1.0010684] [1.0961428]\n",
            "1840 2.3810403e-06 [1.0009985] [1.0963955]\n",
            "1860 2.0793905e-06 [1.0009329] [1.0966314]\n",
            "1880 1.815945e-06 [1.000872] [1.0968521]\n",
            "1900 1.5861657e-06 [1.0008149] [1.097058]\n",
            "1920 1.3851843e-06 [1.0007615] [1.0972506]\n",
            "1940 1.2099865e-06 [1.0007117] [1.0974306]\n",
            "1960 1.0567439e-06 [1.0006652] [1.0975988]\n",
            "1980 9.2280334e-07 [1.0006216] [1.0977559]\n",
            "2000 8.0598363e-07 [1.0005809] [1.0979028]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK-Qp-FaOyw3",
        "colab_type": "text"
      },
      "source": [
        "- W = 1, cost(W) = ?\n",
        "$$ {1 \\over 3}((1*1-1)^2+(1*2-2)^2+(1*3-3)^2) $$\n",
        "- W = 0, cost(W) = 4.67\n",
        "$$ {1 \\over 3}((0*1-1)^2+(0*2-2)^2+(0*3-3)^2) $$\n",
        "- W = 2, cost(W) = 4.67\n",
        "$$ {1 \\over 3}((2*1-1)^2+(2*2-2)^2+(2*3-3)^2) $$\n",
        "\n",
        "- Gradient descent algorithm (경사하강법)\n",
        "  - cost(W, b) 를 최소화 할때 사용하는 알고리즘\n",
        "  - 그래프 상 아무 값에서 시작 후 경사도를 반복 계산 해서 최소점을 찾음\n",
        "\n",
        "- Formal definition\n",
        "$$ cost(W) = {1 \\over 2m}\\sum_{i=1}^m(H(x^{(i)}) - y^{(i)})^2 $$\n",
        "$$ W := W - \\alpha{\\partial\\over\\partial W}cost(W) $$\n",
        "- alpha = learning rate (상수)\n",
        "  - 그래프에서 최소화된 W를 찾기위한 미분 할 때 바깥으로 튀지 않고 안쪽으로 (-는 +쪽으로, +는 -쪽으로) 옮겨올 수 있도록 튜닝하는 값\n",
        "\n",
        "  ![대체 텍스트](https://t1.daumcdn.net/cfile/tistory/223B87385890215809)\n",
        "\n",
        "$$ W := W - \\alpha{1 \\over m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})x^{(i)} $$\n",
        "- 이건 다시 볼 필요 있음.\n",
        "\n"
      ]
    }
  ]
}
